{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Linear regression is one of the most well known and well understood algorithms in statistics and machine learning. In this notebook, we will discuss the linear regression algorithm, how it works and how you can best use it in your machine learning projects. we will introduce the concept of minimizing a cost function to determine the optimal model parameters. We will demonstrate linear regression with scikit learn machine learning module and statsmodel library.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Formalism](#Formalism)\n",
    "\n",
    "[Cost Function](#Cost-Function)\n",
    "\n",
    "[Linear Regression with Scikit Learn](#Linear-Regression-with-Scikit-Learn)\n",
    "\n",
    "- [Data Preparation](#Data-Preparation)\n",
    "\n",
    "- [Linear Regression](#Linear-Regression)\n",
    "\n",
    "[Linear Regression with StatsModel](#Linear-Regression-with-Statsmodels)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the _Formalism_ section of this Notebook, we first have our standard notebook setup code, after which we load a sample data set, the _tips_ data from Seaborn, and perform simple linear regression.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "In this notebook, we will use the _tips_ data set from the Seaborn module. After loading this data into our Notebook, we display several random rows, and next compute a simple linear regression to predict the `tip` feature from the `total_bill` feature.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>34.83</td>\n",
       "      <td>5.17</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Thur</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>25.56</td>\n",
       "      <td>4.34</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>8.52</td>\n",
       "      <td>1.48</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Thur</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>16.32</td>\n",
       "      <td>4.30</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>23.95</td>\n",
       "      <td>2.55</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_bill   tip     sex smoker   day    time  size\n",
       "85        34.83  5.17  Female     No  Thur   Lunch     4\n",
       "54        25.56  4.34    Male     No   Sun  Dinner     4\n",
       "126        8.52  1.48    Male     No  Thur   Lunch     2\n",
       "93        16.32  4.30  Female    Yes   Fri  Dinner     2\n",
       "113       23.95  2.55    Male     No   Sun  Dinner     2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data Set\n",
    "tdf = sns.load_dataset('tips')\n",
    "\n",
    "# Display several random rows\n",
    "tdf.sample(5, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[Back to TOC](#Table-of-Contents)\n",
    "\n",
    "## Formalism\n",
    "\n",
    "-----\n",
    "\n",
    "Linear regression is a linear model that assumes a linear relationship between the input variables (x or independent variable) and the single output variable (y or dependent variable). More specifically, that y can be calculated from a linear combination of the input variables (x). It constructs a simple model, such as\n",
    "$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$\n",
    "from a data set. This model builds on assumptions, such as the y and x are linearly dependent and any errors in the regression are normally distributed, to build a model from the independent variables (i.e., $x_1..x_n$) for the dependent variable ($y$). In some application areas, the independent variables are known as the predictors, while the dependent variable is known as the response variable. If only one feature is used ($x$), the technique is known as simple linear regression, while if more than one feature is used ($x_1, x_2, ..., x_n$), the technique is known as multiple linear regression.\n",
    "\n",
    "To make it simple, we start with simple linear regression. The simple linear model relates the independent variables $x_i$ to the dependent variables $y_i$ via two parameters: an intercept, and a slope. Mathematically, we express this relation in the following form:\n",
    "\n",
    "$f(x_i) = \\alpha + \\beta * x_i + \\epsilon_i$\n",
    "\n",
    "where $\\epsilon_i$ accounts for the difference between the model and the data for each data point $(x_i, y_i)$. If we have a perfect model, these errors, $\\epsilon_i$, are all zero, and $y_i = f(x_i)$. In real life, however, the error terms rarely vanish because even if the original relationship is perfect noise creeps into the measurement process. \n",
    "\n",
    "As a result, in this simple example we wish to determine the model parameters: $\\beta$, and $\\alpha$ that minimize the values of $\\epsilon_i$. We could perform this process in an iterative manner, trying different values for the model parameters and measuring the error function. This approach is often used in machine learning, where we define a **cost function** that we seek to minimize by selecting the best model parameters. \n",
    "\n",
    "In the case of a simple linear model, we have several potential cost (or loss) functions that we could seek to minimize, but we will use the common cost function for ordinary least squares (OLS) regression:\n",
    "\n",
    "$\\epsilon_i^2 = \\left( \\ y_i - f(x_i) \\ \\right)^2$  \n",
    "$cost = \\sum \\epsilon_i^2$\n",
    "\n",
    "Where $f(x_i)$ is defined by our model parameters. We demonstrate this approach visually in the following code block, where we minimize the sum of the _l2-norm_ model residuals, which is done by finding the best model parameters: $\\hat{\\beta}$, and $\\hat{\\alpha}$. \n",
    "\n",
    "**Note:** You are __not__ required to understand the following code. Please pay more attension to the plot and understand how $\\epsilon$ is calculated.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimial Cost (l2 Norm) = 13.76\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Anscombe's Quartet\n",
    "adf = sns.load_dataset(\"anscombe\")\n",
    "\n",
    "# Select only the first data set\n",
    "adfi = adf[adf.dataset == 'I']\n",
    "\n",
    "# Extract the x & y columns as NumPy arrays, \n",
    "# we call them xx and yy to avoid conflict with the tips data\n",
    "xx = adfi.x.as_matrix()\n",
    "yy = adfi.y.as_matrix()\n",
    "\n",
    "# Number of data points\n",
    "n = xx.shape[0]\n",
    "\n",
    "# Determine mean values\n",
    "mux = np.mean(xx)\n",
    "muy = np.mean(yy)\n",
    "\n",
    "# Determine best fit model parameters (from simple linear regression)\n",
    "beta = np.sum((xx - mux) * (yy - muy)) / np.sum((xx - mux)**2)\n",
    "alpha = muy - beta * mux\n",
    "\n",
    "# Plot the data as a Regression Plot\n",
    "ax = sns.regplot(x='x', y='y', data=adfi, fit_reg=False)\n",
    "\n",
    "# Label plot\n",
    "ax.set_xlabel('X', fontsize=18)\n",
    "ax.set_ylabel('Y', fontsize=18)\n",
    "ax.set_title('Regression Plot', fontsize=18)\n",
    "\n",
    "# Compute and draw epsilons\n",
    "fy = beta * xx + alpha\n",
    "ax.vlines(xx, yy, fy)\n",
    "\n",
    "print(f'Minimial Cost (l2 Norm) = {np.sum((fy - yy)**2):5.2f}')\n",
    "\n",
    "# Annotate third point\n",
    "ax.annotate(r'$(x_3, y_3)$', xy=(6, 7), xytext=(5.5, 7.5))\n",
    "\n",
    "ax.annotate(r'$\\epsilon_3$', xy=(6, 6.5), xytext=(4.5, 6.5),\n",
    "            arrowprops=dict(facecolor='black', alpha =0.25, shrink = 0.05))\n",
    "\n",
    "# Annotate tenth point\n",
    "ax.annotate(r'$(x_{10}, y_{10})$', xy=(13, 7.5), xytext=(12.5, 7.0))\n",
    "\n",
    "ax.annotate(r'$\\epsilon_{10}$', xy=(13, 8.75), xytext=(14.5, 8.0),\n",
    "            arrowprops=dict(facecolor='black', alpha =0.25, shrink = 0.05))\n",
    "\n",
    "# Compute and plot linear model\n",
    "xx = np.arange(3, 16)\n",
    "yy = beta * xx + alpha\n",
    "ax.plot(xx, yy)\n",
    "\n",
    "sns.despine(offset = 5, trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[Back to TOC](#Table-of-Contents)\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "This simple example demonstrates a fundamental concept in machine learning, namely the minimization of a cost (or loss) function, which quantifies how well a model represents a data set. For a given data set, the cost function is completely specified by the model, thus a more complex model has a more complex cost function, which can become difficult to minimize. \n",
    "\n",
    "As we move to higher dimensional data sets or more complex cost functions, the challenge of finding the global minimum becomes increasingly difficult. As a result, many mathematical techniques have been developed to find the global minimum of a (potentially) complex function. The standard approach is [gradient descent][wgd], where we use the fact that the first derivative (or gradient) measures the slope of a function at a given point. We can use the slope to infer which direction is _downhill_ and thus travel (hopefully) towards the minimum. \n",
    "\n",
    "A major challenge with this approach is the potential to become stuck in a local and not global minima. Thus, modifications are often added to reduce the likelihood of becoming stuck in a local minimum. One popular example of this approach is known as [stochastic gradient descent][wsgd]. This algorithm employs standard gradient descent, but adds an occasional random jump in the parameter space to reduce the chances of being stuck in a local _valley_. Another, very different, approach to this problem is the use of [genetic algorithms][wga], which employ techniques from evolutionary biology to minimize the cost function.\n",
    "\n",
    "For a mental picture of this process, imagine hiking in the mountains and trying to find the highest peak. We can use gradient ascent in this case. Gradient ascent is similar to finding the local mountain peak and climbing it. This local peak might look like it is the highest, but a random jump away from the local peak might enable one to view higher peaks beyond, which can subsequently be climbed with a new gradient ascent.\n",
    "\n",
    "Whenever you perform machine learning in the future, you should keep in mind that the model that you generate for a given data set has generally resulted from the minimization of a cost function. Thus, there remains the possibility that with more effort, more data, or a better cost minimization strategy, a new, and better model may potentially exist.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "[wgd]: https://en.wikipedia.org/wiki/Gradient_descent\n",
    "[wsgd]: https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "[wga]: https://en.wikipedia.org/wiki/Genetic_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to TOC](#Table-of-Contents)\n",
    "\n",
    "-----\n",
    "\n",
    "## Linear Regression with Scikit Learn\n",
    "\n",
    "------\n",
    "\n",
    "The most commonly used statistical and machine learning library in Python is the [scikit learn][skl] library. This library includes a number of different model fitting techniques (in addition to general regression and classification algorithms) that can be easily applied to a data set. In this notebook, we will focus on the standard linear regression estimator, but much of what you learn in this notebook is applicable to other regressors in the scikit-learn library, since this library employs a standard interface.\n",
    "\n",
    "To perform linear regression with the scikit-learn library, we first create a [`LinearRegression`][sklr] estimator, which is imported from the `linear_model` module in `scikit learn`.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "```\n",
    "  \n",
    "When this estimator is created, the following hyperparameters can be specified (they are all optional):\n",
    "  \n",
    "- `fit_intercept`: If `True`, the _default_, an intercept is fit for this model.\n",
    "- `normalize`: If `True` all the features supplied in the `fit` method will be normalized, the _default_ is `False`.\n",
    "- `copy_X`: If `True`, the _default_, the feature matrix will be copied, otherwise the data may be overwritten during the regression.\n",
    "  \n",
    "This regressor has two commonly used attributes, which can be accessed after the model has been fit to the data (note that model attributes in scikit learn are suffixed by an underscore):\n",
    "\n",
    "- `coef_`: An array of the estimated coefficients for the regressed linear model, in typical usage this is a single dimensional array.\n",
    "- `intercept_`: The constant term in the regressed linear model, only computed if `fit_intercept` is `True`.\n",
    "  \n",
    "Once created, this estimator is _fit_ to the training data, after which it can be used to _predict_ new values from the test data. These two actions, along with a measure of the performance of the regressor, are encapsulated in the following three functions that can be called on a `LinearRegression` estimator:\n",
    "- `fit`: Fits a linear model to the supplied features(both independent and dependent features).\n",
    "- `predict`: Predicts new results from the given model for the new supplied features\n",
    "- `score`: Computes a regression score, specifically the coefficient of determination $R^2$ of the prediction.\n",
    "\n",
    "-----\n",
    "[skl]: http://scikit-learn.org/stable/index.html\n",
    "[sklr]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Before computing a linear regression to the _tips_ data, we must first process these data to  make them ready for the `LinearRegression` estimator. For demonstration, we will choose `total_bill`, `size` and `time` as independent variables. Among them, `total_bill` and `size` are numeric features, while `time` is a categorical feature, which has two values, `Lunch` and `Dinner`. LinearRegression model can only accept numerical features, thus we need to encode categorical features. The scikit-learn library provides several different encodings in the preprocessing module. In this notebook, we will use the patsy module, which has function `dmatrices` that makes creating dependent and independent variables from a dataframe very easy as shown below:\n",
    "\n",
    "```\n",
    "import patsy as pts \n",
    "y, x = pts.dmatrices('tip ~ total_bill + size + C(time)', data=tdf, return_type='dataframe')\n",
    "```\n",
    "\n",
    "`tdf` is the dataframe that holds the tips dataset. The formula `tip ~ total_bill + size + C(time)` indicates that `tip` column is the dependent variable, and `total_bill`, `size` and `time` are independent variables. `time` is categorical feature so it is enclosed in parenthesis and prefixed with a C. `dmatrices` will return y which has values in `tip` column; Encode `time` and return `total_bill`, `size` and encoded `time` as x.\n",
    "\n",
    "The `dmatrices` function takes this formula and the supplied data and returns the aggregated result, which in this case we have requested as two DataFrames: `x` for the independent variables and `y` for the dependent variable. \n",
    "\n",
    "Two important points to notice with `x`, the new independent feature DataFrame. First, patsy automatically created the dummy, or binarized, features for our categorical features. Second, notice that patsy also dropped one option for the categorical feature. This is not a bug, instead it is done to generate more robust results (something we probably should have been doing earlier). If we regress across all possible categorical options, we have introduced [unnecessary correlations][wdf] between our features. That is because if we have $n$ categorical features, we only need $n - 1$ dummy features to fully map all options (if they are all zero, it is the same as saying the $n^{th}$ dummy feature was one). This is known as [multicollinearity][wmc], and can lead to problems with convergence. To avoid this, you should always [drop one dummy feature][wmcr]. \n",
    "\n",
    "We have, for example, explicitly excluded using `time=Lunch` as a predictor. Thus, _Lunch_ is our reference category, for the `day` categorical features. Later, when we quantify the impact of one feature on the prediction, the measurements will be with respect to this reference category. An `Intercept` column with value 1.0 is created for all reference categories.\n",
    "\n",
    "Next, we divide the data and labels into training:testing samples with a 60:40 split.\n",
    "\n",
    "-----\n",
    "[wmc]: https://en.wikipedia.org/wiki/Multicollinearity\n",
    "[wmcr]: https://en.wikipedia.org/wiki/Multicollinearity#Remedies_for_multicollinearity\n",
    "\n",
    "[wdf]: https://en.wikipedia.org/wiki/Dummy_variable_(statistics)#Interactions_among_dummy_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy as pts \n",
    "\n",
    "# Create dependent and independent variables\n",
    "y, x = pts.dmatrices('tip ~ total_bill + size + C(time)', data=tdf, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>C(time)[T.Dinner]</th>\n",
       "      <th>total_bill</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.83</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.56</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.52</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.32</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.95</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Intercept  C(time)[T.Dinner]  total_bill  size\n",
       "85         1.0                0.0       34.83   4.0\n",
       "54         1.0                1.0       25.56   4.0\n",
       "126        1.0                0.0        8.52   2.0\n",
       "93         1.0                1.0       16.32   2.0\n",
       "113        1.0                1.0       23.95   2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sample(5, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we will split the dataset to training and testing. We set `random_state` to ensure repeatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ind_train, ind_test, dep_train, dep_test = train_test_split(x, y, test_size=0.4, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "With these data, we can now employ the scikit-learn library’s linear regression estimator to generate a linear model for these data. In the following code cells, we use the `LinearRegression` estimator to fit our sample data, plot the results, and finally display the fit coefficients.\n",
    "\n",
    "After the model is trained, we can get coefficients from `model.coef_`. As a result, we display the regression formula with the coefficients. We also print the regression score which is $R^2$ of the linear regression.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tip = 0.83 + -0.23 Dinner + 0.09 total_bill + 0.22 size\n",
      "LR Model score =  44.2%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create and fit our linear regression model to training data\n",
    "model = LinearRegression()\n",
    "model.fit(ind_train, dep_train)\n",
    "\n",
    "# Display model fit parameters for training data\n",
    "print(f\"tip = {model.intercept_[0]:4.2f} + \" + \\\n",
    "      f\"{model.coef_[0][1]:4.2f} Dinner + \" + \\\n",
    "      f\"{model.coef_[0][2]:4.2f} total_bill + \" + \n",
    "      f\"{model.coef_[0][3]:4.2f} size\")\n",
    "\n",
    "# Compute model predictions for test data\n",
    "results = model.predict(ind_test)\n",
    "\n",
    "# Compute score and display result (Coefficient of Determination)\n",
    "score = 100.0 * model.score(ind_test, dep_test)\n",
    "print(f'LR Model score = {score:5.1f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In above code, there are several things need more discussion:\n",
    "1. We create `LinearRegression` Model by accepting default values for all hypterparameter. So `fit_intercept` is True, which means the model will calculate intercept. This makes the 'Intercept' column created by `dmatrices` useless. If you check the model's first coefficient by using `model.coef_[0][0]`, you will get 0. That's why we don't need the first coefficient to construct the linear formula. In the next code cell, we create `LinearRegression` model with `fit_intercept` set to False. This time the trained model will not have attribute `intercept_`.  We then use the first coefficient as the intercept. Both approaches give same result.\n",
    "2. `predict` function is normally used to predict on data without known output. `ind_test` has known output `dep_test`. So it's not necessary to predict with `ind_test`. We put it here just to demonstrate how to use `predict` function.\n",
    "3. `score` function takes 2 arguments, `ind_test` and `dep_test`. It will first predict on `ind_test` with the trained model, then compare the predicted result with the groud truth `dep_test`, then calculate $R^2$ value.\n",
    "4. $R^2$ is one of the regression evaluation metrics. The value of $R^2$ is normally between 0 to 1. The larger the $R^2$, the more accurate the regression model is. We will discuss regression evaluation metrics in more detail in future lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tip = 0.83 + -0.23 Dinner + 0.09 total_bill + 0.22 size\n",
      "LR Model score =  44.2%\n"
     ]
    }
   ],
   "source": [
    "# Create model without fit_intercept\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(ind_train, dep_train)\n",
    "\n",
    "# Display model fit parameters for training data\n",
    "print(f\"tip = {model.coef_[0][0]:4.2f} + \" + \\\n",
    "      f\"{model.coef_[0][1]:4.2f} Dinner + \" + \\\n",
    "      f\"{model.coef_[0][2]:4.2f} total_bill + \" + \n",
    "      f\"{model.coef_[0][3]:4.2f} size\")\n",
    "\n",
    "# Compute score and display result (Coefficient of Determination)\n",
    "score = 100.0 * model.score(ind_test, dep_test)\n",
    "print(f'LR Model score = {score:5.1f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we used multi-variate linear regression to predict the `tip` feature from the `total_bill`, `size` and categorical `time` features. In the empty **Code** cell below, repeat this process, but use the `total_bill`, `size`, `sex`, and `time` features. Has the prediction performance improved?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "[Back to TOC](#Table-of-Contents)\n",
    "\n",
    "## Linear Regression with Statsmodels\n",
    "\n",
    "Scikit-learn is the dominant Python machine learning library, and will be the primary library we use to generate models from data. However, an alternative library, [_statsmodels_][sm] is being developed that focuses more on statistical analyses, along with easier integration with the Pandas DataFrame. To demonstrate this new library and how it can be used to perform linear regression, we demonstrate the statsmodels library in the next few code cells to perform linear regression on the _tips_ data. \n",
    "\n",
    "Given an appropriate DataFrame, in this case the _tips_ DataFrame, we can easily employ the formulaic interface to obtain an ordinary least squares fit to the data of interest. The formulaic interface is borrowed from the `R` programming language (which drives much of the development of the statsmodels library), and simply relates the dependent variable to the independent variables (or features).\n",
    "\n",
    "In this example, we map the `tip` label to the `total_bill`, `size` and `time` features, this will include the constant term in the calculation. At the end we call the `summary` method, which generates a detailed report showing the fit parameters, and a number of performance metrics.\n",
    "\n",
    "Notice that the absolute of `t` value of T.Dinner's coefficient is less than 2, which means the coefficient is not significant, or time of the meal has no impact on tip.\n",
    "\n",
    "**Note:** LinearRegression in sckit learn doesn't return coefficient significance directly, you will need to calculate them from the model's attributes.\n",
    "\n",
    "-----\n",
    "\n",
    "[sm]: http://statsmodels.sourceforge.net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tip = 0.67 + -0.00 T.Dinner + 0.09 total_bill + 0.19 size\n",
      "Regression Score: 0.47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Implement simple regression: Result ~ Input\n",
    "\n",
    "# First we fit slope and intercept\n",
    "result = smf.ols(formula='tip ~ total_bill + size + C(time)', data=tdf).fit()\n",
    "\n",
    "print(f\"tip = {result.params[0]:4.2f} + {result.params[1]:4.2f} T.Dinner + {result.params[2]:4.2f} total_bill + {result.params[3]:4.2f} size\") \n",
    "print(f'Regression Score: {result.rsquared:4.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>tip</td>       <th>  R-squared:         </th> <td>   0.468</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.461</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   70.34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 14 Oct 2019</td> <th>  Prob (F-statistic):</th> <td>1.13e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:06:37</td>     <th>  Log-Likelihood:    </th> <td> -347.98</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   244</td>      <th>  AIC:               </th> <td>   704.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   240</td>      <th>  BIC:               </th> <td>   718.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>         <td>    0.6712</td> <td>    0.210</td> <td>    3.197</td> <td> 0.002</td> <td>    0.258</td> <td>    1.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(time)[T.Dinner]</th> <td>   -0.0041</td> <td>    0.148</td> <td>   -0.028</td> <td> 0.978</td> <td>   -0.295</td> <td>    0.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>total_bill</th>        <td>    0.0928</td> <td>    0.009</td> <td>   10.037</td> <td> 0.000</td> <td>    0.075</td> <td>    0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>size</th>              <td>    0.1926</td> <td>    0.085</td> <td>    2.253</td> <td> 0.025</td> <td>    0.024</td> <td>    0.361</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>24.790</td> <th>  Durbin-Watson:     </th> <td>   2.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  46.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.546</td> <th>  Prob(JB):          </th> <td>9.17e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.831</td> <th>  Cond. No.          </th> <td>    76.0</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    tip   R-squared:                       0.468\n",
       "Model:                            OLS   Adj. R-squared:                  0.461\n",
       "Method:                 Least Squares   F-statistic:                     70.34\n",
       "Date:                Mon, 14 Oct 2019   Prob (F-statistic):           1.13e-32\n",
       "Time:                        10:06:37   Log-Likelihood:                -347.98\n",
       "No. Observations:                 244   AIC:                             704.0\n",
       "Df Residuals:                     240   BIC:                             718.0\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "Intercept             0.6712      0.210      3.197      0.002       0.258       1.085\n",
       "C(time)[T.Dinner]    -0.0041      0.148     -0.028      0.978      -0.295       0.286\n",
       "total_bill            0.0928      0.009     10.037      0.000       0.075       0.111\n",
       "size                  0.1926      0.085      2.253      0.025       0.024       0.361\n",
       "==============================================================================\n",
       "Omnibus:                       24.790   Durbin-Watson:                   2.100\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               46.226\n",
       "Skew:                           0.546   Prob(JB):                     9.17e-11\n",
       "Kurtosis:                       4.831   Cond. No.                         76.0\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display model fit and performance metrics\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. The Wikipedia article on [linear regression][1]\n",
    "2. A Python-based [tutorial on linear regression][tlr] that focuses on the statsmodels library\n",
    "3. A blog article using [linear regression to predict housing prices][5] in Python\n",
    "\n",
    "\n",
    "-----\n",
    "[1]: https://en.wikipedia.org/wiki/Linear_regression\n",
    "[tlr]: https://github.com/justmarkham/DAT4/blob/master/notebooks/08_linear_regression.ipynb\n",
    "\n",
    "[5]: http://www.learndatasci.com/predicting-housing-prices-linear-regression-using-python-pandas-statsmodels/?imm_mid=0eddcf&cmp=em-data-na-na-newsltr_20170301\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2019: Gies College of Business at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
