{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Decision Tree\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we introduce the [Decision Tree algorithm][wdt], and demonstrate how to effectively use this algorithm for both classification and regression problems. The Decision Tree algorithm is a simple algorithm that is easy to understand, since a higher level representation of the data is iteratively constructed from the data. A decision tree provides a powerful, predictive model that can capture non-linear effects while also being easy to understand and explain. \n",
    "\n",
    "In this notebook, we first explore the basic formalism of the decision tree algorithm, including a discussion on several important concepts that can be used to determine how the tree is constructed from a data set. Next, we introduce the use of the decision tree for classification problems by using the Iris data set. In this section we will examine feature importance, ~~the decision surface,~~ visualizing the predictive tree, and the effect of different hyperparameters, before switching to a more complex data set. Finally, we will look at constructing a decision tree for regression, by using a new data set.\n",
    "\n",
    "-----\n",
    "[wdt]: https://en.wikipedia.org/wiki/Decision_tree_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major advantage of using decision trees is that they are intuitively very easy to explain. They closely mirror human decision-making compared to other regression and classification approaches. They can be displayed graphically, and they can easily handle qualitative predictors without the need to create dummy variables.\n",
    "\n",
    "Another huge advantage is such the algorithms are completely invariant to scaling of the data. As each feature is processed separately, and the possible splits of the data don’t depend on scaling, no pre-processing like normalization or standardization of features is needed for decision tree algorithms. In particular, decision trees work well when we have features that are on completely different scales, or a mix of binary and continuous features.\n",
    "\n",
    "However, decision trees generally do not have the same level of predictive accuracy as other approaches, since they aren’t quite robust. A small change in the data can cause a large change in the final estimated tree. Even with the use of pre-pruning, they tend to overfit and provide poor generalization performance. Therefore, in most applications, by aggregating many decision trees, using methods like bagging, random forests, and boosting, the predictive performance of decision trees can be substantially improved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Formalism](#Formalism)\n",
    "\n",
    "[Decision Tree: Classification](#Decision-Tree:-Classification)\n",
    "\n",
    "- [Classification: Iris Data](#Classification:-Iris-Data)\n",
    "- [Decision Tree: Decision Surface](#Decision-Tree:-Decision-Surface)\n",
    "- [Decision Tree: Hyperparameters](#Decision-Tree:-Hyperparameters)\n",
    "- [Decision Tree: Feature Importance](#Decision-Tree:-Feature-Importance)\n",
    "- [Decision Tree: Visualizing the Tree](#Decision-Tree:-Visualizing-the-Tree)\n",
    "- [Classification: Adult Data](#Classification:-Adult-Data)\n",
    "\n",
    "[Decision Tree: Regression](#Decision-Tree:-Regression)\n",
    "\n",
    "- [Regression: Auto MPG Data](#Regression:-Auto-MPG-Data)\n",
    "- [Regression Performance Metrics](#Regression-Performance-Metrics)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the _Formalism_ section of this Notebook, we first have our standard notebook setup code.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Formalism\n",
    "\n",
    "One of the simplest machine learning algorithms to understand is the [decision tree][wdt]. For a classification task, a decision tree asks a set of questions of the data, and based on the answers determines the final classification. The tree is constructed by recursively splitting a data set into new groupings based on a statistical measure of the data along each different dimension. Popular metrics to determine the dimension on which to split and the value at which to split include entropy, information gain, Gini coefficient, and variance reduction. The terminal nodes in the tree are known as leaf nodes, and provide the final predictions. In the simplest form, the leaf node simply provides the final answer; however, the values in the leaf node can also be combined to form a probabilistic classification or regression estimate.\n",
    "\n",
    "In addition to their simplicity, decision trees have a number of other benefits. First, they are a _white box model_, which simply means we can understand exactly why a decision tree makes a specific prediction on a given instance. Second, they can handle both numerical and categorical data, and they do not require pre-processing beyond handling missing values. Trees also tend to perform well on large data sets. \n",
    "\n",
    "On the other hand, decision trees are prone to **overfitting**, where they model the training data too well and do not generalize to unseen data. Decision trees can have difficulty classifying on unbalanced classes and they can be unstable to minor changes in the training data. Overall, however, the decision tree is one of a handful of standard machine learning algorithms with which you should be familiar. In future notebooks, we will learn how to overcome many of these disadvantages by employing ensemble learning with decision trees. \n",
    "\n",
    "The following figure shows a simple decision tree. In the image below, the square text box represents a condition, based on which the tree splits into branches. The end of the branch represented by oval text box that doesn’t split anymore is the leaf, or decision. In this case, whether one should wear sunglasses or not.\n",
    "\n",
    "<img src=\"images/dt_sunglasses.png\">\n",
    "   \n",
    "The decision on which feature to split, and the actual value along that feature on which to split can be performed in several different manners:\n",
    "- [Variance reduction][wvr]: the split choice is made to maximally reduce the variance along a feature, useful for regression problems.\n",
    "- [Gini impurity][wgi]: the split choice is made to minimize misclassifications, especially in a multi-class classification domain.\n",
    "- [Information gain][wig]: the split choice is selected to create the purest child nodes, based on the concept of entropy and information theory.\n",
    "\n",
    "The detail of spliting algorithms is out of the scope of this course.\n",
    "\n",
    "-----\n",
    "\n",
    "[wdt]: https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "\n",
    "[wvr]: https://en.wikipedia.org/wiki/Decision_tree_learning#Variance_reduction\n",
    "[wig]: https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain\n",
    "[wgi]: https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Decision Tree: Classification\n",
    "\n",
    "To apply the decision tree algorithm to classification tasks we will use the `DecisionTreeClassifier` estimator from the scikit-learn `tree` module. This estimator will construct, by default, a tree from a training data set. This estimator accepts a number of hyperparameters, including:\n",
    "\n",
    "- `criterion` : the method by which to measure the quality of a potential split. By default the Gini impurity is used, although information gain can be specified by passing the string `entropy`.\n",
    "- `max_depth` : the maximum depth of the tree. By default this is `None`, which means the tree is constructed until either all leaf nodes are pure, or all leaf nodes contain fewer instances than the `min_samples_split` hyperparameter value.\n",
    "- `min_samples_split` : the minimum number of instances required to split a node into two child nodes, by default this is two.\n",
    "- `min_samples_leaf`: the minimum number of instances required to make a node terminal (i.e., a leaf node). By default this value is one.\n",
    "- `max_features`: the number of features to examine when choosing the best split feature and value. By default this is `None`, which means all features will be explored.\n",
    "- `random_state`: the seed for the random number generator used by this estimator. Setting this value ensures reproducibility.\n",
    "- `class_weight`: values that can improve classification performance on unbalanced data sets. By default this value is `None`.\n",
    "\n",
    "Run `help(DecisionTreeClassifier)` to view more details about the model and the hyper parameters.\n",
    "\n",
    "To demonstrate using a decision tree with the scikit-learn library, we will first load in the Iris data. With these data, we will construct a simple decision tree to introduce the concept of _feature importance_. Next, we will explore how to visualize the decision tree constructed by the `DecisionTreeClassifier` estimator. Finally, we will switch to a larger data set to learn how to employ a decision tree on more complex data.\n",
    "\n",
    "----\n",
    "[skdtc]: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Classification: Iris Data\n",
    "\n",
    "We can now apply the Decision Tree algorithm to the Iris data to create a classification model. The basic approach is simple, and follows the standard scikit-learn estimator philosophy:\n",
    "\n",
    "1. Load Iris data\n",
    "2. Encode species column to create a numeric label column.\n",
    "3. split the date into training and testing sets.\n",
    "4. Import our estimator, [`DecisionTreeClassifier`][skdtc], from the proper scikit-learn module, `tree`.\n",
    "5. Create the estimator and specify the appropriate hyperparameters. For a decision tree, we can accept the defaults, or specify values for specific hyperparameters, such as `max_depth`.\n",
    "6. Fit the model to the training data.\n",
    "7. Predict new classes with our trained model and generate a performance metric.\n",
    "\n",
    "These steps are demonstrated in the following code cell.\n",
    "\n",
    "We will also plot a confusion matrix for the model using helper code. The helper code defines `confusion()` function. You may find the source code of `confusion()` in previous lesson notebook.\n",
    "\n",
    "-----\n",
    "[skdtc]: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width     species\n",
       "66            5.6          3.0           4.5          1.5  versicolor\n",
       "123           6.3          2.7           4.9          1.8   virginica\n",
       "25            5.0          3.0           1.6          0.2      setosa\n",
       "115           6.4          3.2           5.3          2.3   virginica\n",
       "78            6.0          2.9           4.5          1.5  versicolor"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = sns.load_dataset('iris')\n",
    "iris_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "      <th>species_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width     species  \\\n",
       "14            5.8          4.0           1.2          0.2      setosa   \n",
       "98            5.1          2.5           3.0          1.1  versicolor   \n",
       "75            6.6          3.0           4.4          1.4  versicolor   \n",
       "16            5.4          3.9           1.3          0.4      setosa   \n",
       "131           7.9          3.8           6.4          2.0   virginica   \n",
       "\n",
       "     species_cat  \n",
       "14             0  \n",
       "98             1  \n",
       "75             1  \n",
       "16             0  \n",
       "131            2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#create new column to hold encoded species\n",
    "iris_df['species_cat'] = LabelEncoder().fit_transform(iris_df.species)\n",
    "iris_df.sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 4), (60, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Define data and label\n",
    "data = iris_df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "label = iris_df['species_cat']\n",
    "\n",
    "# Split data into training and testing\n",
    "# Note that we have both 'data' and 'label'\n",
    "d_train, d_test, l_train, l_test = train_test_split(data, label, test_size=0.4, random_state=23)\n",
    "d_train.shape, d_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    32\n",
       "2    29\n",
       "0    29\n",
       "Name: species_cat, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check training label balance\n",
    "l_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    21\n",
       "0    21\n",
       "1    18\n",
       "Name: species_cat, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check testing label balance\n",
    "l_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree prediction accuracy = 96.7%\n"
     ]
    }
   ],
   "source": [
    "# Next lets try Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# First we construct our decision tree, we only specify the \n",
    "# random_state hyperparameter to ensure reproduceability.\n",
    "dtc = DecisionTreeClassifier(random_state=23)\n",
    "\n",
    "# Fit estimator to scaled training data\n",
    "dtc = dtc.fit(d_train, l_train)\n",
    "\n",
    "# Compute and display accuracy score\n",
    "score = 100.0 * dtc.score(d_test, l_test)\n",
    "print(f\"Decision Tree prediction accuracy = {score:4.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Notice that, even when using only 60% of the total data for training, our decision tree classifier still achieves remarkable accuracy on the testing data. For completeness, we also display the classification report and the confusion matrix in the following two Code cells. The per-class precision and recall are very good, with a minor issue in the prediction of class `Virginica`, which is also demonstrated clearly in the confusion matrix.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Setosa       1.00      1.00      1.00        21\n",
      "  Versicolor       0.90      1.00      0.95        18\n",
      "   Virginica       1.00      0.90      0.95        21\n",
      "\n",
      "   micro avg       0.97      0.97      0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "[[21  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0  2 19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Thre types of Iris in data set\n",
    "labels = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "# Predict on test data and report scores\n",
    "y_pred = dtc.predict(d_test)\n",
    "print(classification_report(l_test, y_pred, target_names = labels))\n",
    "print(confusion_matrix(l_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper_code import mlplots as ml\n",
    "\n",
    "# Call confusion matrix plotting routine\n",
    "ml.confusion(l_test, y_pred, labels, 'Decision Tree Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Decision Tree: Feature Importance\n",
    "\n",
    "As the previous example demonstrated, the decision tree can often provide impressive performance rather easily. We can, however, leverage the fact that the decision tree is constructed by repeatedly determining the most important feature on which to split the data to compute the relative importance of each feature in the training data set. In effect, this is computed by determining to what percentage each feature was used to split the training data, formally this is known as the _Gini importance_. As a result, higher values indicate a more important feature. \n",
    "\n",
    "We demonstrate how to extract the feature importance for a decision tree classifier in the following Code cell, where we see that for this training data set, two features: Petal Width and Petal Length account for most of the importance.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sepal Length importance =  1.67%\n",
      "Sepal Width importance =  2.22%\n",
      "Petal Length importance =  5.59%\n",
      "Petal Width importance = 90.51%\n"
     ]
    }
   ],
   "source": [
    "# Display feature importance as computed from the decision tree\n",
    "\n",
    "# Feature names\n",
    "feature_names = ['Sepal Length', 'Sepal Width', \n",
    "                 'Petal Length', 'Petal Width']\n",
    "\n",
    "# Display name and importance\n",
    "for name, val in zip(feature_names, dtc.feature_importances_):\n",
    "    print(f'{name} importance = {100.0*val:5.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Decision Tree: Visualizing the Tree\n",
    "\n",
    "A decision tree is one of the easiest algorithms to understand since a decision tree essentially asks a list of questions to partition the data. The scikit learn library includes an [`export_graphviz`][sket] method that actually generates a visual tree representation of a constructed decision tree classifier. This representation is in the [`dot`][dot] format recognized by the standard, open source [`graphviz`][gv] library. \n",
    "\n",
    "In the next few Code cells, we export a _dot_ format of the Iris decision tree we just constructed, convert it to an SVG image, and subsequently display this image inline in our notebook. Note, if the _graphviz_ module was available, we could generate and display the decision tree directly in our notebook as demonstrated in the comments of the third Code cell below.\n",
    "\n",
    "-----\n",
    "[sket]: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz\n",
    "[dot]: http://www.graphviz.org/doc/info/lang.html\n",
    "[gv]:http://www.graphviz.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we construct our a shallow decision tree, this is\n",
    "# simply a demonstration used to show feature improtance\n",
    "# and how to view a tree, hence we need a shallow tree\n",
    "dtc = DecisionTreeClassifier(max_depth=3, random_state=23)\n",
    "\n",
    "dtc = dtc.fit(d_train, l_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Note, this decision tree classifier is different than our original decision tree classifier since we set the `max_depth` hyperparameter to three. We display the feature importance for this new tree to compare to our original feature importance. In this case, we see that the two features we identified before are exclusively used to construct the new tree. Thus, we can infer that the original tree first used these two features to split the training data, before using the other two features to make leaf nodes.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sepal Length importance =  0.00%\n",
      "Sepal Width importance =  0.00%\n",
      "Petal Length importance =  4.82%\n",
      "Petal Width importance = 95.18%\n"
     ]
    }
   ],
   "source": [
    "# Display feature importance as computed from the decision tree\n",
    "\n",
    "# Feature names\n",
    "feature_names = ['Sepal Length', 'Sepal Width', \n",
    "                 'Petal Length', 'Petal Width']\n",
    "\n",
    "# Display name and importance\n",
    "for name, val in zip(feature_names, dtc.feature_importances_):\n",
    "    print(f'{name} importance = {100.0*val:5.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "the version of scikit learn we can use in this class currently limits our ability to control the appearance of the tree. If you have [graphivz](https://pypi.org/project/graphviz/) installed, you can simply perform the following steps to create and visualize a tree in a Jupyter notebook:\n",
    "\n",
    "```\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "tree_data = export_graphviz(dtc, out_file=None, feature_names=feature_names) \n",
    "my_tree = graphviz.Source(tree_data) \n",
    "my_tree \n",
    "```\n",
    "\n",
    "In this notebook, we will simply display the image of the tree.\n",
    "\n",
    "We now display the generated tree visualization, and see that, as expected, the Petal Width feature is used for the primary splits, indicating it is more important, while the Petal length feature is used to split into leaf nodes. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"373pt\" viewBox=\"0.00 0.00 515.00 373.00\" width=\"515pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 369)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-369 511,-369 511,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 0 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" points=\"246,-365 119,-365 119,-297 246,-297 246,-365\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182.5\" y=\"-349.8\">Petal Width &lt;= 0.8</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182.5\" y=\"-334.8\">gini = 0.666</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182.5\" y=\"-319.8\">samples = 90</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182.5\" y=\"-304.8\">value = [29, 32, 29]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" points=\"169,-253.5 56,-253.5 56,-200.5 169,-200.5 169,-253.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112.5\" y=\"-238.3\">gini = 0.0</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112.5\" y=\"-223.3\">samples = 29</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112.5\" y=\"-208.3\">value = [29, 0, 0]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>0-&gt;1</title>\n",
       "<path d=\"M159.5794,-296.9465C152.0172,-285.7113 143.5976,-273.2021 135.9933,-261.9043\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"138.8523,-259.8837 130.3649,-253.5422 133.0452,-263.7924 138.8523,-259.8837\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"125.5705\" y=\"-274.3781\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" points=\"317.5,-261 187.5,-261 187.5,-193 317.5,-193 317.5,-261\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.5\" y=\"-245.8\">Petal Width &lt;= 1.75</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.5\" y=\"-230.8\">gini = 0.499</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.5\" y=\"-215.8\">samples = 61</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252.5\" y=\"-200.8\">value = [0, 32, 29]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>0-&gt;2</title>\n",
       "<path d=\"M205.4206,-296.9465C211.2826,-288.2373 217.6598,-278.7626 223.773,-269.6801\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"226.8476,-271.3802 229.5279,-261.13 221.0405,-267.4716 226.8476,-271.3802\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"234.3223\" y=\"-281.966\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" points=\"244,-157 109,-157 109,-89 244,-89 244,-157\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176.5\" y=\"-141.8\">Petal Length &lt;= 5.45</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176.5\" y=\"-126.8\">gini = 0.114</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176.5\" y=\"-111.8\">samples = 33</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176.5\" y=\"-96.8\">value = [0, 31, 2]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>2-&gt;3</title>\n",
       "<path d=\"M227.6148,-192.9465C221.1847,-184.1475 214.1837,-174.5672 207.4841,-165.3993\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"210.1672,-163.1389 201.4412,-157.13 204.5155,-167.269 210.1672,-163.1389\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"none\" points=\"397,-157 262,-157 262,-89 397,-89 397,-157\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329.5\" y=\"-141.8\">Petal Length &lt;= 4.85</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329.5\" y=\"-126.8\">gini = 0.069</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329.5\" y=\"-111.8\">samples = 28</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329.5\" y=\"-96.8\">value = [0, 1, 27]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>2-&gt;6</title>\n",
       "<path d=\"M277.7127,-192.9465C284.2273,-184.1475 291.3205,-174.5672 298.1082,-165.3993\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"301.0931,-167.2496 304.2307,-157.13 295.4673,-163.0843 301.0931,-167.2496\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" points=\"113,-53 0,-53 0,0 113,0 113,-53\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"56.5\" y=\"-37.8\">gini = 0.061</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"56.5\" y=\"-22.8\">samples = 32</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"56.5\" y=\"-7.8\">value = [0, 31, 1]</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>3-&gt;4</title>\n",
       "<path d=\"M134.1925,-88.9777C122.3501,-79.4545 109.4979,-69.1191 97.7195,-59.6473\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"99.7486,-56.7877 89.7624,-53.2485 95.3618,-62.2427 99.7486,-56.7877\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"none\" points=\"237.5,-53 131.5,-53 131.5,0 237.5,0 237.5,-53\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"184.5\" y=\"-37.8\">gini = 0.0</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"184.5\" y=\"-22.8\">samples = 1</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"184.5\" y=\"-7.8\">value = [0, 0, 1]</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>3-&gt;5</title>\n",
       "<path d=\"M179.3205,-88.9777C180.0113,-80.6449 180.7537,-71.6903 181.4545,-63.2364\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"184.9443,-63.5035 182.2825,-53.2485 177.9682,-62.9251 184.9443,-63.5035\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"none\" points=\"375.5,-53 269.5,-53 269.5,0 375.5,0 375.5,-53\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-37.8\">gini = 0.5</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-22.8\">samples = 2</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-7.8\">value = [0, 1, 1]</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>6-&gt;7</title>\n",
       "<path d=\"M327.0321,-88.9777C326.4276,-80.6449 325.778,-71.6903 325.1648,-63.2364\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"328.6547,-62.969 324.4403,-53.2485 321.6731,-63.4755 328.6547,-62.969\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"none\" points=\"507,-53 394,-53 394,0 507,0 507,-53\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450.5\" y=\"-37.8\">gini = 0.0</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450.5\" y=\"-22.8\">samples = 26</text>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450.5\" y=\"-7.8\">value = [0, 0, 26]</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;8 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>6-&gt;8</title>\n",
       "<path d=\"M372.1601,-88.9777C384.1011,-79.4545 397.0605,-69.1191 408.937,-59.6473\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"411.3246,-62.22 416.9605,-53.2485 406.96,-56.7473 411.3246,-62.22\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now display the image inline\n",
    "from IPython.display import SVG\n",
    "SVG(filename='tree.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the previous Code cell, we constructed a decision tree with `max_depth` hyperparameter equals to 3. In the previous Code cell, try making the following changes and think about what you can learn from the outcome.\n",
    "\n",
    "1. Change the values of `max_depth` to 4, and 8.\n",
    "2. Try using a different hyperparameter, such as changing the `criterion` to indicate information gain should be used to make the splits.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Classification: Adult Data\n",
    "\n",
    "We now turn to a more complex data set with which to perform classification by using a decision tree. We will use the [Adult Income Dataset][uciad] introduced in the previous lesson notebook. We will choose `Age`, `HoursPerWeek`, `CapitalGain` and `Sex` as our training data to predict income level as we did in previous lesson.\n",
    "\n",
    "In the following two Code cells, we first prepare data. Note that decision tree can handle categorical feature directly so we don't need to create dummy features for categorical features. But we still need to encode the categorical features if the values are string. We will use LabelEncoder to encode the `Sex` column.\n",
    "\n",
    "-----\n",
    "[uciad]: https://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    24720\n",
      "1     7841\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>HoursPerWeek</th>\n",
       "      <th>CapitalGain</th>\n",
       "      <th>Sex_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30314</th>\n",
       "      <td>59</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32251</th>\n",
       "      <td>49</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27092</th>\n",
       "      <td>37</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20011</th>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13259</th>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  HoursPerWeek  CapitalGain  Sex_code\n",
       "30314   59            40            0         1\n",
       "32251   49            40            0         1\n",
       "27092   37            45            0         1\n",
       "20011   35            40            0         1\n",
       "13259   41            30            0         1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = \"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "col_names = ['Age', 'Workclass', 'FNLWGT', 'Education', \n",
    "             'EducationLevel', 'MaritalStatus', 'Occupation', \n",
    "             'Relationship', 'Race', 'Sex', 'CapitalGain', 'CapitalLoss', \n",
    "             'HoursPerWeek', 'NativeCountry', 'Salary']\n",
    "\n",
    "# Read CSV data from URL return Pandas\n",
    "adult_data = pd.read_csv(data_file, index_col=False, names = col_names)\n",
    "\n",
    "# Create label column, one for >50K, zero otherwise.\n",
    "adult_data['Label'] = adult_data['Salary'].map(lambda x : 1 if '>50K' in x else 0)\n",
    "\n",
    "# Encode Sex column to numerical value\n",
    "adult_data['Sex_code'] = LabelEncoder().fit_transform(adult_data.Sex)\n",
    "\n",
    "data = adult_data[['Age', 'HoursPerWeek', 'CapitalGain', 'Sex_code']]\n",
    "label = adult_data['Label']\n",
    "\n",
    "#display label class count\n",
    "print(label.value_counts())\n",
    "# Display random sample\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "With our feature and label data prepared, we are now ready to begin the machine learning process. In the following two Code cells we first create our decision tree classifier, and then measure its performance on our testing data. \n",
    "\n",
    "In the first Code cell, we start by splitting our data into training and testing samples. Since we have over 30,000 instances in our data set, our standard 60%:40% split should be sufficient. Next, we create the `DecisionTreeClassifier` estimator. The only hyperparameter that we specify at this time is  `random_state` in order to ensure reproducibility. Next, we fit this estimator to our training data, and generate an accuracy score on our test data. \n",
    "\n",
    "In the second Code cell, we compute and display a simple accuracy score before generating and displaying the full classification report. Note that even with this simple approach, our decision tree classifier performs well, easily beating the zero model demonstrated earlier. In addition, the report indicates that our model performs worst in predicting the positive class. Specifically, the recall indicates that we incorrectly label positive targets as negative. This means that our classifier incorrectly labels individuals who do earn a high salary as being in the low salary category. This could prove problematic, for example, if we are seeking to target high wage earners in a marketing campaign.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "d_train, d_test, l_train, l_test = train_test_split(data, label, test_size=0.4, random_state=23)\n",
    "\n",
    "adult_model = DecisionTreeClassifier(random_state=23)\n",
    "\n",
    "adult_model = adult_model.fit(d_train, l_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification [Adult Data] Score = 79.7%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.88      9894\n",
      "           1       0.64      0.36      0.46      3131\n",
      "\n",
      "   micro avg       0.80      0.80      0.80     13025\n",
      "   macro avg       0.73      0.65      0.67     13025\n",
      "weighted avg       0.78      0.80      0.77     13025\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Classify test data and display score and report\n",
    "predicted = adult_model.predict(d_test)\n",
    "score = 100.0 * metrics.accuracy_score(l_test, predicted)\n",
    "print(f'Decision Tree Classification [Adult Data] Score = {score:4.1f}%\\n')\n",
    "print('Classification Report:\\n {0}\\n'.format(\n",
    "    metrics.classification_report(l_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ml.confusion(l_test, predicted, ['low', 'high'], title='Decision Tree Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CapitalGain</td>\n",
       "      <td>0.399104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.328908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HoursPerWeek</td>\n",
       "      <td>0.184966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sex_code</td>\n",
       "      <td>0.087022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Feature  Importance\n",
       "2   CapitalGain    0.399104\n",
       "0           Age    0.328908\n",
       "1  HoursPerWeek    0.184966\n",
       "3      Sex_code    0.087022"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame(list(zip(d_train.columns, adult_model.feature_importances_)), columns=['Feature', 'Importance'])\n",
    "feature_importance.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree classifier achieved a little better accuracy rate than that of logistic regression in previous lesson. The model has better recall rate on high income prediction, which means it identified more high income correctly than the logistic regression model.\n",
    "\n",
    "Decision tree model has feature importances attribute, which shows the impact of each feature on the model nicely. This is one way to help us select features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the previous Code cells, we constructed a decision tree for classification and applied it to the adult income prediction task. The initial result was reasonable, but try making the following changes to see if you can do better.\n",
    "\n",
    "1. Change the features used in the classification, for example add one or more columns such as Relationship and Education. Do the results change? \n",
    "3. Try using different hyperparameter values, such as specifying to use information gain at tree splits (via the `criterion` hyperparameter) or try using a random `splitter`.\n",
    "4. Try setting the `class_weight` hyperparameter to `balanced` to aid in dealing with the unbalanced training data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Decision Tree: Regression\n",
    "\n",
    "A decision tree can also be used to perform regression, however, in this case the goal is to create leaf nodes that contain data that are nearby in the overall feature space. To predict a continuous value from a decision tree, we either have leaf nodes with only one feature, and use the relevant feature from that instance as our predictor, or we compute summary statistics from the instances in the appropriate leaf node, such as the mean or mode.\n",
    "\n",
    "To perform regression with the scikit-learn library we employ the [`DecisionTreeRegressor`][skdtr] estimator in the tree module. This estimator employs the same set of hyperparameters as the `DecisionTreeClassifier` estimator, and is, therefore, used in a similar manner. One other point, which is also true for classification, by specifying the `random_state` hyperparameter, we ensure reproducibility. This is because every time a tree is constructed, the features are always randomly  permuted at every split. Thus, even if we use the same set of hyperparameters and the same set of training data, we can end up with different trees if the `random_state` hyperparameter is not fixed.\n",
    "\n",
    "In this section we employ decision trees to perform regression on a new data set, the automotive fuel performance prediction. First, we will introduce these data, and prepare them for the regression task. We will employ the patsy module to use a regression formula to create our dependent and independent feature matrices. Finally, we will construct a decision tree regressor on these data and evaluate its performance.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "[skdtr]: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Regression: Auto MPG Data\n",
    "\n",
    "The [automobile fuel performance prediction data][uciap] were collated by Ross Quinlan and released in 1993. The data contains nine features: mpg, cylinders, displacement, horsepower, weight, acceleration, model year, origin, and car name. Of these, the first is generally treated as the dependent variable (i.e., we wish to predict the fuel efficiency of the cars), while the next seven features are generally used as the independent variables. The last feature is a string that is unlikely to be useful when predicting on new, unseen data, and is, therefore, not included in our analysis.\n",
    "\n",
    "Of these features, three are discrete: cylinders, year, and origin, and four are continuous: \n",
    "displacement, horsepower, weight, and acceleration. In the first two cell, we first read the data from UCI ML repository into a DataFrame by using our provided column names and by indicating that the features are delimited by whitespace, then print 5 sample rows and the basic information of the dataframe.\n",
    "\n",
    "----\n",
    "[uciap]: https://archive.ics.uci.edu/ml/datasets/auto+mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Year</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>13.0</td>\n",
       "      <td>8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4735.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>chrysler new yorker brougham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>?</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>ford pinto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>29.5</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2135.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>honda accord lx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>23.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>88.00</td>\n",
       "      <td>2957.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>peugeot 504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>34.5</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>?</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>renault 18i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MPG  Cylinders  Displacement Horsepower  Weight  Acceleration  Year  \\\n",
       "94   13.0          8         440.0      215.0  4735.0          11.0    73   \n",
       "32   25.0          4          98.0          ?  2046.0          19.0    71   \n",
       "279  29.5          4          98.0      68.00  2135.0          16.6    78   \n",
       "178  23.0          4         120.0      88.00  2957.0          17.0    75   \n",
       "354  34.5          4         100.0          ?  2320.0          15.8    81   \n",
       "\n",
       "     Origin                          Name  \n",
       "94        1  chrysler new yorker brougham  \n",
       "32        1                    ford pinto  \n",
       "279       3               honda accord lx  \n",
       "178       2                   peugeot 504  \n",
       "354       2                   renault 18i  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "# Names for our columns\n",
    "col_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Year', 'Origin', 'Name']\n",
    "\n",
    "# Create DataFrame and sample the result\n",
    "auto_data = pd.read_csv(data_file, index_col=False, names = col_names, \n",
    "                  delim_whitespace=True)\n",
    "auto_data.sample(5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 9 columns):\n",
      "MPG             398 non-null float64\n",
      "Cylinders       398 non-null int64\n",
      "Displacement    398 non-null float64\n",
      "Horsepower      398 non-null object\n",
      "Weight          398 non-null float64\n",
      "Acceleration    398 non-null float64\n",
      "Year            398 non-null int64\n",
      "Origin          398 non-null int64\n",
      "Name            398 non-null object\n",
      "dtypes: float64(4), int64(3), object(2)\n",
      "memory usage: 28.1+ KB\n"
     ]
    }
   ],
   "source": [
    "auto_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "From the dataframe information, we can see that Horsepower column has type object. This indicates that some values are not number. If we examine the column, we can find that the `Horsepower` feature has several unknown values that are indicated by a `?` in the field of that value. If we continue to use Horsepower, the feature will be treated as strings rather than numbers. Since decision tree regressor only accept numeric values, we will simply ignore the column Horsepower in the analysis in this notebook. An alternative technique would be to convert the `?` into a missing value, and convert the column to float type, then impute the missing values.\n",
    "\n",
    "In the following code, we define the dependent and independent variables.\n",
    "\n",
    "-----\n",
    "[pm]: http://patsy.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose dependent variable and independent variable. Simply ignore Horsepower column due to \"?\"\n",
    "y = auto_data['MPG']\n",
    "x = auto_data[['Cylinders', 'Displacement', 'Weight', 'Acceleration', 'Year', 'Origin']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "With these two DataFrames, we can now build a regressive model. First, we import the `DecisionTreeRegressor` before splitting our independent and dependent variables into training and testing samples. Next, we create our estimator, specifying a value for our `random_state` hyperparameter to enable reproducibility. Finally, we fit the model and display a predictive score. The model achieved a pretty good score or $R^2$ at 81.5%.\n",
    "\n",
    "The second code cell computes a number of different regression performance metrics and displays the results. Notice that, unlike the case for classification, regression performance metrics are generally better when they are smaller. This is because these metrics are often quantifying the difference between the test and predicted features, which we want to minimize. We will discuss regression performance metrics in more details in future lessons.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score = 81.5%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Split data intro training:testing data set\n",
    "ind_train, ind_test, dep_train, dep_test = \\\n",
    "    train_test_split(x, y, test_size=0.4, random_state=23)\n",
    "\n",
    "# Create Regressor with default properties\n",
    "auto_model = DecisionTreeRegressor(random_state=23)\n",
    "\n",
    "# Fit estimator and display score\n",
    "auto_model = auto_model.fit(ind_train, dep_train)\n",
    "print('Score = {:.1%}'.format(auto_model.score(ind_test, dep_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Displacement</td>\n",
       "      <td>0.677543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Weight</td>\n",
       "      <td>0.145087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Year</td>\n",
       "      <td>0.138843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acceleration</td>\n",
       "      <td>0.033601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Origin</td>\n",
       "      <td>0.004742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cylinders</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Feature  Importance\n",
       "1  Displacement    0.677543\n",
       "2        Weight    0.145087\n",
       "4          Year    0.138843\n",
       "3  Acceleration    0.033601\n",
       "5        Origin    0.004742\n",
       "0     Cylinders    0.000185"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_model.feature_importances_\n",
    "feature_importance = pd.DataFrame(list(zip(ind_train.columns, auto_model.feature_importances_)), columns=['Feature', 'Importance'])\n",
    "feature_importance.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score             = 0.815\n",
      "Mean Squared Error    = 9.98\n",
      "Mean Absolute Error   = 2.41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Regress on test data\n",
    "pred = auto_model.predict(ind_test)\n",
    "\n",
    "# Copute performance metrics\n",
    "mr2 = r2_score(dep_test, pred)\n",
    "mae = mean_absolute_error(dep_test, pred)\n",
    "mse = mean_squared_error(dep_test, pred)\n",
    "\n",
    "# Display metrics\n",
    "print(f'R^2 Score             = {mr2:5.3f}')\n",
    "print(f'Mean Squared Error    = {mse:4.2f}')\n",
    "print(f'Mean Absolute Error   = {mae:4.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Regression Performance Metrics\n",
    "\n",
    "There are many ways to measure the performance of a regression model. In above code, to evaluate the model, we printed the $R^2$ score, mean absolute error and mean squared error. We will discuss these terms briefly below.\n",
    "\n",
    "Regression performance metrics are very import concept, we will explore them in more details in future lessons.\n",
    "\n",
    "#### R-squared ($R^2$)\n",
    "R-squared($R^2$), also known as the coefficient of determination, is the most commonly known evaluation metric for a regression model. It is the proportion of variation in the outcome(dependent variable) that is explained by the predictor variables(independent variables). R^2 normally ranges from 0 to 1. The Higher the R-squared, the better the model.\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "Mean squared error (MSE) is the average squared difference between the observed actual outome values and the values predicted by the model. So, $MSE = mean((observeds - predicteds)^2)$. The lower the MSE, the better the model.\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "Mean Absolute Error (MAE) is the average absolute difference between observed and predicted outcomes, $MAE = mean(abs(observeds - predicteds))$. MAE is less sensitive to outliers compared to MSE. The lower the MAE, the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the previous Code cells, we constructed a decision tree for regression and applied it to the automobile fuel performance prediction task. The initial result was reasonable, but try making the following changes to see if you can do better.\n",
    "\n",
    "1. Change the features used in the regression, for example drop one column, such as `origin`. Do the results change? \n",
    "3. Try using different hyperparameter values, such as specifying a different metric to reduce the variance at tree splits (via the `criterion` hyperparameter), or try using a random `splitter`.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. A detailed [blog article][1] on decision trees and computing split features\n",
    "1. A presentation style web article on [building decision trees][2] with scikit learn\n",
    "2. An article on [building decision trees][3] from scratch in Python at Analytics Vidhya\n",
    "12. A readable article on [building and using][4] decision trees in Python\n",
    "\n",
    "-----\n",
    "[1]: https://web.archive.org/web/20170628054712/http://decisiontrees.net/decision-trees-tutorial/tutorial-4-id3/\n",
    "\n",
    "[2]: http://www.ritchieng.com/machine-learning-decision-trees/\n",
    "\n",
    "[3]: https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/\n",
    "\n",
    "[4]: https://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2019: Gies College of Business at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
