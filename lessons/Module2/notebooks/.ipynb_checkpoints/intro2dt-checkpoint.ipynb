{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Decision Tree\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we introduce the [Decision Tree algorithm][wdt], and demonstrate how to effectively use this algorithm for both classification and regression problems. The Decision Tree algorithm is a simple algorithm that is easy to understand, since a higher-level representation of the data is iteratively constructed from the data. A decision tree provides a powerful, predictive model that can capture non-linear effects while also being easy to understand and explain. \n",
    "\n",
    "In this notebook, we first explore the basic formalism of the decision tree algorithm, including a discussion on several important concepts that can be used to determine how the tree is constructed from a data set. Next, we introduce the use of the decision tree for classification problems by using the Iris data set. In this section we will examine feature importance, visualize the predictive tree, and discuss the effect of different hyperparameters, before switching to a more complex data set. Then, we will look at constructing a decision tree for regression, by using a new data set. Finally, we will briefly introduce several popular regression performance metrics.\n",
    "\n",
    "-----\n",
    "[wdt]: https://en.wikipedia.org/wiki/Decision_tree_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Formalism](#Formalism)\n",
    "\n",
    "[Decision Tree: Classification](#Decision-Tree:-Classification)\n",
    "\n",
    "- [Classification: Iris Data](#Classification:-Iris-Data)\n",
    "- [Decision Tree: Decision Surface](#Decision-Tree:-Decision-Surface)\n",
    "- [Decision Tree: Hyperparameters](#Decision-Tree:-Hyperparameters)\n",
    "- [Decision Tree: Feature Importance](#Decision-Tree:-Feature-Importance)\n",
    "- [Decision Tree: Visualizing the Tree](#Decision-Tree:-Visualizing-the-Tree)\n",
    "- [Classification: Adult Data](#Classification:-Adult-Data)\n",
    "\n",
    "[Decision Tree: Regression](#Decision-Tree:-Regression)\n",
    "\n",
    "- [Regression: Auto MPG Data](#Regression:-Auto-MPG-Data)\n",
    "\n",
    "[Regression Performance Metrics](#Regression-Performance-Metrics)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the _Formalism_ section of this Notebook, we first have our standard notebook setup code.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Formalism\n",
    "\n",
    "One of the simplest machine learning algorithms to understand is the [decision tree][wdt]. For a classification task, a decision tree asks a set of questions of the data, and based on the answers, determines the final classification. The tree is constructed by recursively splitting a data set into new groupings based on a statistical measure of the data along each different dimension. The terminal nodes in the tree are known as leaf nodes and provide the final predictions. In the simplest form, the leaf node simply provides the final answer; however, the values in the leaf node can also be combined to form a probabilistic classification or regression estimate.\n",
    "\n",
    "In addition to their simplicity, decision trees have several other benefits. First, they are a _white box model_, which simply means we can understand exactly why a decision tree makes a specific prediction on a given instance. Second, they can handle both numerical and categorical data, and they do not require pre-processing beyond handling missing values. Trees also tend to perform well on large data sets. \n",
    "\n",
    "On the other hand, decision trees are prone to **overfitting**, where they model the training data too well and do not generalize to unseen data. Decision trees can have difficulty classifying on unbalanced classes and they can be unstable to minor changes in the training data. Overall, however, the decision tree is one of a handful of standard machine learning algorithms with which you should be familiar. In future notebooks, we will learn how to overcome many of these disadvantages by employing ensemble learning with decision trees. \n",
    "\n",
    "The following figure shows a simple decision tree. In the image below, the square text box represents a condition, based on which the tree splits into branches. The end of the branch represented by oval text box that doesnâ€™t split anymore is the leaf, or decision; in this case, whether one should wear sunglasses or not.\n",
    "\n",
    "<img src=\"images/dt_sunglasses.png\">\n",
    "   \n",
    "The decision on which feature to split, and the actual value along that feature on which to split can be performed in several different manners:\n",
    "- [Variance reduction][wvr]: the split choice is made to maximally reduce the variance along a feature, useful for regression problems.\n",
    "- [Gini impurity][wgi]: the split choice is made to minimize misclassifications, especially in a multi-class classification domain.\n",
    "- [Information gain][wig]: the split choice is selected to create the purest child nodes, based on the concept of entropy and information theory.\n",
    "\n",
    "The detail of spliting algorithms is out of the scope of this course.\n",
    "\n",
    "-----\n",
    "\n",
    "[wdt]: https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "\n",
    "[wvr]: https://en.wikipedia.org/wiki/Decision_tree_learning#Variance_reduction\n",
    "[wig]: https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain\n",
    "[wgi]: https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Decision Tree: Classification\n",
    "\n",
    "To apply the decision tree algorithm to classification tasks we will use the `DecisionTreeClassifier` estimator from the scikit-learn `tree` module. This estimator will construct, by default, a tree from a training data set. This estimator accepts several hyperparameters, including:\n",
    "\n",
    "- `criterion`: The method by which to measure the quality of a potential split. By default the Gini impurity is used, although information gain can be specified by passing the string `entropy`.\n",
    "- `max_depth`: The maximum depth of the tree. By default this is `None`, which means the tree is constructed until either all leaf nodes are pure, or all leaf nodes contain fewer instances than the `min_samples_split` hyperparameter value.\n",
    "- `min_samples_split`: The minimum number of instances required to split a node into two child nodes. By default this value is two.\n",
    "- `min_samples_leaf`: The minimum number of instances required to make a node terminal (i.e., a leaf node). By default this value is one.\n",
    "- `max_features`: The number of features to examine when choosing the best split feature and value. By default this is `None`, which means all features will be explored.\n",
    "- `random_state`: The seed for the random number generator used by this estimator. Setting this value ensures reproducibility.\n",
    "- `class_weight`: Values that can improve classification performance on unbalanced data sets. By default this value is `None`.\n",
    "\n",
    "Run `help(DecisionTreeClassifier)` to view more details about the model and the hyperparameters.\n",
    "\n",
    "To demonstrate using a decision tree with the scikit-learn library, we will first load in the Iris data. With these data, we will construct a simple decision tree to introduce the concept of _feature importance_. Next, we will explore how to visualize the decision tree constructed by the `DecisionTreeClassifier` estimator. Finally, we will switch to a larger data set to learn how to employ a decision tree on more complex data.\n",
    "\n",
    "----\n",
    "[skdtc]: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Classification: Iris Data\n",
    "\n",
    "We can now apply the Decision Tree algorithm to the Iris data to create a classification model. The basic approach is simple, and follows the standard scikit-learn estimator philosophy:\n",
    "\n",
    "1. Load Iris data\n",
    "2. Encode species column to create a numeric label column.\n",
    "3. Split the data into training and testing sets.\n",
    "4. Import our estimator, [`DecisionTreeClassifier`][skdtc], from the proper scikit-learn module, `tree`.\n",
    "5. Create the estimator and specify the appropriate hyperparameters. For a decision tree, we can accept the defaults, or specify values for specific hyperparameters, such as `max_depth`.\n",
    "6. Fit the model to the training data.\n",
    "7. Predict new classes with our trained model and generate performance metrics.\n",
    "\n",
    "These steps are demonstrated in the following code cells.\n",
    "\n",
    "We will also plot a confusion matrix for the model using helper code. The helper code defines `confusion()` function. You may find the source code of `confusion()` in previous lesson notebook.\n",
    "\n",
    "-----\n",
    "[skdtc]: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width     species\n",
       "15            5.7          4.4           1.5          0.4      setosa\n",
       "69            5.6          2.5           3.9          1.1  versicolor\n",
       "3             4.6          3.1           1.5          0.2      setosa\n",
       "134           6.1          2.6           5.6          1.4   virginica\n",
       "113           5.7          2.5           5.0          2.0   virginica"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = sns.load_dataset('iris')\n",
    "iris_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "      <th>species_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width     species  \\\n",
       "14            5.8          4.0           1.2          0.2      setosa   \n",
       "98            5.1          2.5           3.0          1.1  versicolor   \n",
       "75            6.6          3.0           4.4          1.4  versicolor   \n",
       "16            5.4          3.9           1.3          0.4      setosa   \n",
       "131           7.9          3.8           6.4          2.0   virginica   \n",
       "\n",
       "     species_cat  \n",
       "14             0  \n",
       "98             1  \n",
       "75             1  \n",
       "16             0  \n",
       "131            2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#create new column to hold encoded species\n",
    "iris_df['species_cat'] = LabelEncoder().fit_transform(iris_df.species)\n",
    "iris_df.sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 4), (60, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Define data and label\n",
    "data = iris_df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "label = iris_df['species_cat']\n",
    "\n",
    "# Split data into training and testing\n",
    "# Note that we have both 'data' and 'label'\n",
    "d_train, d_test, l_train, l_test = train_test_split(data, label, test_size=0.4, random_state=23)\n",
    "d_train.shape, d_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    32\n",
       "2    29\n",
       "0    29\n",
       "Name: species_cat, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check training label balance\n",
    "l_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    21\n",
       "0    21\n",
       "1    18\n",
       "Name: species_cat, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check testing label balance\n",
    "l_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree prediction accuracy = 96.7%\n"
     ]
    }
   ],
   "source": [
    "# Next lets try Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# First we construct our decision tree, we only specify the \n",
    "# random_state hyperparameter to ensure reproduceability.\n",
    "dtc = DecisionTreeClassifier(random_state=23)\n",
    "\n",
    "# Fit estimator to scaled training data\n",
    "dtc = dtc.fit(d_train, l_train)\n",
    "\n",
    "# Compute and display accuracy score\n",
    "score = 100.0 * dtc.score(d_test, l_test)\n",
    "print(f\"Decision Tree prediction accuracy = {score:4.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "For completeness, we also display the classification report and the confusion matrix in the following two Code cells. The per-class precision and recall are very good, with a minor issue in the prediction of class `Virginica`, which is also demonstrated clearly in the confusion matrix.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Setosa       1.00      1.00      1.00        21\n",
      "  Versicolor       0.90      1.00      0.95        18\n",
      "   Virginica       1.00      0.90      0.95        21\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "Primitive Confusion Matrix:\n",
      "[[21  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0  2 19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Thre types of Iris in data set\n",
    "labels = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "# Predict on test data and report scores\n",
    "y_pred = dtc.predict(d_test)\n",
    "print(classification_report(l_test, y_pred, target_names = labels))\n",
    "print('Primitive Confusion Matrix:')\n",
    "print(confusion_matrix(l_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_code import mlplots as ml\n",
    "\n",
    "# Call confusion matrix plotting routine\n",
    "ml.confusion(l_test, y_pred, labels, 'Decision Tree Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Decision Tree: Feature Importance\n",
    "\n",
    "As the previous example demonstrated, the decision tree can often provide impressive performance rather easily. We can, however, leverage the fact that the decision tree is constructed by repeatedly determining the most important feature on which to split the data to compute the relative importance of each feature in the training data set. In effect, this is computed by determining to what percentage each feature was used to split the training data; formally this is known as the _Gini importance_. As a result, higher values indicate a more important feature. \n",
    "\n",
    "We demonstrate how to extract the feature importance for a decision tree classifier in the following Code cell, where we see that for this training data set, two features: Petal Width and Petal Length account for most of the importance.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importance as computed from the decision tree\n",
    "\n",
    "# Feature names\n",
    "feature_names = ['Sepal Length', 'Sepal Width', \n",
    "                 'Petal Length', 'Petal Width']\n",
    "\n",
    "# Display name and importance\n",
    "for name, val in zip(feature_names, dtc.feature_importances_):\n",
    "    print(f'{name} importance = {100.0*val:5.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Decision Tree: Visualizing the Tree\n",
    "\n",
    "A decision tree is one of the easiest algorithms to understand since a decision tree essentially asks a list of questions to partition the data. The scikit-learn library includes an [`export_graphviz`][sket] method that actually generates a visual tree representation of a constructed decision tree classifier. This representation is in the [`dot`][dot] format recognized by the standard, open source [`graphviz`][gv] library. \n",
    "\n",
    "In the next few Code cells, we export a _dot_ format of the Iris decision tree we just constructed, convert it to an SVG image, and subsequently display this image inline in our notebook. Note, if the _graphviz_ module was available, we could generate and display the decision tree directly in our notebook as demonstrated in the comments of the third Code cell below.\n",
    "\n",
    "-----\n",
    "[sket]: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz\n",
    "[dot]: http://www.graphviz.org/doc/info/lang.html\n",
    "[gv]:http://www.graphviz.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we construct our a shallow decision tree, this is\n",
    "# simply a demonstration used to show feature improtance\n",
    "# and how to view a tree, hence we need a shallow tree with max_depth=3\n",
    "dtc = DecisionTreeClassifier(max_depth=3, random_state=23)\n",
    "\n",
    "dtc = dtc.fit(d_train, l_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Note, this decision tree classifier is different than our original decision tree classifier since we set the `max_depth` hyperparameter to three. We display the feature importance for this new tree to compare to our original feature importance. In this case, we see that the two features we identified before are exclusively used to construct the new tree. Thus, we can infer that the original tree first used these two features to split the training data, before using the other two features to make leaf nodes.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importance as computed from the decision tree\n",
    "\n",
    "# Feature names\n",
    "feature_names = ['Sepal Length', 'Sepal Width', \n",
    "                 'Petal Length', 'Petal Width']\n",
    "\n",
    "# Display name and importance\n",
    "for name, val in zip(feature_names, dtc.feature_importances_):\n",
    "    print(f'{name} importance = {100.0*val:5.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The version of scikit-learn we can use in this class currently limits our ability to control the appearance of the tree. If you have [graphivz](https://pypi.org/project/graphviz/) installed, you can simply perform the following steps to create and visualize a tree in a Jupyter notebook:\n",
    "\n",
    "```\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "tree_data = export_graphviz(dtc, out_file=None, feature_names=feature_names) \n",
    "my_tree = graphviz.Source(tree_data) \n",
    "my_tree \n",
    "```\n",
    "\n",
    "In this notebook, we will simply display the image of the tree.\n",
    "\n",
    "We now display the generated tree visualization, and see that, as expected, the Petal Width feature is used for the primary splits, (indicating it is more important), while the Petal length feature is used to split into leaf nodes. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now display the image inline\n",
    "from IPython.display import SVG\n",
    "SVG(filename='tree.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the following blank Code cell, try create decision tree classfiers with different `max_depth` and apply the models on iris dataset. Compare the model accurracy for different `max_depth` values.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Classification: Adult Data\n",
    "\n",
    "We now turn to a more complex data set with which to perform classification by using a decision tree. We will use the [Adult Income Dataset][uciad] introduced in the previous lesson notebook. We will choose `Age`, `HoursPerWeek`, `CapitalGain` and `Sex` as our training data to predict income level as we did in previous lesson.\n",
    "\n",
    "In the following two Code cells, we first prepare data. Note that decision tree can handle categorical feature directly so we don't need to create dummy features for categorical features. But we still need to encode the categorical features if the values are string. We will use LabelEncoder to encode the `Sex` column.\n",
    "\n",
    "-----\n",
    "[uciad]: https://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read CSV data\n",
    "adult_data = pd.read_csv('data/adult_income.csv')\n",
    "\n",
    "# Create label column, one for >50K, zero otherwise.\n",
    "adult_data['Label'] = adult_data['Salary'].map(lambda x : 1 if '>50K' in x else 0)\n",
    "\n",
    "# Encode Sex column to numerical value\n",
    "adult_data['Sex_code'] = LabelEncoder().fit_transform(adult_data.Sex)\n",
    "\n",
    "data = adult_data[['Age', 'HoursPerWeek', 'CapitalGain', 'Sex_code']]\n",
    "label = adult_data['Label']\n",
    "\n",
    "#display label class count\n",
    "print(label.value_counts())\n",
    "# Display random sample\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "With our feature and label data prepared, we are now ready to begin the machine learning process. In the following two Code cells we first create our decision tree classifier, and then measure its performance on our testing data. \n",
    "\n",
    "In the first Code cell, we start by splitting our data into training and testing samples. Next, we create the `DecisionTreeClassifier` estimator. The only hyperparameter that we specify at this time is `random_state` in order to ensure reproducibility. Next, we fit this estimator to our training data, and generate an accuracy score on our test data. \n",
    "\n",
    "In the second Code cell, we compute and display a simple accuracy score before generating and displaying the full classification report. The report indicates that our model performs worst in predicting the positive class. Specifically, the recall indicates that we incorrectly label positive targets as negative. This means that our classifier incorrectly labels individuals who do earn a high salary as being in the low salary category.\n",
    "\n",
    "In the third Code cell, we plot the confusion matrix.\n",
    "\n",
    "Decision tree model has a feature importances attribute, which nicely shows the impact of each feature on the model. In the fourth Code cell, we create a DataFrame with two columns, Feature and Importance, from training data column names, and the feature importances attribute of the model. We create the DataFrame to make it easier to display feature importance in descending order.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "d_train, d_test, l_train, l_test = train_test_split(data, label, test_size=0.4, random_state=23)\n",
    "\n",
    "adult_model = DecisionTreeClassifier(random_state=23)\n",
    "\n",
    "adult_model = adult_model.fit(d_train, l_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Classify test data and display score and report\n",
    "predicted = adult_model.predict(d_test)\n",
    "score = 100.0 * metrics.accuracy_score(l_test, predicted)\n",
    "print(f'Decision Tree Classification [Adult Data] Score = {score:4.1f}%\\n')\n",
    "print('Classification Report:\\n {0}\\n'.format(\n",
    "    metrics.classification_report(l_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_code import mlplots as ml\n",
    "ml.confusion(l_test, predicted, ['low', 'high'], title='Decision Tree Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(list(zip(d_train.columns, adult_model.feature_importances_)),\n",
    "                                  columns=['Feature', 'Importance'])\n",
    "feature_importance.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree classifier has a little worse accuracy rate than that of the logistic regression introduced in the previous lesson. But it has better recall rate on high income(class 1) prediction, which means it identified more high income correctly than the logistic regression model. You can compare the confusion matrix of the two models for more detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the previous Code cells, we constructed a decision tree for classification and applied it to the adult income prediction task. Try making the following changes to see if you can do better.\n",
    "\n",
    "1. Change the features used in the classification; for example add one or more columns such as Relationship and Education. Do the results change? \n",
    "2. Try setting the `class_weight` hyperparameter to `balanced` to aid in dealing with the unbalanced training data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Decision Tree: Regression\n",
    "\n",
    "A decision tree can also be used to perform regression. To perform regression with the scikit-learn library we employ the [`DecisionTreeRegressor`][skdtr] estimator in the tree module. This estimator employs the same set of hyperparameters as the `DecisionTreeClassifier` estimator, and is, therefore, used in a similar manner. One other point, which is also true for classification, by specifying the `random_state` hyperparameter, we ensure reproducibility. This is because every time a tree is constructed, the features are always randomly  permuted at every split. Thus, even if we use the same set of hyperparameters and the same set of training data, we can end up with different trees if the `random_state` hyperparameter is not fixed.\n",
    "\n",
    "In this section we employ decision trees to perform regression on a new data set, the automotive fuel performance prediction. First, we will introduce the data, and prepare them for the regression task. Next, we will employ the patsy module to use a regression formula to create our dependent and independent feature matrices. Finally, we will construct a decision tree regressor on these data and evaluate its performance.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "[skdtr]: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Regression: Auto MPG Data\n",
    "\n",
    "The automobile fuel performance prediction data were collated by Ross Quinlan and released in 1993. The data contains nine features: mpg, cylinders, displacement, horsepower, weight, acceleration, model_year, origin, and car name. Of these, the first is generally treated as the dependent variable (i.e., we wish to predict the fuel efficiency of the cars), while the next seven features are generally used as the independent variables. The last feature is a string that is unlikely to be useful when predicting on new, unseen data, and is, therefore, not included in our analysis.\n",
    "\n",
    "Of these features, three are discrete: cylinders, model_year, and origin; and four are continuous: \n",
    "displacement, horsepower, weight, and acceleration. \n",
    "\n",
    "In the first two Code cells, we load the dataset from `seaborn` module, then print 5 sample rows and the basic information of the dataframe.\n",
    "\n",
    "----\n",
    "[uciap]: https://archive.ics.uci.edu/ml/datasets/auto+mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auto_data = sns.load_dataset('mpg')\n",
    "auto_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "From the dataframe information, we can see that horsepower column has some missing values. We will have to deal with missing values since the decision tree model(as well as most other machine learning models) doesn't work with missing values. We have several options here: simply drop the missing values and fill them with values like mean of the column, or estimate with the help of other features(imputing). But first, let's visualize the relations among all continuous features with `seaborn pairplot`. In the next Code cell, we plot pairplot for the four continuous features. \n",
    "\n",
    "We can see that horsepower is pretty much positively correlated with weight and displacement, which is understandable. In regression, when multiple features are highly correlated, multicollinearity occurs. Multicollinearity causes problems when you fit the model and interpret the results. A simple way to fix multicollinearity is to drop the highly correlated features. When the correlation of two features is greater than 0.8, they are considered as highly correlated. To quantify feature correlations, we display the correlation matrix of the continuous features in the next Code cell. We will drop both horsepower and weight, since they both are highly correlated with displacement. Since we decide to drop horsepower, we don't have to handle missing values in the feature.\n",
    "\n",
    "In the third Code cell, we encode the origin column and define the dependent and independent variables.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=auto_data[['displacement', 'horsepower', 'weight', 'acceleration']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data[['displacement', 'horsepower', 'weight', 'acceleration']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode origin\n",
    "auto_data['origin_code'] = LabelEncoder().fit_transform(auto_data.origin)\n",
    "#Choose dependent variable and independent variable.\n",
    "y = auto_data['mpg']\n",
    "#Drop horsepower and weight from independent variables\n",
    "x = auto_data[['cylinders', 'displacement', 'acceleration', 'model_year', 'origin_code']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "With the dependent and independent variables ready, we can now build a regressive model. \n",
    "\n",
    "First, we import the `DecisionTreeRegressor` before splitting our independent and dependent variables into training and testing samples. \n",
    "\n",
    "Then, we create our estimator, specifying a value for our `random_state` hyperparameter to enable reproducibility. \n",
    "\n",
    "Next, we fit the model and display a predictive score. The model achieved a pretty good score or $R^2$ at 73.7%.\n",
    "\n",
    "Then, we display the feature importance which shows that displacement is the most important feature when predicting mpg.\n",
    "\n",
    "Finally, we compute a number of different regression performance metrics and displays the results. Notice that, unlike the case for classification, regression performance metrics(except for $R^2$) are generally better when they are smaller. This is because these metrics are often quantifying the difference between the test and predicted features, which we want to minimize. We will discuss regression performance metrics in more details in future lessons.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Split data intro training:testing data set\n",
    "ind_train, ind_test, dep_train, dep_test = \\\n",
    "    train_test_split(x, y, test_size=0.4, random_state=23)\n",
    "\n",
    "# Create Regressor with default properties\n",
    "auto_model = DecisionTreeRegressor(random_state=23)\n",
    "\n",
    "# Fit estimator and display score\n",
    "auto_model = auto_model.fit(ind_train, dep_train)\n",
    "print('Score = {:.1%}'.format(auto_model.score(ind_test, dep_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_model.feature_importances_\n",
    "feature_importance = pd.DataFrame(list(zip(ind_train.columns, auto_model.feature_importances_)), columns=['Feature', 'Importance'])\n",
    "feature_importance.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Regress on test data\n",
    "pred = auto_model.predict(ind_test)\n",
    "\n",
    "# Copute performance metrics\n",
    "mr2 = r2_score(dep_test, pred)\n",
    "mae = mean_absolute_error(dep_test, pred)\n",
    "mse = mean_squared_error(dep_test, pred)\n",
    "\n",
    "# Display metrics\n",
    "print(f'R^2 Score             = {mr2:5.3f}')\n",
    "print(f'Mean Squared Error    = {mse:4.2f}')\n",
    "print(f'Mean Absolute Error   = {mae:4.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Regression Performance Metrics\n",
    "\n",
    "There are many ways to measure the performance of a regression model. In above code, to evaluate the model, we printed the $R^2$ score, mean absolute error and mean squared error. We will discuss these terms briefly below.\n",
    "\n",
    "Regression performance metrics are a very import concept; we will explore them in more details in future lessons.\n",
    "\n",
    "#### R-squared ($R^2$)\n",
    "R-squared($R^2$), also known as the Coefficient of Determination, is the most commonly known evaluation metric for a regression model. It is the proportion of variation in the outcome(dependent variable) that is explained by the predictor variables(independent variables). R^2 normally ranges from 0 to 1. The Higher the R-squared, the better the model.\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "Mean squared error (MSE) is the average squared difference between the observed actual outcome values and the values predicted by the model. So, $MSE = mean((observeds - predicteds)^2)$. The lower the MSE, the better the model.\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "Mean Absolute Error (MAE) is the average absolute difference between observed and predicted outcomes, $MAE = mean(abs(observeds - predicteds))$. MAE is less sensitive to outliers compared to MSE. The lower the MAE, the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the previous Code cells, we constructed a decision tree for regression and applied it to the automobile fuel performance prediction task. The initial result was reasonable, but try making the following changes to see if you can do better.\n",
    "\n",
    "1. Change the features used in the regression, for example drop one column, such as `origin`. Do the results change? \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. A detailed [blog article][1] on decision trees and computing split features\n",
    "1. A presentation style web article on [building decision trees][2] with scikit-learn\n",
    "2. An article on [building decision trees][3] from scratch in Python at Analytics Vidhya\n",
    "12. A readable article on [building and using][4] decision trees in Python\n",
    "\n",
    "-----\n",
    "[1]: https://web.archive.org/web/20170628054712/http://decisiontrees.net/decision-trees-tutorial/tutorial-4-id3/\n",
    "\n",
    "[2]: http://www.ritchieng.com/machine-learning-decision-trees/\n",
    "\n",
    "[3]: https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/\n",
    "\n",
    "[4]: https://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2019: Gies College of Business at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
