{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Feature Selection\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection helps you in the mission to create an accurate predictive model. It helps you by choosing features that will give you as good or better accuracy while requiring less data.\n",
    "\n",
    "The key benefits of performing feature selection on the data are:\n",
    "\n",
    "- Reduces Overfitting: Less redundant data means less chance to make decisions based on noise.\n",
    "- Improves Accuracy: Less misleading data means improvements in modeling accuracy.\n",
    "- Reduces Training Time: Less data means algorithms train faster.\n",
    "- Improves Interpretability: Less complexity of a model makes it easier to interpret.\n",
    "\n",
    "In some cases, a domain expert can indicate which features have the most predictive power and which features can be ignored. When this is not possible (and in some cases even when it is possible), we can employ algorithmic feature selection to automatically quantify the importance of features so that a threshold can be used to identify the best features for a particular application.\n",
    "\n",
    "Broadly speaking there are three general classes of feature selection algorithms:\n",
    "- Filter methods\n",
    "- Wrapper methods\n",
    "- Embedded methods\n",
    "\n",
    "The scikit-learn provides a number of [feature selection algorithms][skfs] that implement these techniques. The rest of this notebook explores them in more detail.\n",
    "\n",
    "-----\n",
    "\n",
    "[skfs]: http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Data](#Data)\n",
    "\n",
    "[Filter Methods](#Filter-Methods)\n",
    "- [Statistical Tests](#Statistical-Tests)\n",
    "\n",
    "- [Univariate Techniques](#Univariate-Techniques)\n",
    "\n",
    "[Wrapper methods](#Wrapper-methods)\n",
    "- [Recursive Feature Elimination](#Recursive-Feature-Elimination)\n",
    "\n",
    "[Embedded Methods](#Embedded-Methods)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the rest of this notebook, we first have our standard notebook setup code.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set global fiugure properties\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({'axes.titlesize' : 20,\n",
    "                     'axes.labelsize' : 18,\n",
    "                     'legend.fontsize': 16})\n",
    "\n",
    "# Set default seaborn plotting style\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Data\n",
    "\n",
    "To perform feature selection, we need representative data. In this section we introduce two data sets that we use to perform feature selection within this notebook.\n",
    "\n",
    "\n",
    "### Iris Data\n",
    "\n",
    "The first data set we use to perform feature selection is the [Iris data][id]. Previously, we used seaborn to load iris data to a dataframe. In this notebook we use scikit-learn library which loads the iris data to an object. The object has data and target attributes which contains training features and target label of iris data in numpy array format. These data contain four features: sepal length, sepal width, petal length and petal width, for three different Iris varieties. There are fifty examples of each type of Iris, for 150 total instances in the data set. **To increase the challenge, we will occasionally add random _noise_ features to these data in order to test if a feature selection technique can distinguish between signal and noise.**\n",
    "\n",
    "-----\n",
    "\n",
    "[id]: http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Feature [5.1 3.5 1.4 0.2]: Label 0\n",
      "Feature [7.  3.2 4.7 1.4]: Label 1\n",
      "Feature [6.3 3.3 6.  2.5]: Label 2\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as ds\n",
    "\n",
    "# Load Iris Data\n",
    "iris = ds.load_iris()\n",
    "\n",
    "# Extract features & labels\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "print(f'Feature names:{iris.feature_names}')\n",
    "# Output examples of each class\n",
    "print(f'Feature {features[0]}: Label {labels[0]}')\n",
    "print(f'Feature {features[50]}: Label {labels[50]}')\n",
    "print(f'Feature {features[100]}: Label {labels[100]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Income Data\n",
    "The second data set we use throughout this notebook is the [Adult income prediction task][uciad]. These data were extracted by Barry Becker from the 1994 Census database and consist of the following features: age, workclass, fnlwgt, education, education-level, marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week, native-country, and salary. Of these, five are continuous features:  fnlwgt, education-num, capital-gain, capital-loss, and hours-per-week, the others are discrete features. The last column, salary, is discrete and contains one of two strings to indicate if the salary was below or above $50,000. This is the column we will use to make our label.\n",
    "\n",
    "The following Code cell prepares the data:\n",
    "\n",
    "1. Load data(we use a subset of original data)\n",
    "2. Create label from Salary column\n",
    "3. Encode categorical features that have string value\n",
    "4. Combine numerical features and encoded categorical features.\n",
    "\n",
    "-----\n",
    "[uciad]: https://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>FNLWGT</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>CapitalGain</th>\n",
       "      <th>CapitalLoss</th>\n",
       "      <th>HoursPerWeek</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Education</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Sex</th>\n",
       "      <th>NativeCountry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62</td>\n",
       "      <td>68268</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>215990</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>185405</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>258006</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>39388</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  FNLWGT  EducationLevel  CapitalGain  CapitalLoss  HoursPerWeek  \\\n",
       "0   62   68268               9            0            0            40   \n",
       "1   50  215990               9            0            0            40   \n",
       "2   36  185405               9            0            0            50   \n",
       "3   64  258006              10            0            0            40   \n",
       "4   28   39388              11            0            0            60   \n",
       "\n",
       "   Workclass  Education  MaritalStatus  Occupation  Relationship  Race  Sex  \\\n",
       "0          2         11              2          14             0     4    1   \n",
       "1          4         11              2           3             0     4    1   \n",
       "2          4         11              2           1             0     4    1   \n",
       "3          4         15              6           1             1     4    0   \n",
       "4          6          8              2           5             0     4    1   \n",
       "\n",
       "   NativeCountry  \n",
       "0             36  \n",
       "1             36  \n",
       "2             36  \n",
       "3              5  \n",
       "4             36  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Read CSV data\n",
    "adult_data = pd.read_csv('data/adult_income.csv')\n",
    "\n",
    "# Create label column, one for >50K, zero otherwise.\n",
    "adult_data['Label'] = adult_data['Salary'].map(lambda x : 1 if '>50K' in x else 0)\n",
    "\n",
    "# Generate categorical features\n",
    "categorical_features = adult_data[['Workclass', 'Education', 'MaritalStatus', \n",
    "               'Occupation', 'Relationship', 'Race', 'Sex', 'NativeCountry']]\n",
    "\n",
    "# Encocde categorical features\n",
    "categorical_features = categorical_features.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Extract numerical features\n",
    "numerical_features = adult_data[['Age', 'FNLWGT', 'EducationLevel', 'CapitalGain', 'CapitalLoss', 'HoursPerWeek']]\n",
    "\n",
    "adult_features = pd.concat([numerical_features, categorical_features], axis=1)\n",
    "\n",
    "adult_label = adult_data['Label']\n",
    "adult_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Filter Methods\n",
    "Filter methods typically involve the application of a statistical measure to score the different features. This score allows the features to be ranked, and this ranking is used to determine which features to keep and which can be removed from the data. Generally each feature is considered on its own (i.e., a univariate test).\n",
    "\n",
    "### Statistical Tests\n",
    "\n",
    "One of the simplest techniques for algorithmically selecting features is to measure the variance in each feature. Some machine learning algorithms, such as the Decision Tree, explicitly measure the variance of features and split those features with the greatest variance. The reason for this approach is that features with greatest variance contain significant information, whereas features with the least variance are tightly bunched and contain little discriminative power. As an extreme example, a feature that has zero variance provides no descriptive power (since all data have same value) and can easily be removed from analysis without impacting the predictive performance of an algorithm.\n",
    "\n",
    "Formally, this technique is known as [variance thresholding][vt], which is implemented in the scikit-learn library by the [`VarianceThreshold`][skvt] selector. The following two Code cells demonstrate this technique on the Iris data. First, the technique is applied directly to the Iris data, which provides a ranking of feature importance (via the variance measures). \n",
    "\n",
    "`VarianceThreshold` takes one argument `threshold`. The selector will remove all features with variance lower than the `threshold`. The default value of `threshold` is 0 which means all features will be kept. We will use default `threshold` in this notebook so that we can get variance of all features. Once the `VarianceThreshold` is fit and transformed on the features dataset, we can retrieve feature variances from the selector's `variances_` attribute. We then zip the variances with feature names and display each feature's variance.\n",
    "\n",
    "However, since the original features are not scaled, the variance comparison is inaccurate. Some features have a naturally larger spread due to the sizes of the widths and lengths of the petals and sepals. Thus, the second Code cell scales these features to the same zero to one range, and then perform variance thresholding. Notice how the results change such that the petal width becomes more important than the petal length. This example emphasizes the importance of ensuring that the statistical tests are performed in a uniform manner in order to avoid biasing the results. \n",
    "\n",
    "The major disadvantage of variance threshold is that it doesn't take the target feature in consideration when calculating the score. This is especially problematic with an unbalanced dataset. For example, in a cancer screening dataset, assume only 10% of the data has a positive label. There could be a feature that's highly correlated to the label, i.e., has small values for negative labels and large values for positive labels. About 90% of the feature then have small values, which makes the feature's variance very low. Since the feature is highly correlated to the label, this feature actually has great predicting power which makes it a very important feature.\n",
    "\n",
    "On the other hand, since variance threshold doesn't need a target feature, this technique can be used to select features for unsupervised learning which will be introduced in the future lessons.\n",
    "\n",
    "-----\n",
    "\n",
    "[vt]: http://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance\n",
    "[skvt]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) variance = 0.681\n",
      "sepal width (cm) variance = 0.189\n",
      "petal length (cm) variance = 3.096\n",
      "petal width (cm) variance = 0.577\n"
     ]
    }
   ],
   "source": [
    "# Perform variance thresholding on raw features\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vt = VarianceThreshold()\n",
    "\n",
    "# Compute and display variances\n",
    "vt.fit_transform(features)\n",
    "feature_variances = vt.variances_\n",
    "for var, name in zip(feature_variances, iris.feature_names):\n",
    "    print(f'{name:>10} variance = {var:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) variance = 0.053\n",
      "sepal width (cm) variance = 0.033\n",
      "petal length (cm) variance = 0.089\n",
      "petal width (cm) variance = 0.100\n"
     ]
    }
   ],
   "source": [
    "# Scale features and then perform variance thresholding\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize data to [0, 1] range\n",
    "features_ss = MinMaxScaler().fit_transform(features)\n",
    "\n",
    "# Compute and display variances\n",
    "vt.fit_transform(features_ss)\n",
    "for var, name in zip(vt.variances_, iris.feature_names):\n",
    "    print(f'{name:>10} variance = {var:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "As an additional example, we can apply variance thresholding to the adult income data set. In this case, we display feature variances in descending order.\n",
    "\n",
    "The feature variances show that `HoursPerWeek` is among the features with lowest variance. However, we know that `HoursPerWeek` is definitely a very important factor of income. On the other hand, binary categorical features like `Sex` normally have variance close to 0.25 when classes are balanced(50% male and 50% female). This tells us that variance threshold may not be a reliable indicator of feature importance when using alone.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Workclass variance = 0.034\n",
      "               Sex variance = 0.220\n",
      "      Relationship variance = 0.102\n",
      "              Race variance = 0.046\n",
      "        Occupation variance = 0.092\n",
      "     NativeCountry variance = 0.033\n",
      "     MaritalStatus variance = 0.063\n",
      "      HoursPerWeek variance = 0.016\n",
      "            FNLWGT variance = 0.007\n",
      "    EducationLevel variance = 0.029\n",
      "         Education variance = 0.069\n",
      "       CapitalLoss variance = 0.008\n",
      "       CapitalGain variance = 0.006\n",
      "               Age variance = 0.036\n"
     ]
    }
   ],
   "source": [
    "# Normalize data to [0, 1] range\n",
    "adult_features_ss = MinMaxScaler().fit_transform(adult_features)\n",
    "\n",
    "# Compute and display variances\n",
    "vt.fit_transform(adult_features_ss)\n",
    "\n",
    "for var, name in sorted(zip(vt.variances_, adult_features.columns), key=lambda x: x[1], reverse=True):\n",
    "    print(f'{name:>18} variance = {var:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Univariate Techniques\n",
    "\n",
    "Another technique for identifying the features that encode the majority of the signal in a data set is to examine each feature individually to determine the strength of the relationship of the feature with the target feature.\n",
    "\n",
    "The two main techniques for performing this type of feature selection are [`SelectKBest`][skb] and [`SelectPercentile`][sp]. The former selects the **k** best features, while the latter selects the best percentage of features. Each of these techniques accepts a `score_func` that implements the statistical measure. Provided measures include the following:\n",
    "- `f_classif`: default value, computes the ANOVA F-value between the features and labels, used for classification.\n",
    "- `mutual_info_classif`: computes the mutual information of discrete label, used for classification.\n",
    "- `chi2`: computes chi-squared statistic of non-negative features, used for classification.\n",
    "- `f_regression`: computes the ANOVA F-value between the features and labels, used for regression.\n",
    "- `mutual_info_regression`: computes the mutual information for continuous label, used for regression.\n",
    "\n",
    "Several other specific techniques are also provided by the scikit-learn library, but they are beyond the scope of this notebook. The [online documentation][skut] provides more details on all of these methods.\n",
    "\n",
    "To demonstrate these techniques, we will start with the original Iris data set. We use the `SelectKBest` technique to compute the scores for all features, and we use the default scoring function which is [`f_classif`][fc]. This statistic measures the degree of linear dependence between two features. We indicate all features should be kept by setting `k='all'` so that we can print out scores of all features. If we set `k` to a number _n_, then only the best *n* features are kept.\n",
    "\n",
    "The results indicate that the petal features are most important, which agrees with the feature importance results we saw in earlier notebooks.\n",
    "\n",
    "-----\n",
    "[skut]: http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "[skb]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "[sp]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html\n",
    "[fc]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sepal length (cm) score = 119.265\n",
      "  sepal width (cm) score = 49.160\n",
      " petal length (cm) score = 1180.161\n",
      "  petal width (cm) score = 960.007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "skb = SelectKBest(k='all')\n",
    "\n",
    "skb.fit(features, labels)\n",
    "for var, name in zip(skb.scores_, iris.feature_names):\n",
    "    print(f'{name:>18} score = {var:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "To test these techniques more thoroughly, we can add random noise features into the analysis. To do this, the following Code cell generates ten new features (called _NoiseXX_ where the XX is replaced by the ordinal number of the new feature) that contain values that are uniformly sampled from the range zero to one. We combine these new noise features with our Iris data, which have been properly normalized to the same range, by using the NumPy `hstack` method, and we also create a new list of feature names that aligns with our new set of features.\n",
    "\n",
    "Next, we again perform feature selection by using the `SelectKBest` technique. Now, however, we display the features, sorted by their relative importance. In this case, the real features are identified with higher importance.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " petal length (cm) score = 1180.161\n",
      "  petal width (cm) score = 960.007\n",
      " sepal length (cm) score = 119.265\n",
      "  sepal width (cm) score = 49.160\n",
      "          Noise 03 score = 7.669\n",
      "          Noise 09 score = 3.705\n",
      "          Noise 00 score = 1.888\n",
      "          Noise 04 score = 1.447\n",
      "          Noise 08 score = 0.944\n",
      "          Noise 02 score = 0.554\n",
      "          Noise 05 score = 0.417\n",
      "          Noise 06 score = 0.209\n",
      "          Noise 01 score = 0.105\n",
      "          Noise 07 score = 0.020\n"
     ]
    }
   ],
   "source": [
    "# Number of noise features to add\n",
    "num_nf = 10\n",
    "\n",
    "# Set random state\n",
    "rng = np.random.RandomState(23)\n",
    "\n",
    "# Create noise features\n",
    "noise = rng.uniform(0., 1.0, size=(len(iris.data), num_nf))\n",
    "\n",
    "# Features plus noise\n",
    "features_pn = np.hstack((features_ss, noise))\n",
    "\n",
    "# Feature names\n",
    "f_names = iris.feature_names.copy()\n",
    "for i in range(noise.shape[1]):\n",
    "    f_names.append(f'Noise {i:0>2}')\n",
    "    \n",
    "# Fit features plus noise\n",
    "skb.fit(features_pn, labels)\n",
    "\n",
    "# Display scores for features and noise\n",
    "for var, name in sorted(zip(skb.scores_, f_names), key=lambda x: x[0], reverse=True):\n",
    "    print(f'{name:>18} score = {var:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "To provide additional insight into these techniques, we now switch to the adult income data set. In the following example, we change the score_func to mutual_info_classif which quantifies the dependency between features and the label. When two features are independent, this statistic goes to zero, and as the dependency increases the statistic also increases.\n",
    "\n",
    "Notice that we get a completely different order of importance when comparing to the result from variance threshold. In this case, since the adult data is an unbalanced dataset(25% high income and 75% low income), SelectKBest is more reliable than VarianceThreshold for this dataset.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Relationship score = 0.111\n",
      "     MaritalStatus score = 0.105\n",
      "       CapitalGain score = 0.074\n",
      "    EducationLevel score = 0.066\n",
      "               Age score = 0.064\n",
      "        Occupation score = 0.054\n",
      "         Education score = 0.052\n",
      "               Sex score = 0.037\n",
      "      HoursPerWeek score = 0.032\n",
      "       CapitalLoss score = 0.026\n",
      "            FNLWGT score = 0.019\n",
      "         Workclass score = 0.006\n",
      "              Race score = 0.000\n",
      "     NativeCountry score = 0.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "skb = SelectKBest(mutual_info_classif, k='all')\n",
    "skb.fit(adult_features, adult_label)\n",
    "\n",
    "# Display scores for features and noise\n",
    "for var, name in sorted(zip(skb.scores_, adult_features.columns), key=lambda x: x[0], reverse=True):\n",
    "    print(f'{name:>18} score = {var:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Wrapper methods\n",
    "Wrapper methods consider the selection of a set of features as a search problem, where different combinations are prepared, evaluated and compared to other combinations. A predictive model is used to evaluate a combination of features and assign a score based on model accuracy. Since we must train a model for each feature combination, this approach is much more expensive than a filter method. \n",
    "\n",
    "One popular wrapper method is the recursive feature elimination algorithm.\n",
    "\n",
    "\n",
    "### Recursive Feature Elimination\n",
    "\n",
    "[Recursive Feature Elimination (RFE)][rfe] works by recursively removing attributes and building a model from the remaining attributes. The model accuracy is used to identify the attributes (and combination of attributes) that most contribute to predicting the target attribute. The `RFE` implementation provided by the scikit-learn library is in the `feature_selection` module.\n",
    "\n",
    "There are two key arguments to construct a `RFE` selector:\n",
    "- estimator : A supervised learning estimator with a ``fit`` method that provides information about feature importance either through a ``coef_`` attribute or through a ``feature_importances_`` attribute.\n",
    "- n_features_to_select : int or None (default=None). The number of features to select. If `None`, half of the features are selected.\n",
    "\n",
    "For detail of other `RFE` arguments please refer to the help document.(`help(RFE)`)\n",
    "\n",
    "In the next two cells, we employ RFE to determine the most important features for both the Iris and adult income  data. The first Code cell below uses a linear support vector classifier to perform RFE. In this case, we analyze the Iris data set plus ten _noise_ features. We set `n_features_to_select` to 1 so the result of this operation identifies the most important feature, but we can still retrieve the ranks of all features via the selector's `ranking` attribute. Notice how three of the _real_ features are the top three ranked features, but next are several _noise_ features, indicating the remaining _real_ feature encodes less information than a random feature.\n",
    "\n",
    "-----\n",
    "\n",
    "[rfe]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  petal width (cm) rank = 1\n",
      " petal length (cm) rank = 2\n",
      "  sepal width (cm) rank = 3\n",
      "          Noise 03 rank = 4\n",
      "          Noise 09 rank = 5\n",
      "          Noise 00 rank = 6\n",
      " sepal length (cm) rank = 7\n",
      "          Noise 07 rank = 8\n",
      "          Noise 05 rank = 9\n",
      "          Noise 06 rank = 10\n",
      "          Noise 02 rank = 11\n",
      "          Noise 08 rank = 12\n",
      "          Noise 04 rank = 13\n",
      "          Noise 01 rank = 14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Create classifier\n",
    "svc = LinearSVC(random_state=23)\n",
    "\n",
    "# Perform RFE, select top feature (but rank all)\n",
    "rfe = RFE(estimator=svc, n_features_to_select=1)\n",
    "\n",
    "# Fit features plus noise\n",
    "rfe.fit(features_pn, labels)\n",
    "    \n",
    "# Display feature ranking\n",
    "for var, name in sorted(zip(rfe.ranking_, f_names), key=lambda x: x[0]):\n",
    "    print(f'{name:>18} rank = {var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We now transition to the adult income data set. The following Code cell takes a little longer time to run. The reason is that RFE trains the selected model(RandomForestClassifier in this case) repeatedly to rank the features. It'll take a while when the dataset is large.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            FNLWGT rank = 1\n",
      "               Age rank = 2\n",
      "      Relationship rank = 3\n",
      "    EducationLevel rank = 4\n",
      "       CapitalGain rank = 5\n",
      "      HoursPerWeek rank = 6\n",
      "        Occupation rank = 7\n",
      "     MaritalStatus rank = 8\n",
      "         Workclass rank = 9\n",
      "         Education rank = 10\n",
      "       CapitalLoss rank = 11\n",
      "     NativeCountry rank = 12\n",
      "               Sex rank = 13\n",
      "              Race rank = 14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state=23)\n",
    "# Create RFE model with only one feature\n",
    "rfe = RFE(estimator=rfc, n_features_to_select=1)\n",
    "rfe.fit(adult_features, adult_label)\n",
    "\n",
    "# Display feature ranking\n",
    "for var, name in sorted(zip(rfe.ranking_, adult_features.columns), key=lambda x: x[0]):\n",
    "    print(f'{name:>18} rank = {var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "Now that you have run the previous cells, try making changes to the notebook:\n",
    "\n",
    "1. Try using a different classifier, such as a decision tree or logistic regression.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Embedded Methods\n",
    "Embedded Methods perform feature selection directly in the model construction. Some algorithms, such as the decision tree and the ensemble techniques based on a decision tree, provide access to measures of the feature importance. This extra information can be used to rank features for use with these models. For example, the [Random Forest Classifier (RFC)][rfc], as an ensemble method, builds models by randomly selecting features when building each tree. In this process, RFC computes the overall importance of each feature in building the final model. By extracting the feature importances from the final model, we obtain a ranked ordering of the features used to build the model.\n",
    "\n",
    "In the following Code cells, we apply random forest classifier on the Iris dataset (with noise added) and adult income dataset. Then we display the feature importance of each dataset with the model's `feature_importances_` attribute.\n",
    "\n",
    "Among the machine learning algorithms we learned so far, both the Decision Tree and the Random Forest model have `feature_importances_` attribute.\n",
    "\n",
    "-----\n",
    "\n",
    "[rfc]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label             : Importance\n",
      "--------------------------\n",
      "  petal width (cm): 31.63%\n",
      " petal length (cm): 28.89%\n",
      " sepal length (cm): 12.09%\n",
      "  sepal width (cm): 7.35%\n",
      "          Noise 03: 3.44%\n",
      "          Noise 04: 2.63%\n",
      "          Noise 09: 2.50%\n",
      "          Noise 02: 2.00%\n",
      "          Noise 00: 1.95%\n",
      "          Noise 05: 1.76%\n",
      "          Noise 01: 1.64%\n",
      "          Noise 07: 1.62%\n",
      "          Noise 08: 1.26%\n",
      "          Noise 06: 1.23%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Build model\n",
    "rfc = RandomForestClassifier(random_state=23)\n",
    "rfc.fit(features_pn, labels)\n",
    "\n",
    "# Display scores for features and noise\n",
    "print(f'{\"Label\":18s}: Importance')\n",
    "print(26*'-')\n",
    "for val, name in sorted(zip(rfc.feature_importances_, f_names), \n",
    "                        key=lambda x: x[0], reverse=True):\n",
    "    print(f'{name:>18}: {val:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We now use random forest classifier to determine feature importance in  the adult income data. \n",
    "\n",
    "-----\n",
    "[sfm]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label             : Importance\n",
      "--------------------------\n",
      "            FNLWGT: 14.72%\n",
      "               Age: 14.20%\n",
      "       CapitalGain: 11.84%\n",
      "    EducationLevel: 10.30%\n",
      "      Relationship: 9.54%\n",
      "      HoursPerWeek: 8.55%\n",
      "        Occupation: 7.34%\n",
      "     MaritalStatus: 7.33%\n",
      "         Workclass: 4.31%\n",
      "         Education: 3.83%\n",
      "       CapitalLoss: 3.15%\n",
      "     NativeCountry: 1.86%\n",
      "               Sex: 1.52%\n",
      "              Race: 1.51%\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=23)\n",
    "rfc.fit(adult_features, adult_label)\n",
    "\n",
    "# Display scores for features and noise\n",
    "print(f'{\"Label\":18s}: Importance')\n",
    "print(26*'-')\n",
    "for val, name in sorted(zip(rfc.feature_importances_, adult_features.columns), \n",
    "                        key=lambda x: x[0], reverse=True):\n",
    "    print(f'{name:>18}: {val:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "Now that you have run the previous cells, try making changes to the\n",
    "notebook:\n",
    "\n",
    "1. Try changing some hyperparameters (i.e., max_depth) for the RFC estimator. How do these changes affect the feature importance?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. Series of blog articles on feature selection in Python: [Part I][2a], [Part II][2b], [Part II][2c], and [Part IV][2d]\n",
    "2. An introduction to [feature selection][3]\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "[1]: http://adataanalyst.com/machine-learning/comprehensive-guide-feature-engineering/\n",
    "[2a]: http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/\n",
    "[2b]: http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/\n",
    "[2c]: http://blog.datadive.net/selecting-good-features-part-iii-random-forests/\n",
    "[2d]: http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/\n",
    "\n",
    "[3]: https://machinelearningmastery.com/an-introduction-to-feature-selection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2019: Gies College of Business at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
