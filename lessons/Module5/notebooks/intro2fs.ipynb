{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Feature Selection\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection helps you in the mission to create an accurate predictive model. It helps you by choosing features that will give you as good or better accuracy while requiring less data.\n",
    "\n",
    "The key benefits of performing feature selection on the data are:\n",
    "\n",
    "- Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "- Improves Accuracy: Less misleading data means modeling accuracy improves.\n",
    "- Reduces Training Time: Less data means that algorithms train faster.\n",
    "- Improves Interpretability: Less complexity of a model and makes it easier to interpret.\n",
    "\n",
    "In some cases, a domain expert can indicate which features have the most predictive power and which features can be ignored. When this is not possible (and in some cases even when it is possible), we can employ algorithmic feature selection to automatically quantify the importance of features so that a threshold can be used to identify the best features for a particular application.\n",
    "\n",
    "Broadly speaking there are three general classes of feature selection algorithms:\n",
    "- filter methods\n",
    "- wrapper methods\n",
    "- embedded methods.\n",
    "\n",
    "The scikit learn provides a number of [feature selection algorithms][skfs] that implement these techniques. The rest of this notebook explores them in more detail.\n",
    "\n",
    "-----\n",
    "\n",
    "[wfs]: https://en.wikipedia.org/wiki/Feature_selection\n",
    "[skfs]: http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Data](#Data)\n",
    "\n",
    "[Filter Methods](#Filter_Methods)\n",
    "- [Statistical Tests](#Statistical-Tests)\n",
    "\n",
    "- [Univariate Techniques](#Univariate-Techniques)\n",
    "\n",
    "[Wrapper methods](#Wrapper-methods)\n",
    "- [Recursive Feature Extraction](#Recursive-Feature-Extraction)\n",
    "\n",
    "[Embedded Methods](#Embedded-Methods)\n",
    "- [Select From Model](#Select-From-Model)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the rest of this notebook, we first have our standard notebook setup code.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set global fiugure properties\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({'axes.titlesize' : 20,\n",
    "                     'axes.labelsize' : 18,\n",
    "                     'legend.fontsize': 16})\n",
    "\n",
    "# Set default seaborn plotting style\n",
    "sns.set_style('white')\n",
    "\n",
    "# Some cells take a while to run, so we will time them\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Data\n",
    "\n",
    "To perform feature selection, we need representative data. In this section we introduce the two data sets that we use to perform feature selection within this notebook.\n",
    "\n",
    "\n",
    "### Iris Data\n",
    "\n",
    "The first data set we use to perform feature selection is the [Iris data][id]. Previously, we used seaborn to load iris data to a dataframe. In this notebook we use scikit learn library which loads the iris data to an object. The object has data and target attributes which contains training features and target label of iris data in numpy array format. These data contain four features: sepal length, sepal width, petal length and petal width, for three different Iris varieties. In total, there are fifty examples of each type of Iris, for 150 total instances in the data set. **To increase the challenge, we will occasionally add random _noise_ features to these data in order to test if a feature selection technique can distinguish between signal and noise.**\n",
    "\n",
    "-----\n",
    "\n",
    "[id]: http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Feature [5.1 3.5 1.4 0.2]: Label 0\n",
      "Feature [7.  3.2 4.7 1.4]: Label 1\n",
      "Feature [6.3 3.3 6.  2.5]: Label 2\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as ds\n",
    "\n",
    "# Load Iris Data\n",
    "iris = ds.load_iris()\n",
    "\n",
    "# Extract features & labels\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "print(f'Feature names:{iris.feature_names}')\n",
    "# Output examples of each class\n",
    "print(f'Feature {features[0]}: Label {labels[0]}')\n",
    "print(f'Feature {features[50]}: Label {labels[50]}')\n",
    "print(f'Feature {features[100]}: Label {labels[100]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Income Data\n",
    "The second data set we use throughout this notebook is the [Adult income prediction task][uciad]. These data were extracted by Barry Becker from the 1994 Census database and consist of the following features: age, workclass, fnlwgt, education, education-level, marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week, native-country, and salary. Of these, only five are continuous:  fnlwgt, education-num, capital-gain, capital-loss, and hours-per-week, the others are discrete. The last column, salary, is discrete and contains one of two strings to indicate if the salary was below or above $50,000. This is the column we will use to make our label.\n",
    "\n",
    "The following code cell prepares the data:\n",
    "\n",
    "1. Load data from uci data repository\n",
    "2. Create label from Salary column\n",
    "3. Encode categorical features with string value\n",
    "4. Combine numerical features and encoded categorical features.\n",
    "\n",
    "-----\n",
    "[uciad]: https://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>FNLWGT</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>CapitalGain</th>\n",
       "      <th>CapitalLoss</th>\n",
       "      <th>HoursPerWeek</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Education</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Sex</th>\n",
       "      <th>NativeCountry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  FNLWGT  EducationLevel  CapitalGain  CapitalLoss  HoursPerWeek  \\\n",
       "0   39   77516              13         2174            0            40   \n",
       "1   50   83311              13            0            0            13   \n",
       "2   38  215646               9            0            0            40   \n",
       "3   53  234721               7            0            0            40   \n",
       "4   28  338409              13            0            0            40   \n",
       "\n",
       "   Workclass  Education  MaritalStatus  Occupation  Relationship  Race  Sex  \\\n",
       "0          7          9              4           1             1     4    1   \n",
       "1          6          9              2           4             0     4    1   \n",
       "2          4         11              0           6             1     4    1   \n",
       "3          4          1              2           6             0     2    1   \n",
       "4          4          9              2          10             5     2    0   \n",
       "\n",
       "   NativeCountry  \n",
       "0             39  \n",
       "1             39  \n",
       "2             39  \n",
       "3             39  \n",
       "4              5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data_file = \"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "col_names = ['Age', 'Workclass', 'FNLWGT', 'Education', \n",
    "             'EducationLevel', 'MaritalStatus', 'Occupation', \n",
    "             'Relationship', 'Race', 'Sex', 'CapitalGain', 'CapitalLoss', \n",
    "             'HoursPerWeek', 'NativeCountry', 'Salary']\n",
    "\n",
    "# Read CSV data from URL return Pandas\n",
    "adult_data = pd.read_csv(data_file, index_col=False, names = col_names)\n",
    "\n",
    "# Create label column, one for >50K, zero otherwise.\n",
    "adult_data['Label'] = adult_data['Salary'].map(lambda x : 1 if '>50K' in x else 0)\n",
    "\n",
    "# Generate categorical features\n",
    "categorical_features = adult_data[['Workclass', 'Education', 'MaritalStatus', \n",
    "               'Occupation', 'Relationship', 'Race', 'Sex', 'NativeCountry']]\n",
    "\n",
    "categorical_features = categorical_features.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Extract numerical features\n",
    "numerical_features = adult_data[['Age', 'FNLWGT', 'EducationLevel', 'CapitalGain', 'CapitalLoss', 'HoursPerWeek']]\n",
    "\n",
    "adult_features = pd.concat([numerical_features, categorical_features], axis=1)\n",
    "\n",
    "adult_label = adult_data['Label']\n",
    "adult_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Filter Methods\n",
    "Filter methods typically involve the application of a statistical measure to score the different features. This score allows the features to be ranked, and this ranking is used to determine which features to keep and which can be removed from the data. Generally each feature is considered on its own (i.e., a univariate test).\n",
    "\n",
    "### Statistical Tests\n",
    "\n",
    "One of the simplest techniques for algorithmically selecting features is to measure the variance, or spread, in each feature. Some machine learning algorithms, such as the decision tree, explicitly measure the variance of features and split those features with the greatest variance. The reason for this approach is that features with the greatest variance contain significant information, whereas features with the least variance are tightly bunched and contain little discriminative power. As an extreme example, a feature that has zero variance provides no descriptive power (since all features have the same value) and can easily be removed from analysis without impacting the predictive performance of an algorithm.\n",
    "\n",
    "Formally, this technique is known as [variance thresholding][vt], which is implemented in the scikit learn library by the [`VarianceThreshold`][skvt] selector. The following two Code cells demonstrate this technique on the Iris data. First, the technique is applied directly to the Iris data, which provides a ranking of feature importance (via the variance measures). \n",
    "\n",
    "However, since the original features are unnormalized, the variance comparison is inaccurate, some features have a naturally larger spread due to the sizes of the widths and lengths of the petals and sepals. Thus, the second Code cell normalizes these features to the same zero to one range, and then performance variance thresholding. Notice how the results change such that the petal width becomes more important than the petal length. This example emphasizes the importance of ensuring that the statistical tests are performed in a uniform manner in order to avoid biasing the results. \n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "[vt]: http://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance\n",
    "[skvt]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) variance = 0.681\n",
      "sepal width (cm) variance = 0.189\n",
      "petal length (cm) variance = 3.096\n",
      "petal width (cm) variance = 0.577\n"
     ]
    }
   ],
   "source": [
    "# Perform variance thresholding on raw features\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vt = VarianceThreshold()\n",
    "\n",
    "# Compute and display variances\n",
    "vt.fit_transform(features)\n",
    "for var, name in zip(vt.variances_, iris.feature_names):\n",
    "    print(f'{name:>10} variance = {var:5.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) variance = 0.053\n",
      "sepal width (cm) variance = 0.033\n",
      "petal length (cm) variance = 0.089\n",
      "petal width (cm) variance = 0.100\n"
     ]
    }
   ],
   "source": [
    "# Scale features and then perform variance thresholding\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "features_ss = MinMaxScaler().fit_transform(features)\n",
    "\n",
    "# Compute and display variances\n",
    "vt.fit_transform(features_ss)\n",
    "for var, name in zip(vt.variances_, iris.feature_names):\n",
    "    print(f'{name:>10} variance = {var:5.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "As an additional example, we can apply variance thresholding to the adult income data set. In this case, we display feature variances in descending order.\n",
    "\n",
    "The feature variances shows that `HoursPerWeek` is among the features with lowest variance. However, we know that `HoursPerWeek` is definitely a very important factor of income. On the other hand, binary categorical features like `Sex` normally have variance close to 0.25 when classes are balanced(50% male and 50% female). This tells us that variance threshold may not be a reliable indicator of feature importance when using alone.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Sex variance = 0.221\n",
      "      Relationship variance = 0.103\n",
      "        Occupation variance = 0.091\n",
      "         Education variance = 0.067\n",
      "     MaritalStatus variance = 0.063\n",
      "              Race variance = 0.045\n",
      "     NativeCountry variance = 0.036\n",
      "               Age variance = 0.035\n",
      "         Workclass variance = 0.033\n",
      "    EducationLevel variance = 0.029\n",
      "      HoursPerWeek variance = 0.016\n",
      "       CapitalLoss variance = 0.009\n",
      "       CapitalGain variance = 0.005\n",
      "            FNLWGT variance = 0.005\n"
     ]
    }
   ],
   "source": [
    "# Transform data to [0, 1]\n",
    "adult_features_ss = MinMaxScaler().fit_transform(adult_features)\n",
    "\n",
    "# Compute and display variances\n",
    "vt.fit_transform(adult_features_ss)\n",
    "\n",
    "for name, var in sorted(zip(adult_features.columns, vt.variances_), key=lambda x: x[1], reverse=True):\n",
    "    print(f'{name:>18} variance = {var:5.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Univariate Techniques\n",
    "\n",
    "Another technique for identifying the features that encode the majority of the signal in a data set is to employ a function that computes a statistical measure of the amount of information contained in a given feature. For example, the function might measure the correlation between two features, since correlated features provide redundant information. \n",
    "\n",
    "The two main techniques for performing this type of feature selection are [`SelectKBest`][skb] and [`SelectPercentile`][sp]. The former selects the **k** best features, while the latter selects the best percentage of features. Each of these techniques accepts a `score_func` that implements the statistical measure. Provided measures include the following:\n",
    "- `f_classif`: default value, computes the ANOVA F-value between the features and labels, used for classification.\n",
    "- `mutual_info_classif`: computes the mutual information of discrete label, used for classification.\n",
    "- `chi2`: computes chi-squared statistic of non-negative features, used for classification.\n",
    "- `f_regression`: computes the ANOVA F-value between the features and labels, used for regression.\n",
    "- `mutual_info_regression`: computes the mutual information for continuous label, used for regression.\n",
    "\n",
    "Several other specific techniques are also provided by the scikit learn library, but they are beyond the scope of this notebook. The [online documentation][skut] provides more details on all of these methods.\n",
    "\n",
    "To demonstrate these techniques, we will start with the original Iris data set. We use the `SelectKBest` technique to compute the scores for all features, and we use the default scoring function which is [`f_classif`][fc]. This statistic measures the degree of linear dependence between two features. The more dependent two features are the less information the second feature provides to the classification method. We indicate all features should be kept by setting `k='all'` so that we can print out scores of all features. If we set `k` to a number _n_, then only the best *n* features are kept.\n",
    "\n",
    "The results indicate that the petal features are most important, which agrees with the feature importance results we saw in earlier notebooks.\n",
    "\n",
    "-----\n",
    "[skut]: http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "[skb]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "[sp]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html\n",
    "[fc]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sepal length (cm) score = 119.265\n",
      "  sepal width (cm) score = 49.160\n",
      " petal length (cm) score = 1180.161\n",
      "  petal width (cm) score = 960.007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "skb = SelectKBest(k='all')\n",
    "\n",
    "fs = skb.fit(features, labels)\n",
    "for var, name in zip(fs.scores_, iris.feature_names):\n",
    "    print(f'{name:>18} score = {var:5.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "To test these techniques more thoroughly, we can add random noise features into the analysis. To do this, the following Code cell generates ten new features (called _NoiseXX_ where the XX is replaced by the ordinal number of the new feature) that contain values that are uniformly sampled from the range zero to one. We combine these new noise features with our Iris data, which have been properly normalized to the same range, by using the NumPy `hstack` method, and we also create a new list of feature names that aligns with our new set of features.\n",
    "\n",
    "Next, we again perform feature selection by using the `SelectKBest` technique. Now, however, we display the features, sorted by their relative importance. In this case, the real features are identified with higher importance.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " petal length (cm) score = 1180.161\n",
      "  petal width (cm) score = 960.007\n",
      " sepal length (cm) score = 119.265\n",
      "  sepal width (cm) score = 49.160\n",
      "          Noise 03 score = 7.669\n",
      "          Noise 09 score = 3.705\n",
      "          Noise 00 score = 1.888\n",
      "          Noise 04 score = 1.447\n",
      "          Noise 08 score = 0.944\n",
      "          Noise 02 score = 0.554\n",
      "          Noise 05 score = 0.417\n",
      "          Noise 06 score = 0.209\n",
      "          Noise 01 score = 0.105\n",
      "          Noise 07 score = 0.020\n"
     ]
    }
   ],
   "source": [
    "# Number of noise features to add\n",
    "num_nf = 10\n",
    "\n",
    "# Set random state\n",
    "rng = np.random.RandomState(23)\n",
    "\n",
    "# Create noise features\n",
    "noise = rng.uniform(0., 1.0, size=(len(iris.data), num_nf))\n",
    "\n",
    "# Features plus noise\n",
    "features_pn = np.hstack((features_ss, noise))\n",
    "\n",
    "# Feature names\n",
    "f_names = iris.feature_names.copy()\n",
    "for i in range(noise.shape[1]):\n",
    "    f_names.append(f'Noise {i:0>2}')\n",
    "    \n",
    "# Fit features plus noise\n",
    "fs = skb.fit(features_pn, labels)\n",
    "\n",
    "# Display scores for features and noise\n",
    "for var, name in sorted(zip(fs.scores_, f_names), key=lambda x: x[0], reverse=True):\n",
    "    print(f'{name:>18} score = {var:5.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "To provide additional insight into these techniques, we now switch to the adult income data set. In the following example, we change the score_func to mutual_info_classif which quantifies the dependency between features and the label. This measure uses a non-parametric (or model free) entropy measurement from the nearest neighbor distances. When two features are independent, this statistic goes to zero, and as the dependency increases the statistic also increases.\n",
    "\n",
    "Notice that we get a completely different order of importance when comparing to the result from variance threshold. In this case, since the adult data is an imbalanced dataset(25% high income and 75% low income), SelectKBest is a better choice to VarianceThreshold.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Relationship score = 0.114\n",
      "     MaritalStatus score = 0.113\n",
      "       CapitalGain score = 0.083\n",
      "         Education score = 0.067\n",
      "    EducationLevel score = 0.066\n",
      "               Age score = 0.065\n",
      "        Occupation score = 0.060\n",
      "      HoursPerWeek score = 0.041\n",
      "            FNLWGT score = 0.034\n",
      "       CapitalLoss score = 0.034\n",
      "               Sex score = 0.030\n",
      "         Workclass score = 0.021\n",
      "     NativeCountry score = 0.011\n",
      "              Race score = 0.009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "skb = SelectKBest(mutual_info_classif, k='all')\n",
    "fs = skb.fit(adult_features, adult_label)\n",
    "\n",
    "# Display scores for features and noise\n",
    "for var, name in sorted(zip(fs.scores_, adult_features.columns), key=lambda x: x[0], reverse=True):\n",
    "    print(f'{name:>18} score = {var:5.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Wrapper methods\n",
    "Wrapper methods consider the selection of a set of features as a search problem, where different combinations are prepared, evaluated and compared to other combinations. A predictive model is used to evaluate a combination of features and assign a score based on model accuracy. One popular wrapper method is the recursive feature elimination algorithm.\n",
    "\n",
    "\n",
    "### Recursive Feature Extraction\n",
    "\n",
    "[Recursive Feature Elimination (RFE)][rfe] works by recursively removing attributes and building a model from the remaining attributes. The model accuracy is used to identify the attributes (and combination of attributes) that most contribute to predicting the target (or held-out) attribute. The RFE implementation provided by the scikit learn library is in the `feature_selection` module.\n",
    "\n",
    "In the next few cells, we employ both RFE to determine the most important features for both the Iris and adult income  data. The first Code cell below uses a linear support vector classifier to perform RFE. In this case, we analyze the Iris data set plus ten _noise_ features. The result of this operation identifies the most important feature (since we specified that the top feature should be identified),  but also ranks the remaining features. Notice how three of the _real_ features are the top three ranked features, but next are several _noise_ features, indicating the remaining _real_ feature encodes less information than a random feature.\n",
    "\n",
    "-----\n",
    "\n",
    "[rfe]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  petal width (cm) rank = 1.000\n",
      " petal length (cm) rank = 2.000\n",
      "  sepal width (cm) rank = 3.000\n",
      "          Noise 03 rank = 4.000\n",
      "          Noise 09 rank = 5.000\n",
      "          Noise 00 rank = 6.000\n",
      " sepal length (cm) rank = 7.000\n",
      "          Noise 07 rank = 8.000\n",
      "          Noise 05 rank = 9.000\n",
      "          Noise 06 rank = 10.000\n",
      "          Noise 02 rank = 11.000\n",
      "          Noise 08 rank = 12.000\n",
      "          Noise 04 rank = 13.000\n",
      "          Noise 01 rank = 14.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Create classifier\n",
    "svc = LinearSVC(random_state=23)\n",
    "\n",
    "# Perform RFE, select top feature (but rank all)\n",
    "rfe = RFE(svc, 1)\n",
    "\n",
    "# Fit features plus noise\n",
    "fs = rfe.fit(features_pn, labels)\n",
    "    \n",
    "# Display scores for features and noise\n",
    "for var, name in sorted(zip(fs.ranking_, f_names), key=lambda x: x[0]):\n",
    "    print(f'{name:>18} rank = {var:5.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We now transition to the adult income data set. The following code cell takes considerable time to run. The reason is that RFE trains the selected model(RandomeForestClassifier in this case) repeatly to rank the features. It'll take a while when the dataset is large.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            FNLWGT rank = 1.000\n",
      "      Relationship rank = 2.000\n",
      "       CapitalGain rank = 3.000\n",
      "               Age rank = 4.000\n",
      "    EducationLevel rank = 5.000\n",
      "      HoursPerWeek rank = 6.000\n",
      "        Occupation rank = 7.000\n",
      "         Workclass rank = 8.000\n",
      "     MaritalStatus rank = 9.000\n",
      "       CapitalLoss rank = 10.000\n",
      "         Education rank = 11.000\n",
      "     NativeCountry rank = 12.000\n",
      "               Sex rank = 13.000\n",
      "              Race rank = 14.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state=23)\n",
    "# Create RFE model with only one feature\n",
    "rfe = RFE(rfc, 1)\n",
    "fs = rfe.fit(adult_features, adult_label)\n",
    "\n",
    "# Display scores for features and noise\n",
    "for var, name in sorted(zip(fs.ranking_, adult_features.columns), key=lambda x: x[0]):\n",
    "    print(f'{name:>18} rank = {var:5.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "Now that you have run the previous cells, try making changes to the notebook:\n",
    "\n",
    "1. Try using a different classifier, such as a decision tree or logistic regression.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Embedded Methods\n",
    "Embedded Methods perform feature selection directly in the model construction. For example, decision tree-based techniques compute feature importance. This extra information can be used to rank features for use with these models.\n",
    "\n",
    "## Select From Model\n",
    "\n",
    "Of course some algorithms, such as the decision tree and the ensemble techniques based on a decision tree, provide access to measures of the feature importance. For example, the [Random Forest Classifier (RFC)][rfc], as an ensemble method, builds models by randomly selecting features when building each tree. In this process, RFC computes the overall importance of each feature in building the final model. By extracting the feature importances from the final model, we obtain a ranked ordering of the features used to build the model.\n",
    "\n",
    "In the following Code cell, we employ RFC to determine the most important features for the Iris data set (along with the added _noise_ features). With this estimator, the four _real_ features are all ranked as the most important, followed by the _noise_ features. However, the sepal features are ranked with low importance, nearly the same as the noise features. This explains why different techniques can produce different feature rankings, and small changes to the algorithm or any inherent randomness in the process can generate different results.\n",
    "\n",
    "-----\n",
    "\n",
    "[rfc]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label             : Importance\n",
      "--------------------------\n",
      " petal length (cm): 40.89%\n",
      "  petal width (cm): 24.89%\n",
      " sepal length (cm): 06.57%\n",
      "  sepal width (cm): 04.34%\n",
      "          Noise 04: 04.11%\n",
      "          Noise 03: 03.62%\n",
      "          Noise 02: 03.38%\n",
      "          Noise 00: 02.80%\n",
      "          Noise 01: 02.29%\n",
      "          Noise 05: 02.22%\n",
      "          Noise 09: 01.99%\n",
      "          Noise 07: 01.47%\n",
      "          Noise 06: 01.19%\n",
      "          Noise 08: 00.25%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Build model\n",
    "rfc = RandomForestClassifier(random_state=23)\n",
    "rfc.fit(features_pn, labels)\n",
    "\n",
    "# Display scores for features and noise\n",
    "print(f'{\"Label\":18s}: Importance')\n",
    "print(26*'-')\n",
    "for val, name in sorted(zip(rfc.feature_importances_, f_names), \n",
    "                        key=lambda x: x[0], reverse=True):\n",
    "    print(f'{name:>18}: {100.0*val:05.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We now use randome forest classifier to determine feature importance in  the adult income data. \n",
    "\n",
    "-----\n",
    "[sfm]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label             : Importance\n",
      "--------------------------\n",
      "            FNLWGT: 17.25%\n",
      "               Age: 14.53%\n",
      "       CapitalGain: 11.39%\n",
      "    EducationLevel: 09.94%\n",
      "      Relationship: 09.46%\n",
      "      HoursPerWeek: 08.02%\n",
      "     MaritalStatus: 07.29%\n",
      "        Occupation: 06.72%\n",
      "       CapitalLoss: 03.91%\n",
      "         Workclass: 03.91%\n",
      "         Education: 03.00%\n",
      "     NativeCountry: 01.85%\n",
      "               Sex: 01.39%\n",
      "              Race: 01.34%\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=23)\n",
    "rfc.fit(adult_features, adult_label)\n",
    "\n",
    "# Display scores for features and noise\n",
    "print(f'{\"Label\":18s}: Importance')\n",
    "print(26*'-')\n",
    "for val, name in sorted(zip(rfc.feature_importances_, adult_features.columns), \n",
    "                        key=lambda x: x[0], reverse=True):\n",
    "    print(f'{name:>18}: {100.0*val:05.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "Now that you have run the previous cells, try making changes to the\n",
    "notebook:\n",
    "\n",
    "1. Try changing some hyperparameters(ie. max_depth) for the RFC estimator. How does these changes affect the feature importance?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. Detailed discussion of [feature engineering][1]\n",
    "1. Series of blog articles on feature selection in Python: [Part I][2a], [Part II][2b], [Part II][2c], and [Part IV][2d]\n",
    "2. An introduction to [feature selection][3]\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "[1]: http://adataanalyst.com/machine-learning/comprehensive-guide-feature-engineering/\n",
    "[2a]: http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/\n",
    "[2b]: http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/\n",
    "[2c]: http://blog.datadive.net/selecting-good-features-part-iii-random-forests/\n",
    "[2d]: http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/\n",
    "\n",
    "[3]: https://machinelearningmastery.com/an-introduction-to-feature-selection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2019: Gies College of Business at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
