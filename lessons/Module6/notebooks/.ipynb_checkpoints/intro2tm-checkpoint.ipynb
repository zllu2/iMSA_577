{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Classification II\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore more advanced machine learning techniques with text data. First, we further explore the text classification we did in the previous lesson to see how can we improve. Next, we introduce the concept of n-grams, which are combinations of one or more tokens. For example, bigrams are combinations of two tokens, while trigrams are combinations of three tokens. Finally, we introdce sentiment analysis with a new text data set.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Explore Classifiers](#Explore-Classifiers)\n",
    "\n",
    "[N-Grams](#N-Grams)\n",
    "\n",
    "[N-Gram Classification](#N-Gram-Classification)\n",
    "\n",
    "[Sentiment Analysis](#Sentiment-Analysis)\n",
    "\n",
    "[Stemming](#Stemming)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the rest of this notebook, we first include the notebook setup code and we define our _home_ directory.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "\n",
    "# We do this to ignore several specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Classifiers\n",
    "\n",
    "Let's explore the classifiers we used in the previous lesson. We can get top features(words, tokens) used by a classifier to predict each class. By examining the top words of each class, we get better understanding of the data set and the classifier. We hope this will help us improve the classification.\n",
    "\n",
    "We first repeat the text classification on 20newsgroup dataset with MultinomialNB classifier. In the next code cell, we load the data and create DTM with `TfidfVectorizer`, then train the `MultinomialNB` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (TF-IDF with Stop Words) prediction accuracy =  81.7%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# To learn more about these data, either browse the relevant \n",
    "# scikit learn documentation, or enter help(fetch_20newsgroups) in a Code cell\n",
    "\n",
    "# Get training text set\n",
    "train = fetch_20newsgroups(subset='train')\n",
    "# Get testing text set\n",
    "test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "# Create DTM\n",
    "tf_cv = TfidfVectorizer(stop_words='english')\n",
    "train_dtm_tf = tf_cv.fit_transform(train['data'])\n",
    "test_dtm_tf = tf_cv.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm_tf, train['target'])\n",
    "\n",
    "predicted = nb.predict(test_dtm_tf)\n",
    "scr = 100.0 * nb.score(test_dtm_tf, test['target'])\n",
    "print(f'NB (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can use the `coef_` attribute of a linear classifier(A linear classifier makes a classification decision based on the value of a linear combination of the characteristics or features. ie. MultinomialNB, LinearSVC, LogisticRegressoin) to identify the top features(words, tokens) for each category. `coef_` holds coefficients of all the features for each class. Since in text analysis, the features are tokens(words) in the whole training text set. Larger coefficient means more impact of the feature on predicting the class.\n",
    "\n",
    "In the following code cell we use `coef_` to find out top 5 features(words, tokens) used to predicting each class. I'll explain the code a bit:\n",
    "\n",
    "- first get all features from the vectorizer and convert the return value to a numpy array.\n",
    "- iterate through all classes, in our case, the 20 newsgroups\n",
    "- for each class, sort the classifiers' `coef_` attribute. For example, `nb.coef_[0]` is a list of coefficients for the first class, or _alt.atheism_ in our case. `np.argsort` is a numpy sort function that sorts a numpy array, but return a numpy array with indexes of the sorted array. For example, `np.argsort(np.array([2, 1, 3]))` returns a numpy array `[1, 0, 2]`. Because in the original array, item with index 1 is the samllest, item with index 0 is the second smallest and item with index 2 is the largest.\n",
    "- Once we get index of sorted coefficient, keep the last 5 indexes as top_word_index since those indexes are corresponding to the top 5 largest coefficients.\n",
    "- Use the top_word_index which is an array of indexes to find the top feature names(words)\n",
    "- Reverse the list of words so that the most important one is at the first.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alt.atheism:\n",
      "['keith', 'edu', 'god', 'caltech', 'atheists']\n",
      "\n",
      "comp.graphics:\n",
      "['graphics', 'edu', 'image', '3d', 'files']\n",
      "\n",
      "comp.os.ms-windows.misc:\n",
      "['windows', 'edu', 'file', 'dos', 'files']\n",
      "\n",
      "comp.sys.ibm.pc.hardware:\n",
      "['scsi', 'drive', 'ide', 'card', 'edu']\n",
      "\n",
      "comp.sys.mac.hardware:\n",
      "['mac', 'apple', 'edu', 'drive', 'quadra']\n",
      "\n",
      "comp.windows.x:\n",
      "['window', 'motif', 'mit', 'server', 'com']\n",
      "\n",
      "misc.forsale:\n",
      "['sale', 'edu', '00', 'offer', 'shipping']\n",
      "\n",
      "rec.autos:\n",
      "['car', 'com', 'cars', 'edu', 'engine']\n",
      "\n",
      "rec.motorcycles:\n",
      "['bike', 'com', 'dod', 'edu', 'ride']\n",
      "\n",
      "rec.sport.baseball:\n",
      "['edu', 'baseball', 'year', 'team', 'game']\n",
      "\n",
      "rec.sport.hockey:\n",
      "['hockey', 'team', 'game', 'ca', 'edu']\n",
      "\n",
      "sci.crypt:\n",
      "['key', 'clipper', 'encryption', 'chip', 'com']\n",
      "\n",
      "sci.electronics:\n",
      "['edu', 'com', 'use', 'lines', 'subject']\n",
      "\n",
      "sci.med:\n",
      "['pitt', 'edu', 'geb', 'banks', 'gordon']\n",
      "\n",
      "sci.space:\n",
      "['space', 'nasa', 'edu', 'henry', 'moon']\n",
      "\n",
      "soc.religion.christian:\n",
      "['god', 'jesus', 'christians', 'church', 'edu']\n",
      "\n",
      "talk.politics.guns:\n",
      "['gun', 'edu', 'guns', 'com', 'people']\n",
      "\n",
      "talk.politics.mideast:\n",
      "['israel', 'israeli', 'jews', 'turkish', 'armenian']\n",
      "\n",
      "talk.politics.misc:\n",
      "['edu', 'com', 'cramer', 'optilink', 'people']\n",
      "\n",
      "talk.religion.misc:\n",
      "['god', 'sandvik', 'jesus', 'edu', 'com']\n"
     ]
    }
   ],
   "source": [
    "# Display top 5 important words\n",
    "all_words = np.array(tf_cv.get_feature_names())\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_word_index = np.argsort(nb.coef_[idx])[-5:]\n",
    "    tn_lst = [word for word in all_words[top_word_index]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print(f'\\n{target}:')\n",
    "    print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "From the top words that are used to identify each class(label), we can see that \"edu\" and \"com\" are in the top 5 words of many classes. Those words are from email addresses which every message has. They don't really have much predicting power. The fact that they are in top 5 of so many classes proves this. In the previous lesson we mentioned that we hope that TF-IDF can help mitigate the problem but seems it doesn't. We can solve this problem by defining our own stop words and add \"com\" and \"edu\" to the stop words to filter them out manually. We'll also add \"re\" which is in many message headers to the stop words. We demonstrate this approach in the following code cell.\n",
    "\n",
    "The change doesn't improve the classifcation accuracy. This shows that not all improvements work as desired. But with the customized stop words, we do get more meaningful top words. This can be very valuable information sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (TF-IDF with Stop Words) prediction accuracy =  81.1%\n"
     ]
    }
   ],
   "source": [
    "#get current stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#add com, edu and re to stop words\n",
    "stop_words.extend(['com', 'edu', 're'])\n",
    "\n",
    "# Create DTM, use custom defined stop words\n",
    "tf_cv = TfidfVectorizer(stop_words=stop_words)\n",
    "train_dtm_tf = tf_cv.fit_transform(train['data'])\n",
    "test_dtm_tf = tf_cv.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm_tf, train['target'])\n",
    "\n",
    "predicted = nb.predict(test_dtm_tf)\n",
    "scr = 100.0 * nb.score(test_dtm_tf, test['target'])\n",
    "print(f'NB (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alt.atheism:\n",
      "['keith', 'god', 'caltech', 'atheists', 'livesey']\n",
      "\n",
      "comp.graphics:\n",
      "['graphics', 'image', '3d', 'files', 'lines']\n",
      "\n",
      "comp.os.ms-windows.misc:\n",
      "['windows', 'file', 'dos', 'files', 'driver']\n",
      "\n",
      "comp.sys.ibm.pc.hardware:\n",
      "['scsi', 'drive', 'ide', 'card', 'bus']\n",
      "\n",
      "comp.sys.mac.hardware:\n",
      "['mac', 'apple', 'drive', 'quadra', 'se']\n",
      "\n",
      "comp.windows.x:\n",
      "['window', 'motif', 'mit', 'server', 'widget']\n",
      "\n",
      "misc.forsale:\n",
      "['sale', '00', 'offer', 'shipping', 'new']\n",
      "\n",
      "rec.autos:\n",
      "['car', 'cars', 'engine', 'article', 'would']\n",
      "\n",
      "rec.motorcycles:\n",
      "['bike', 'dod', 'ride', 'bikes', 'motorcycle']\n",
      "\n",
      "rec.sport.baseball:\n",
      "['baseball', 'year', 'team', 'game', 'players']\n",
      "\n",
      "rec.sport.hockey:\n",
      "['hockey', 'team', 'game', 'ca', 'nhl']\n",
      "\n",
      "sci.crypt:\n",
      "['key', 'clipper', 'encryption', 'chip', 'keys']\n",
      "\n",
      "sci.electronics:\n",
      "['use', 'lines', 'one', 'subject', 'power']\n",
      "\n",
      "sci.med:\n",
      "['pitt', 'geb', 'banks', 'gordon', 'msg']\n",
      "\n",
      "sci.space:\n",
      "['space', 'nasa', 'henry', 'moon', 'alaska']\n",
      "\n",
      "soc.religion.christian:\n",
      "['god', 'jesus', 'christians', 'church', 'bible']\n",
      "\n",
      "talk.politics.guns:\n",
      "['gun', 'guns', 'would', 'people', 'fbi']\n",
      "\n",
      "talk.politics.mideast:\n",
      "['israel', 'israeli', 'jews', 'turkish', 'armenian']\n",
      "\n",
      "talk.politics.misc:\n",
      "['cramer', 'optilink', 'people', 'clinton', 'writes']\n",
      "\n",
      "talk.religion.misc:\n",
      "['god', 'sandvik', 'jesus', 'christian', 'kent']\n"
     ]
    }
   ],
   "source": [
    "# Display top 5 important words\n",
    "all_words = np.array(tf_cv.get_feature_names())\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_word_index = np.argsort(nb.coef_[idx])[-5:]\n",
    "    tn_lst = [word for word in all_words[top_word_index]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print(f'\\n{target}:')\n",
    "    print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next we apply LogisticRegression on the DTM which is created with customized stop words. The classification accuracy again doesn't improve. Then we print out the top 5 words from each class. We can see that `LogisticRegerssion` gives different top words to that of `MultinomialNB`. This is because that different models use different features for prediction. There are still quite a few overlaps of words in the top words of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR (TF-IDF with Stop Words) prediction accuracy =  84.9%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1000)\n",
    "\n",
    "lr = lr.fit(train_dtm_tf, train['target'])\n",
    "predicted = lr.predict(test_dtm_tf)\n",
    "\n",
    "scr = 100.0 * lr.score(test_dtm_tf, test['target'])\n",
    "print(f'LR (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alt.atheism:\n",
      "['atheism', 'keith', 'atheists', 'islamic', 'cobb']\n",
      "\n",
      "comp.graphics:\n",
      "['graphics', '3d', '3do', 'image', 'pov']\n",
      "\n",
      "comp.os.ms-windows.misc:\n",
      "['windows', 'cica', 'win3', '13', 'ax']\n",
      "\n",
      "comp.sys.ibm.pc.hardware:\n",
      "['gateway', 'pc', 'pentium', '486', 'ide']\n",
      "\n",
      "comp.sys.mac.hardware:\n",
      "['mac', 'apple', 'quadra', 'powerbook', 'duo']\n",
      "\n",
      "comp.windows.x:\n",
      "['motif', 'widget', 'server', 'x11r5', 'window']\n",
      "\n",
      "misc.forsale:\n",
      "['sale', 'scanning', 'wanted', 'distribution', 'forsale']\n",
      "\n",
      "rec.autos:\n",
      "['car', 'cars', 'toyota', 'automotive', 'dealer']\n",
      "\n",
      "rec.motorcycles:\n",
      "['dod', 'bike', 'bikes', 'motorcycle', 'riding']\n",
      "\n",
      "rec.sport.baseball:\n",
      "['baseball', 'phillies', 'sox', 'cubs', 'tigers']\n",
      "\n",
      "rec.sport.hockey:\n",
      "['hockey', 'nhl', 'playoff', 'pens', 'team']\n",
      "\n",
      "sci.crypt:\n",
      "['clipper', 'key', 'encryption', 'tapped', 'security']\n",
      "\n",
      "sci.electronics:\n",
      "['electronics', 'circuit', '8051', 'ee', 'tv']\n",
      "\n",
      "sci.med:\n",
      "['doctor', 'msg', 'disease', 'photography', 'medical']\n",
      "\n",
      "sci.space:\n",
      "['space', 'orbit', 'moon', 'launch', 'dc']\n",
      "\n",
      "soc.religion.christian:\n",
      "['clh', 'athos', 'church', 'rutgers', 'christ']\n",
      "\n",
      "talk.politics.guns:\n",
      "['guns', 'gun', 'firearms', 'waco', 'atf']\n",
      "\n",
      "talk.politics.mideast:\n",
      "['israeli', 'israel', 'turkish', 'armenians', 'armenian']\n",
      "\n",
      "talk.politics.misc:\n",
      "['kaldis', 'clinton', 'tax', 'cramer', 'drugs']\n",
      "\n",
      "talk.religion.misc:\n",
      "['beast', 'morality', 'christian', '666', 'koresh']\n"
     ]
    }
   ],
   "source": [
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_word_index = np.argsort(lr.coef_[idx])[-5:]\n",
    "    tn_lst = [word for word in all_words[top_word_index]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print(f'\\n{target}:')\n",
    "    print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we print out top words used in predicting each class for MultinomialNB and LogisticRegression. In the following code cell, try display top 5 words used by LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## N-Grams\n",
    "\n",
    "Formally, a [_n-gram_][ngd] is a contiguous sequence of **n** items from a parent sequence of items, such as characters or words in a text document. In general, we will focus solely on words in a document. Thus, our initial approach has simply been to look at unigrams or single words in a document when building a classification model. However, sometimes the combination of words can be more descriptive, for example, _unbelievably bad_ is generally viewed as a more powerful description than just _bad_. As a result, the concept of an _n-gram_ was created, where collections of words can be treated as features. In fact google allows a user to search for [specific n-gram][gnv] combinations in books that they have scanned.\n",
    "\n",
    "While this clearly can improve classification power, it also increases computational requirements. This is a result of the rise in the number of possible features. While this is not a problem for small vocabularies, for larger vocabularies (and corresponding documents) the number of possible features can quickly become very large. Thus, many text mining applications will make use of Hadoop or Spark clusters to leverage the inherent parallelism in these tasks.\n",
    "\n",
    "To demonstrate using n-grams, we first demonstrate the concept on a single sentence.\n",
    "\n",
    "-----\n",
    "[gnv]: https://books.google.com/ngrams\n",
    "[ngd]: https://en.wikipedia.org/wiki/N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'course', 'introduces', 'many', 'concepts', 'in', 'data', 'science', 'this course', 'course introduces', 'introduces many', 'many concepts', 'concepts in', 'in data', 'data science', 'this course introduces', 'course introduces many', 'introduces many concepts', 'many concepts in', 'concepts in data', 'in data science']\n"
     ]
    }
   ],
   "source": [
    "my_text = 'This course introduces many concepts in data science.'\n",
    "\n",
    "# Tokenize sentance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "# Analyze sentance\n",
    "tk_func = cv.build_analyzer()\n",
    "\n",
    "# Display n-grams\n",
    "print(tk_func(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "`my_text` has 8 words. When we set `ngram_range=(1,3)` which means create tokens for 1, 2 and 3 consecutive words, there are 21 tokens now.\n",
    "\n",
    "We can create a new document-term matrix based on this sentence, and use this matrix (or vector since it is only one row), to provide a representation space for new sentences. In the following Code cell, we tokenize our original sentence, and sort the resulting vocabulary (i.e., n-grams) with their ranking (or order). Next, we create a second, simple sentence and compute the indices into the original DTM for the n-grams in the new sentence. The result is display as a bit vector, where `1` means the corresponding n-gram is in the new sentence, and `0` means it is not.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token mapping:\n",
      "----------------------------------------\n",
      "0 concepts\n",
      "1 concepts in\n",
      "2 concepts in data\n",
      "3 course\n",
      "4 course introduces\n",
      "5 course introduces many\n",
      "6 data\n",
      "7 data science\n",
      "8 in\n",
      "9 in data\n",
      "10 in data science\n",
      "11 introduces\n",
      "12 introduces many\n",
      "13 introduces many concepts\n",
      "14 many\n",
      "15 many concepts\n",
      "16 many concepts in\n",
      "17 science\n",
      "18 this\n",
      "19 this course\n",
      "20 this course introduces\n",
      "----------------------------------------\n",
      "['This course is data science!']\n",
      "----------------------------------------\n",
      "[[0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentence\n",
    "cv = cv.fit([my_text])\n",
    "\n",
    "# Sort tokens\n",
    "import operator\n",
    "my_voc = sorted(cv.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "\n",
    "# Display token mapping\n",
    "print('Token mapping:')\n",
    "print(40*'-')\n",
    "\n",
    "for tokens, rank in my_voc:\n",
    "    print(rank, tokens)\n",
    "\n",
    "# Display new sentence\n",
    "print(40*'-')\n",
    "out_list = ['This course is data science!']\n",
    "\n",
    "# Transform new sentence to original sentence DTM\n",
    "xsm = cv.transform(out_list)\n",
    "print(out_list)\n",
    "\n",
    "# Display count vector indices for new sentance tokens\n",
    "print(40*'-')\n",
    "print(xsm.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we used `CountVectorizer` to create n-gram tokens. Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the `CountVectorizer` to use stop words. How does this change the tokens and mappings? \n",
    "2. Try changing the ngram range to different values (e.g., `ngram_range=(1,4)`). Notice how the number of tokens quickly increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## N-Gram Classification\n",
    "\n",
    "Having n-grams often offers improved classification, since word or token combinations often include more information than single words or tokens. For example, _University Illinois_ means more than just _University_ and _Illinois_. We can build on our previous simple text classification to now develop a more complete code example that builds a feature vector containing both single words and n-grams from the documents. We use this new sparse matrix to classify the documents by using our simple Naive Bayes classifier. We actually get a slightly worse accuracy score. This shows again that not all the optimzation attemps will improve the model performance.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (TF-IDF with ngram_range 1-2) prediction accuracy =  80.7%\n"
     ]
    }
   ],
   "source": [
    "# Create DTM\n",
    "tf_cv = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "train_dtm_tf = tf_cv.fit_transform(train['data'])\n",
    "test_dtm_tf = tf_cv.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm_tf, train['target'])\n",
    "\n",
    "predicted = nb.predict(test_dtm_tf)\n",
    "scr = 100.0 * nb.score(test_dtm_tf, test['target'])\n",
    "print(f'NB (TF-IDF with ngram_range 1-2) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we set ngram_range to (1,2) and apply MultinomialNB. In the next code cell, try applying other classifiers like LinearSRC or LogisticRegression with ngram_range (1,2) to see how the results change.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "Sentiment Analysis aims to identify and extract emotion or opinion from text. It is a useful technology that businesses can apply in social media, customer reviews, and customer support.\n",
    "\n",
    "In this section, we turn to a classification problem where the goal is to classify based on sentiment, negative or positive. We will approach this as a simple classification problem. \n",
    "\n",
    "We first load our data. We will use the movie reviews data built-in in the `nltk.corpus` module. The movie_reviews corpus contains 2000 movie reviews with sentiment polarity classification. Out of the 2000 reviews, the first 1000 are negative reviews, and next 1000 are positive reviews.\n",
    "\n",
    "In the following code cell, we first load the movie reviews corpus from ntlk. Each review has a sentiment label, either _'neg'_ or _'pos'_, stand for negative and positive respectively. The reviews are stored as list of words. We combine the list of words to generate the text reviews, which will be our text data. We then create classification label by mapping _'neg'_ to 0 and _'pos'_ to 1. Finally, we split the data set to training and testing date set. We print out the first review and its label in the training text set to show what a review looks like.\n",
    "\n",
    "**Note:** Don't worry if you have trouble to understand the code to load the movie reviews from nltk. Just focus on the final text data set.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews: 2000\n",
      "Number of Negative Reviews: 1000\n",
      "Number of Positive Reviews: 1000\n",
      "Sample Review:\n",
      "Label: 0\n",
      "not a great twelve months for either of the principals from this movie . earlier this year , nora ephron wrote and produced one of the year ' s least likeable \" comedies \" called hanging up , featuring a bunch of annoying women ( ironically , lisa kudrow played one in that film as well ) who barely have time to care about anyone but themselves . ick . . . real sweet stuff . but her little unsuccessful project was nothing compared to what john travolta went through earlier this summer , with a film entitled battlefield earth . i seemed to be the only person on this planet who somehow appreciated the film , even if it was on a \" cheese \" factor , as everybody , and i mean everybody else , pretty much classified the movie as one of the worst disasters of all time . yipes . . . another beauty . so what happens when you put these two people in the same room and come out with a movie co - starring the ever - versatile lisa kudrow ? uhhhm , you guessed it . . . not much . plot : in order to escape major financial difficulties , a local weatherman hooks up with his ball - picking lotto girlfriend and rigs the state lottery . but as more and more people find out about their scheme , more and more people demand a part of their winnings , and more and more problems arise . critique : simply stated , i didn ' t laugh once during this entire picture . for a comedy , it offered me a few smiles , a bunch of nincompoops as characters , a miscast john travolta hamming it up and lisa kudrow , in what can only described as a \" sluttier \" version of her character of phoebe on tv ' s \" friends \" . this film was not as disastrous as i thought it would be , but it was pretty close . thankfully , the clips of travolta dressed up in goofy outfits , dancing as the weatherman were left in the film ' s trailer , and not in this final cut . and not unlike the worst movie of the year so far , beautiful , this film also managed to feature many unsympathetic , idiotic and just plain irritating characters in its cast . foremost was travolta ' s character , who declined to give us any reason to care for him once in the entire movie . and for me , the casting choice of john travolta for this role was just plain wrong . he didn ' t fit the part . i just saw him in get shorty the other day and thought about how perfect he was for that role . a cool , calculating roughneck with a certain hip , suave \" je ne sais quoi \" . in this film , he looks like he ' s trying to be funny , trying to be bad , trying to be good . we ' re not supposed to be able to notice that , and when we do , at least in my case , i consider it a wrong choice in casting . add that to lisa kudrow , boring us with yet another one of her patented \" dumb blonde \" routines , but this time , dressed in sexier outfits . michael rapaport , stretching one small acting muscle to play the guy who isn ' t quite up at the same speed level as everyone else . and a truckload of empty comedic bullet shells , and you ' ve got yourself an extremely quiet audience anticipating punch lines that never quite materialize . the only real good thing that i could say about this film is that its story was actually half - interesting and never really bored me . i also liked michael moore ' s perverted cousin character , and i loved , and i say it again , loved the character that bill pullman played . give this dude his own movie ! he played a lazy cop , a man who tries everything not to do any real work . he fakes injuries to get off duty , tries to avoid arrest situations so that he won ' t have to fill out any forms . . . now there ' s a base of humor . sadly , the filmmakers decided to bring him into play with only about half an hour left in the film . and there ' s not much else i can say about this movie , folks . on the whole , it was lame , included a slew of unlikable characters fiddling around in a pool of unfunny lines , and very little of interest for any target audience . but get somebody to write up a movie featuring that lazy cop played by pullman and i ' m there !\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import movie_reviews\n",
    "#load movie reviews, each review is a list of words\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "print (\"Number of Reviews:\", len(documents))\n",
    "#shuffle reviews to mix negative and positive reviews\n",
    "#set random seed for reproducibility\n",
    "random.seed(23)\n",
    "random.shuffle(documents)\n",
    "\n",
    "#list to store all review text\n",
    "text_data = []\n",
    "#label\n",
    "label = []\n",
    "for i in range(len(documents)):\n",
    "    #join list of words to create a review and add to text_data\n",
    "    text_data.append(' '.join(documents[i][0]))\n",
    "    #map neg to 0, pos to 1 and add to label\n",
    "    label.append(0 if documents[i][1]=='neg' else 1)\n",
    "\n",
    "print(\"Number of Negative Reviews:\", label.count(0))\n",
    "print(\"Number of Positive Reviews:\", label.count(1))    \n",
    "\n",
    "#split to train and text\n",
    "mvr_train, mvr_test, y_train, y_test = train_test_split(text_data, label, test_size=0.25, random_state=23)\n",
    "\n",
    "#print one example review in the training text set\n",
    "print(\"Sample Review:\")\n",
    "print('Label:', y_train[0])\n",
    "print(mvr_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next we create bag of words from training text set with `TfidfVectorizer`, transform both training and testing set to the document term matrix, then apply `MultinomaiNB` classifier. We print out the score, the classification report and plot the confusion matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (TF-IDF with stop words) prediction accuracy =  78.6%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.80      0.79       256\n",
      "    Positive       0.79      0.77      0.78       244\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       500\n",
      "   macro avg       0.79      0.79      0.79       500\n",
      "weighted avg       0.79      0.79      0.79       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes pipeline to classify\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "tf_cv = TfidfVectorizer(stop_words='english')\n",
    "train_dtm_tf = tf_cv.fit_transform(mvr_train)\n",
    "test_dtm_tf = tf_cv.transform(mvr_test)\n",
    "\n",
    "# Fit model, predict, and display results\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm_tf, y_train)\n",
    "y_pred = nb.predict(test_dtm_tf)\n",
    "scr = 100.0 * nb.score(test_dtm_tf, y_test)\n",
    "print(f'NB (TF-IDF with stop words) prediction accuracy = {scr:5.1f}%')\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = ['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper_code import mlplots as ml\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(13, 10))\n",
    "# Call confusion matrix plotting routine\n",
    "ml.confusion(y_test, y_pred, ['Negative', 'Positive'], 'Naive Bayes Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next we display top words in predicting positive reviews. Note that since there are only two classes, either 0 or 1. There's only one list of coefficient for positive class. Because the model only need to classify positive reviews and the rest are negative reviews. If you want to find out top words for predicting negative reviews, you may reverse the label, map _neg_ to 1 and _pos_ to 0 and repeat following code.\n",
    "\n",
    "We print out top 20 words for predicting positive movie reviews. There are _good_, _great_, _best_ and _love_ which make sense, but there are also many words like _film_, _movie_, _story_, and _really_ etc that don't make much sense. We can try add words like _film_ and _movie_ to stop words since those words are likely to appear in both positive and negative reviews. Or we can try another algorithm to see if we get better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Words of Positive Reviews:\n",
      "['film', 'movie', 'like', 'story', 'life', 'good', 'just', 'time', 'character', 'characters', 'films', 'great', 'best', 'does', 'way', 'love', 'really', 'people', 'man', 'little']\n"
     ]
    }
   ],
   "source": [
    "all_words = np.array(tf_cv.get_feature_names())\n",
    "\n",
    "top_word_index = np.argsort(nb.coef_[0])[-20:]\n",
    "tn_lst = [word for word in all_words[top_word_index]]\n",
    "tn_lst.reverse()\n",
    "\n",
    "print(f'\\nTop 20 Words of Positive Reviews:')\n",
    "print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We now try `LogisticRegression` with `C` set to `1000`. This model gives better accuracy and a much better list of top words for predicting positive reviews.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR (TF-IDF with Stop Words) prediction accuracy =  82.2%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=1000)\n",
    "\n",
    "lr = lr.fit(train_dtm_tf, y_train)\n",
    "predicted = lr.predict(test_dtm_tf)\n",
    "\n",
    "scr = 100.0 * lr.score(test_dtm_tf, y_test)\n",
    "print(f'LR (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Words of Positive Reviews:\n",
      "['great', 'fun', 'life', 'hilarious', 'memorable', 'overall', 'quite', 'different', 'good', 'terrific', 'especially', 'trek', 'works', 'seen', 'performances', 'perfect', 'perfectly', 'comic', 'town', 'gives']\n"
     ]
    }
   ],
   "source": [
    "top_word_index = np.argsort(lr.coef_[0])[-20:]\n",
    "tn_lst = [word for word in all_words[top_word_index]]\n",
    "tn_lst.reverse()\n",
    "\n",
    "print(f'\\nTop 20 Words of Positive Reviews:')\n",
    "print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next we reverse the label value, train the model again and get the top words for predicting negative reviews. Most words in the list make a lot of sense.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 words of Negative Reviews:\n",
      "['bad', 'worst', 'plot', 'supposed', 'unfortunately', 'harry', 'boring', 'script', 'stupid', 'reason', 'poor', 'awful', 'waste', 'cheap', 'attempt', 'jakob', 'dull', 'lame', 'looks', 'better']\n"
     ]
    }
   ],
   "source": [
    "#reverse label value so that negative reviews have label 1\n",
    "y_train_reverse = [0 if y==1 else 1 for y in y_train]\n",
    "lr = lr.fit(train_dtm_tf, y_train_reverse)\n",
    "\n",
    "top_word_index = np.argsort(lr.coef_[0])[-20:]\n",
    "tn_lst = [word for word in all_words[top_word_index]]\n",
    "tn_lst.reverse()\n",
    "\n",
    "print(f'\\nTop 20 words of Negative Reviews:')\n",
    "print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the previous code cells, we classified movie reviews with default English stop words. Try add common words in movie reviews like _movie_ and _film_ to stop words. What impact this change has on each model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Stemming\n",
    "\n",
    "In previous lesson, we introduced the concept of stemming. In the next code cell, we demonstrate how to apply stemming in text classification. We will use `PorterStemmer` in the `nltk` module for stemming.\n",
    "\n",
    "We first define a function `tokenize`. The function has one argument `text` which is the text to be tokenized. The function uses `nltk.word_tokenize` function to tokenize `text` then apply `PorterStemmer` to stem the tokens. We then  set `tokenizer` argument in `CounterVectorizer` or `TfidfVectorizer` with this custom `tokenize` function and use the new vectorizer to create bag of words.\n",
    "\n",
    "The following code takes longer to finish due to stemming, but it does give a better classification accuracy.\n",
    "\n",
    "-----\n",
    "[ws]: https://en.wikipedia.org/wiki/Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR (TF-IDF with Stemming) prediction accuracy =  84.2%\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Define function to tokenize text and apply stemmer\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    return stems\n",
    "\n",
    "# use custom tokenize when creating vectorizer\n",
    "tf_cv = TfidfVectorizer()#tokenizer=tokenize)\n",
    "train_dtm_tf = tf_cv.fit_transform(mvr_train)\n",
    "test_dtm_tf = tf_cv.transform(mvr_test)\n",
    "\n",
    "lr = LogisticRegression(C=1000)\n",
    "\n",
    "lr = lr.fit(train_dtm_tf, y_train)\n",
    "predicted = lr.predict(test_dtm_tf)\n",
    "\n",
    "scr = 100.0 * lr.score(test_dtm_tf, y_test)\n",
    "print(f'LR (TF-IDF with Stemming) prediction accuracy = {scr:5.1f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. Wikipedia articles on [n-grams][wng], [Stemming][wst], and [Lemmatization][wl]\n",
    "1. Google [n-gram viewer][gnv]\n",
    "1. Alternative [n-gram viewer][anv]\n",
    "1. Blog on Sentiment Analysis with NLTK, [Part III][bsa3] and [Part IV][bsa4]\n",
    "1. An online [Stemming Demo][std] using NLTK\n",
    "1. A [treatise on Snowball][tsb] discussing, in depth, the process of stemming.\n",
    "\n",
    "-----\n",
    "\n",
    "[wst]: https://en.wikipedia.org/wiki/Stemming\n",
    "[wl]: https://en.wikipedia.org/wiki/Lemmatisation\n",
    "[wtc]: https://en.wikipedia.org/wiki/Document_clustering\n",
    "\n",
    "[tsb]: http://snowball.tartarus.org/texts/introduction.html\n",
    "[std]: http://text-processing.com/demo/stem/\n",
    "\n",
    "[wng]: https://en.wikipedia.org/wiki/N-gram\n",
    "\n",
    "[gnv]: https://books.google.com/ngrams\n",
    "[anv]: http://xkcd.culturomics.org\n",
    "\n",
    "[bsa3]: http://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/\n",
    "[bsa4]: http://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/\n",
    "\n",
    "[msdr]: http://research.microsoft.com/pubs/150728/FnT_dimensionReduction.pdf\n",
    "[lle]: http://science.sciencemag.org/content/290/5500/2323.abstract\n",
    "[ica]: http://www.cs.rutgers.edu/~mlittman/topics/dimred02/kolenda99independent.pdf1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2019: Gies College of Business at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
