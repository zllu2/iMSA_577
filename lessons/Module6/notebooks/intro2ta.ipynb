{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analytics\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduce basic concepts in text analytics, which is one of the most exciting application areas for machine learning. Text analytics forms the basis for [natural language processing][nlp], and is explicitly used for [sentiment analysis][sa], for [language identification][lc], for [spelling and grammar correction][sc], as well as for mining text data contained in forms, such as [medical informatics][mi]. Before moving into text classification, in this notebook we focus on basic text analytics such as accessing data, tokenizing a corpus, and computing token frequencies. We demonstrate these tasks by using basic Python concepts and by using functionality from within the scikit-learn library. We also introduce the NLTK library and use methods within this library to perform text analytic tasks.\n",
    "\n",
    "---\n",
    "\n",
    "[nlp]: https://en.wikipedia.org/wiki/Natural_language_processing\n",
    "[sa]: https://en.wikipedia.org/wiki/Sentiment_analysis\n",
    "[lc]: https://en.wikipedia.org/wiki/Language_identification\n",
    "[sc]: http://norvig.com/spell-correct.html\n",
    "[mi]: https://en.wikipedia.org/wiki/Health_informatics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Text Data Preparation](#Text-Data-Preparation)\n",
    "- [Tokenization](#Tokenization)\n",
    "- [Remove Unwanted Characters](#Remove-Unwanted-Characters)\n",
    "- [Convert to Same Case](#Convert-to-Same-Case)\n",
    "- [NLTK](#NLTK)\n",
    "- [Stop Words](#Stop-Words)\n",
    "- [Stemming](#Stemming)\n",
    "\n",
    "[Bag of Words](#Bag-of-Words)\n",
    "- [CountVectorizer](#CountVectorizer)\n",
    "\n",
    "- [TF-IDF](#TF-IDF)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the rest of this notebook, we first include our notebook setup code.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We do this to ignore several specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Text Data Preparation\n",
    "    \n",
    "Unlike datasets we've used so far, text analysis datasets don't have many features. A typical text analysis dataset is just a collection of text, which is called **corpus**. Machine learning algorithms can't directly train on text, so we will have to first pre-process the text data. **Tokenization** is normally the first step in text analysis.\n",
    "\n",
    "### Tokenization\n",
    "Tokenization is a process to break down a paragraph of text to smaller chunks, normally a single word or a phrase, which are called tokens.\n",
    "\n",
    "In the next Code cell, we tokenize a paragraph of text to single words with string function `split`, then print out all the words or tokens in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'Learning', 'is', 'a', 'technique', 'of', 'parsing', 'data,', 'learn', 'from', 'that', 'data', 'and', 'then', 'apply', 'what', 'is', 'learned', 'to', 'make', 'an', 'informed', 'decision.', 'Machine', 'learning', 'focuses', 'on', 'designing', 'algorithms', 'that', 'can', 'learn', 'from', 'and', 'make', 'predictions', 'on', 'the', 'data.', 'The', 'learning', 'can', 'be', 'supervised', 'or', 'unsupervised.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Machine Learning is a technique of parsing data, learn from that data and then \n",
    "apply what is learned to make an informed decision. Machine learning focuses on \n",
    "designing algorithms that can learn from and make predictions on the data. \n",
    "The learning can be supervised or unsupervised.\n",
    "\"\"\"\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "With the collection of tokens, we can make a small step forward: get count of tokens. In the following code we will use Python `collections` module to get a count of words in the text. We can already get some useful information from the word count. For example, a few top-used words may give us a better idea what this piece of text is about. In the next Code cell, we will print the word count, as well as the top 10 most used words in the text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count:\n",
      " Counter({'Machine': 2, 'is': 2, 'learn': 2, 'from': 2, 'that': 2, 'and': 2, 'make': 2, 'learning': 2, 'on': 2, 'can': 2, 'Learning': 1, 'a': 1, 'technique': 1, 'of': 1, 'parsing': 1, 'data,': 1, 'data': 1, 'then': 1, 'apply': 1, 'what': 1, 'learned': 1, 'to': 1, 'an': 1, 'informed': 1, 'decision.': 1, 'focuses': 1, 'designing': 1, 'algorithms': 1, 'predictions': 1, 'the': 1, 'data.': 1, 'The': 1, 'be': 1, 'supervised': 1, 'or': 1, 'unsupervised.': 1})\n",
      "\n",
      " Top 10 Words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Machine', 2),\n",
       " ('is', 2),\n",
       " ('learn', 2),\n",
       " ('from', 2),\n",
       " ('that', 2),\n",
       " ('and', 2),\n",
       " ('make', 2),\n",
       " ('learning', 2),\n",
       " ('on', 2),\n",
       " ('can', 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections as cl\n",
    "\n",
    "# Tokenize and create counter\n",
    "words = text.split()\n",
    "wc = cl.Counter(words)\n",
    "print(\"Word Count:\\n\", wc)\n",
    "print(\"\\n Top 10 Words:\")\n",
    "wc.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's examine the output of previous Code cell. You may have already found some issues from the output. In the word count, `'data'`, `'data,'` and `'data.'` are considered as different words due to the connected punctuations; `'Learning'` and `'learning'` are considered as different words due to different cases. These two issues have big impact on the effectiveness of our analysis but they are easy to fix.\n",
    "\n",
    "---\n",
    "\n",
    "### Remove Unwanted Characters\n",
    "In text analysis, we want pure *text*. We want to remove all the special characters which typically are any characters that are not letters or numbers. One simply way to remove special characters is to use _regular expression_. A regular expression, also known as Regex, is a sequence of characters that define a search pattern. For example, this Regex `r'[^\\w]'` matches all characters that are not letters(both upper or lower case) or numbers.\n",
    "\n",
    "In the following Code cell, we will use Python Regular Expression module [re](https://docs.python.org/3/library/re.html) to replace all special characters in `text` with whitespace.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning is a technique of parsing data  learn from that data and then  apply what is learned to make an informed decision  Machine learning focuses on  designing algorithms that can learn from and make predictions on the data   The learning can be supervised or unsupervised  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text_ns = re.sub(r'[^\\w]',' ', text)\n",
    "print(text_ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Regular expression is very powerful. In our case, we eliminate all characters that are not letter or number. Sometimes, you may want to keep some information that contains special characters, like '@' in email addresses. You can then define more complicated regular expression to keep certain pattern of characters. To learn more about Python Regular Expression, you may refer to Python [re module](https://docs.python.org/3/library/re.html).\n",
    "\n",
    "### Convert to Same Case\n",
    "Next, we will convert all the words to same case. In the following code we will use string function `lower()` to convert all characters to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning is a technique of parsing data  learn from that data and then  apply what is learned to make an informed decision  machine learning focuses on  designing algorithms that can learn from and make predictions on the data   the learning can be supervised or unsupervised  \n"
     ]
    }
   ],
   "source": [
    "text_nsl = text_ns.lower()\n",
    "print(text_nsl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The text is much cleaner now. Let's perform word count on the cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count:\n",
      " Counter({'learning': 3, 'data': 3, 'machine': 2, 'is': 2, 'learn': 2, 'from': 2, 'that': 2, 'and': 2, 'make': 2, 'on': 2, 'can': 2, 'the': 2, 'a': 1, 'technique': 1, 'of': 1, 'parsing': 1, 'then': 1, 'apply': 1, 'what': 1, 'learned': 1, 'to': 1, 'an': 1, 'informed': 1, 'decision': 1, 'focuses': 1, 'designing': 1, 'algorithms': 1, 'predictions': 1, 'be': 1, 'supervised': 1, 'or': 1, 'unsupervised': 1})\n",
      "\n",
      " Top 10 Words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('learning', 3),\n",
       " ('data', 3),\n",
       " ('machine', 2),\n",
       " ('is', 2),\n",
       " ('learn', 2),\n",
       " ('from', 2),\n",
       " ('that', 2),\n",
       " ('and', 2),\n",
       " ('make', 2),\n",
       " ('on', 2)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc = cl.Counter(text_nsl.split())\n",
    "print(\"Word Count:\\n\", wc)\n",
    "print(\"\\n Top 10 Words:\")\n",
    "wc.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The new list of most used words gives us a better idea on what the text is about. However, if we examine the list more closely, we can see that there's another issue. \n",
    "\n",
    "Some words in the list, like 'from', 'that', 'and', 'on', 'can', don't give any useful information. They are _useless_ for text analysis and can be ignored. This kind of frequently used but useless words are called **stop words**. We can use Python [Natural Language ToolKit][nltk], or NLTK module to deal with stop words. \n",
    "\n",
    "---\n",
    "\n",
    "### NLTK\n",
    "\n",
    "The scikit-learn library is a general purpose, Python machine learning library that does include some basic text analysis functionality. Text analysis, however, is an extremely large and growing topic. As a result, we want to explore an additional library for natural language processing. This library, known as [Natural Language ToolKit][nltk], or NLTK, enables a wide range of text analyses either on its own, or in conjunction with scikit-learn. The NLTK library is extensive and includes [documentation][nltkd] covering many of the topics we have demonstrated previously.\n",
    "\n",
    "In the rest of this notebook, we will explore how to use NLTK to perform basic text analysis, in a similar manner as demonstrated earlier via standard Python and the scikit-learn library. \n",
    "\n",
    "-----\n",
    "[nltk]: http://www.nltk.org\n",
    "[nltkd]: http://www.nltk.org/book/\n",
    "\n",
    "### Stop Words\n",
    "In the following two Code cells, we will use NLTK module to remove stop words from the text. We will first print out all English stop words, then we will remove all the stop words from the word list.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count:\n",
      " Counter({'learning': 3, 'data': 3, 'machine': 2, 'learn': 2, 'make': 2, 'technique': 1, 'parsing': 1, 'apply': 1, 'learned': 1, 'informed': 1, 'decision': 1, 'focuses': 1, 'designing': 1, 'algorithms': 1, 'predictions': 1, 'supervised': 1, 'unsupervised': 1})\n",
      "\n",
      " Top 10 Words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('learning', 3),\n",
       " ('data', 3),\n",
       " ('machine', 2),\n",
       " ('learn', 2),\n",
       " ('make', 2),\n",
       " ('technique', 1),\n",
       " ('parsing', 1),\n",
       " ('apply', 1),\n",
       " ('learned', 1),\n",
       " ('informed', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stop words\n",
    "words = text_nsl.split()\n",
    "words_no_stop = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        words_no_stop.append(word)\n",
    "wc = cl.Counter(words_no_stop)\n",
    "print(\"Word Count:\\n\", wc)\n",
    "print(\"\\n Top 10 Words:\")\n",
    "wc.most_common(10)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Stemming\n",
    "Now the most common words are all informative but there are three words that are different forms of *learn*. They are *'learn'*, *'learning'* and *'learned'*. It would be better that they are treated as the same word. This is called **stemming**. `PorterStammer` in NLTK can be used to accomplish this task. In the next Code cell, we will demonstrate this process. \n",
    "\n",
    "In the following Code cell, we will first create a `PorterStammer` object, then use the `stem()` function of the object to *stem* the words.\n",
    "\n",
    "The new most common words list shows that the count of the word 'learn' is now 6, both 'learning' and 'learned' are converted to 'learn'. \n",
    "\n",
    "However, it seems PorterStemmer also makes some mistakes. It converts 'machine' to 'machin', 'parse' to 'pars', 'apply' to 'appli' and 'decision' to 'decis'. The reason is that the purpose of stemming is to bring variant forms of a word together, not to map a word onto its _paradigm_ form. For example, all 'apply', 'applied' and 'applying' will be converted to 'appli'. In fact, stemming doesn't always improve text analysis result. Whether to apply stemming or not is a rather advanced topic. An easy way to determine whether to stem or not is to do the analysis with and without stemming and compare the results.\n",
    "\n",
    "Now the top 3 most common words really describe the text well. From just the top 3 most common words, 'learn', 'data' and 'machine', we can already figure out what this piece of text is about with very high confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count:\n",
      " Counter({'learn': 6, 'data': 3, 'machin': 2, 'make': 2, 'techniqu': 1, 'pars': 1, 'appli': 1, 'inform': 1, 'decis': 1, 'focus': 1, 'design': 1, 'algorithm': 1, 'predict': 1, 'supervis': 1, 'unsupervis': 1})\n",
      "\n",
      " Top 10 Words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('learn', 6),\n",
       " ('data', 3),\n",
       " ('machin', 2),\n",
       " ('make', 2),\n",
       " ('techniqu', 1),\n",
       " ('pars', 1),\n",
       " ('appli', 1),\n",
       " ('inform', 1),\n",
       " ('decis', 1),\n",
       " ('focus', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "\n",
    "words_clean = []\n",
    "for word in words_no_stop:\n",
    "    words_clean.append(st.stem(word))\n",
    "    \n",
    "wc = cl.Counter(words_clean)\n",
    "print(\"Word Count:\\n\", wc)\n",
    "print(\"\\n Top 10 Words:\")\n",
    "wc.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Bag of Words\n",
    "\n",
    "A simple question about text data mining that you might have is _How can we classify documents made up of words when machine learning algorithms work on numerical data?_ The answer is simple. We need to build a numerical summary of a text data set that our algorithms can manipulate. A conceptually easy approach to implement this idea is to identify all possible words in the documents of interest and track the number of times each word occurs in specific documents. This produces a (very) sparse matrix for our sample of documents, where the columns are the possible words (or tokens) and the rows are different documents. \n",
    "\n",
    "This concept, where one tokenizes documents to build these sparse matrices, is more formally known as [_bag of words_][bwd], because we effectively create the bag of words out of the documents. In the bag of words model, each document can be mapped into a vector, of which a individual element corresponds to the number of times the word (associated with the particular column) appears in the document.\n",
    "\n",
    "For example, if we build *bag of words* with `text` defined above, we will have a matrix with 15 columns. Because there are 15 different words (tokens) after we clean up `text`, each word is a column in the matrix as shown below:\n",
    "\n",
    "|make|inform|unsupervis|machin|design|techniqu|focus|data|predict|decis|supervis|algorithm|learn|pars|appli|\n",
    "| :-: | :-: | :-: | :-: | :- :| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "\n",
    "\n",
    "With the matrix, we can convert another piece of text to a series numbers as a row in the matrix. \n",
    "\n",
    "For example, adding `'Machine learning has supervised and unsupervised learning'` to the matrix, we will get this:\n",
    "\n",
    "|make|inform|unsupervis|machin|design|techniqu|focus|data|predict|decis|supervis|algorithm|learn|pars|appli|\n",
    "| :-: | :-: | :-: | :-: | :- :| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "|0|0|1|1|0|0|0|0|0|0|1|0|2|0|0|\n",
    "\n",
    "This is a matrix with only one row. The row represent the new text. The number in each column represents the frequency of the word in the new text. Most columns have value 0, meaning those words are not in the new text. This kind of matrix, in which only a small subset of cells have non-zero value, is called a sparse matrix. When we add another piece of text into the matrix, the matrix will have one more row.\n",
    "\n",
    "When we use a large collection of text to build the bag of words, we can get a matrix with thousands of columns. This matrix is also called Document Term Matrix (DTM).\n",
    "\n",
    "### CountVectorizer\n",
    "\n",
    "With scikit-learn, we can use the [`CountVectorizer`][skcv] to break our document into tokens (in this case words), which are used to construct our _bag of words_ for the given set of documents. Given this tokenizer, we first need to construct the list of tokens, which we do with the `fit` method. Second, we need to transform our documents into this sparse matrix(DTM), which we do with the `transform` method. If both steps use the same input data, there is a convenient method to perform both operations at the same time, called `fit_transform`.\n",
    "\n",
    "`CountVectorizer` by default will ignore special characters and convert all letters to lower case. It can even remove stop words with argument `stop_words` set. It can't do stemming, however. So you'll have to do it yourself if you want to apply stemming.\n",
    "\n",
    "In the following code, we demonstrate how to build document term matrix with `CounterVectorizer`. We set argument `stop_words='english'` to apply English stop words filtering. We will use a list of training text to create tokens in the bag of words, then transform both training text set and testing text set. Both the training and testing text sets contain two pieces of texts. In real text analysis, both may contain hundreds or thousands of different texts.\n",
    "\n",
    "For simplicity purpose, we will not do stemming this time.\n",
    "\n",
    "-----\n",
    "[bwd]: https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "[skcv]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens = 38\n",
      "Tokens:\n",
      "['additional', 'algorithms', 'apply', 'appropriate', 'behavioral', 'catering', 'children', 'data', 'decision', 'designed', 'designing', 'difficulties', 'disabilities', 'education', 'educational', 'focuses', 'informed', 'learn', 'learned', 'learning', 'machine', 'make', 'needs', 'parsing', 'physical', 'predictions', 'problems', 'provide', 'resourced', 'school', 'schools', 'special', 'specifically', 'staffed', 'students', 'supervised', 'technique', 'unsupervised']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of Training Samples = 2\n",
      "Number of Testing Samples = 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Testing dtm\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0\n",
      "  0 0]]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#training corpus\n",
    "training_text = [\"\"\"Machine Learning is a technique of parsing data, learn from that data and then \n",
    "        apply what is learned to make an informed decision. Machine learning focuses on \n",
    "        designing algorithms that can learn from and make predictions on the data. \n",
    "        The learning can be supervised or unsupervised.\"\"\",\n",
    "        \"\"\"A special school is a school catering for students who have special educational needs \n",
    "        due to learning difficulties, physical disabilities or behavioral problems. Special \n",
    "        schools may be specifically designed, staffed and resourced to provide appropriate \n",
    "        special education for children with additional needs.\n",
    "        \"\"\"]\n",
    "#testing corpus\n",
    "testing_text = [\"Machine learning has supervised and unsupervised learning.\",\n",
    "               \"Special education is important because children with special needs have equal rights to education.\"]\n",
    "\n",
    "# Define our vectorizer\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Build a vocabulary from our training texts\n",
    "cv.fit(training_text)\n",
    "\n",
    "#transform training texts to a Document Term Matrix (DTM)\n",
    "training_dtm = cv.transform(training_text)\n",
    "\n",
    "#transform testing texts to a DTM\n",
    "testing_dtm = cv.transform(testing_text)\n",
    "\n",
    "#explore the characteristics of the matrix\n",
    "print(f'Number of Tokens = {training_dtm.shape[1]}')\n",
    "print('Tokens:')\n",
    "print(cv.get_feature_names())\n",
    "print(100*'-')\n",
    "print(f'Number of Training Samples = {training_dtm.shape[0]}')\n",
    "print(f'Number of Testing Samples = {training_dtm.shape[0]}')\n",
    "\n",
    "# print out testing DTM\n",
    "print(100*'-')\n",
    "print('Testing dtm')\n",
    "print(testing_dtm.todense())\n",
    "print(100*'-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we fit `CounterVectorizer` with `training_text` and then transform `training_text` to get `training_dtm`. Because we fit and transform on the same text, we can combine the two steps with one function `fit_transform()` as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Build a vocabulary from our training text and transform training text\n",
    "training_dtm = cv.fit_transform(training_text)\n",
    "\n",
    "#transform testing set\n",
    "testing_dtm = cv.transform(testing_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In the next Code cells we create two dataframe to display the training and testing DTM. This is not necessary for text analysis. It's just to show you the structure of the two document term matrices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DTM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>additional</th>\n",
       "      <th>algorithms</th>\n",
       "      <th>apply</th>\n",
       "      <th>appropriate</th>\n",
       "      <th>behavioral</th>\n",
       "      <th>catering</th>\n",
       "      <th>children</th>\n",
       "      <th>data</th>\n",
       "      <th>decision</th>\n",
       "      <th>designed</th>\n",
       "      <th>designing</th>\n",
       "      <th>difficulties</th>\n",
       "      <th>disabilities</th>\n",
       "      <th>education</th>\n",
       "      <th>educational</th>\n",
       "      <th>focuses</th>\n",
       "      <th>informed</th>\n",
       "      <th>learn</th>\n",
       "      <th>learned</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>make</th>\n",
       "      <th>needs</th>\n",
       "      <th>parsing</th>\n",
       "      <th>physical</th>\n",
       "      <th>predictions</th>\n",
       "      <th>problems</th>\n",
       "      <th>provide</th>\n",
       "      <th>resourced</th>\n",
       "      <th>school</th>\n",
       "      <th>schools</th>\n",
       "      <th>special</th>\n",
       "      <th>specifically</th>\n",
       "      <th>staffed</th>\n",
       "      <th>students</th>\n",
       "      <th>supervised</th>\n",
       "      <th>technique</th>\n",
       "      <th>unsupervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   additional  algorithms  apply  appropriate  behavioral  catering  children  \\\n",
       "0           0           1      1            0           0         0         0   \n",
       "1           1           0      0            1           1         1         1   \n",
       "\n",
       "   data  decision  designed  designing  difficulties  disabilities  education  \\\n",
       "0     3         1         0          1             0             0          0   \n",
       "1     0         0         1          0             1             1          1   \n",
       "\n",
       "   educational  focuses  informed  learn  learned  learning  machine  make  \\\n",
       "0            0        1         1      2        1         3        2     2   \n",
       "1            1        0         0      0        0         1        0     0   \n",
       "\n",
       "   needs  parsing  physical  predictions  problems  provide  resourced  \\\n",
       "0      0        1         0            1         0        0          0   \n",
       "1      2        0         1            0         1        1          1   \n",
       "\n",
       "   school  schools  special  specifically  staffed  students  supervised  \\\n",
       "0       0        0        0             0        0         0           1   \n",
       "1       2        1        4             1        1         1           0   \n",
       "\n",
       "   technique  unsupervised  \n",
       "0          1             1  \n",
       "1          0             0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set option to display all columns of a dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#create dataframe for training DTM\n",
    "training_data = training_dtm.todense().tolist()\n",
    "training_df = pd.DataFrame(training_data, columns=cv.get_feature_names())\n",
    "print('Training DTM:')\n",
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DTM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>additional</th>\n",
       "      <th>algorithms</th>\n",
       "      <th>apply</th>\n",
       "      <th>appropriate</th>\n",
       "      <th>behavioral</th>\n",
       "      <th>catering</th>\n",
       "      <th>children</th>\n",
       "      <th>data</th>\n",
       "      <th>decision</th>\n",
       "      <th>designed</th>\n",
       "      <th>designing</th>\n",
       "      <th>difficulties</th>\n",
       "      <th>disabilities</th>\n",
       "      <th>education</th>\n",
       "      <th>educational</th>\n",
       "      <th>focuses</th>\n",
       "      <th>informed</th>\n",
       "      <th>learn</th>\n",
       "      <th>learned</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>make</th>\n",
       "      <th>needs</th>\n",
       "      <th>parsing</th>\n",
       "      <th>physical</th>\n",
       "      <th>predictions</th>\n",
       "      <th>problems</th>\n",
       "      <th>provide</th>\n",
       "      <th>resourced</th>\n",
       "      <th>school</th>\n",
       "      <th>schools</th>\n",
       "      <th>special</th>\n",
       "      <th>specifically</th>\n",
       "      <th>staffed</th>\n",
       "      <th>students</th>\n",
       "      <th>supervised</th>\n",
       "      <th>technique</th>\n",
       "      <th>unsupervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   additional  algorithms  apply  appropriate  behavioral  catering  children  \\\n",
       "0           0           0      0            0           0         0         0   \n",
       "1           0           0      0            0           0         0         1   \n",
       "\n",
       "   data  decision  designed  designing  difficulties  disabilities  education  \\\n",
       "0     0         0         0          0             0             0          0   \n",
       "1     0         0         0          0             0             0          2   \n",
       "\n",
       "   educational  focuses  informed  learn  learned  learning  machine  make  \\\n",
       "0            0        0         0      0        0         2        1     0   \n",
       "1            0        0         0      0        0         0        0     0   \n",
       "\n",
       "   needs  parsing  physical  predictions  problems  provide  resourced  \\\n",
       "0      0        0         0            0         0        0          0   \n",
       "1      1        0         0            0         0        0          0   \n",
       "\n",
       "   school  schools  special  specifically  staffed  students  supervised  \\\n",
       "0       0        0        0             0        0         0           1   \n",
       "1       0        0        2             0        0         0           0   \n",
       "\n",
       "   technique  unsupervised  \n",
       "0          0             1  \n",
       "1          0             0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataframe for testing DTM\n",
    "testing_data = testing_dtm.todense().tolist()\n",
    "testing_df = pd.DataFrame(testing_data, columns=cv.get_feature_names())\n",
    "print('Testing DTM')\n",
    "testing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now we've converted both training and testing text sets to bag of words representation(or document term matrix). Each row in `training_dtm` represents a piece of training text and each row in `testing_dtm` represents a piece of testing text. To understand the DTM, let's look at the second row in the testing DTM which represent the text `'Special education is important because children with special needs have equal rights to education.'`. The second row shows the word count of this text. There are one `'children'`, two `'education'`, one `'needs'` and two `'special'`. There are some words in the text, like `'equal'` or `'rights'` that are not captured by the matrix. The reason is that these two words are not in the training text with which the bag of words are built. But this problem will be gone when we have a large collection of training texts which produce a matrix with much more comprehensive tokens.\n",
    "\n",
    "With the numeric representation of texts, we can then apply machine learning algorithms on the dataset for classification tasks. \n",
    "\n",
    "Let's say our goal is to find out what topic a new piece of text is about. Assume there are only two topics, *Machine Learning* and *Special Education*. The first text in the training DTM is labeled as Machine Learning and the second is labeled as Special Education. We can train our model with the training dtm and label, then evaluate the model with testing DTM and label. With the trained model, we can predict the topic of an unseen text into one of the two topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding Code cells, we used scikit-learn to parse our sample text message by using `CountVectorizer`. \n",
    "\n",
    "Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Try vectorizing a different message (i.e., transform a different message, or messages) with already fit `cv`. How do the results change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "Previously, we have simply used counts of tokens. Even with the removal of stop words, however, this can still overemphasize tokens that might generally occur across many documents (e.g., names or general concepts). An alternative technique that often provides robust improvements in text analysis accuracy is to employ the frequency of token occurrence, normalized over the frequency with which the token occurs in all documents. In this manner, we give higher weight in the text analysis process to tokens that are more strongly tied to a particular label(ie. topic). \n",
    "\n",
    "Formally this concept is known as [term frequency–inverse document frequency][tfd] (or tf-idf), and scikit-learn provides this functionality via the [`TfidfTransformer`][tftd] that can either follow a tokenizer, such as `CountVectorizer` or can be combined together into a single transformer via the [`TfidfVectorizer`][tfvd].\n",
    "\n",
    "In the following code, we will build the DTM with `TfidfVectorizer`. Then we will print out the training DTM. \n",
    "\n",
    "In the first row of the training DTM created by `CounterVectorizer`, both `'data'` and `'learning'` has count 3, which means both words carry the same weight in classification.\n",
    "\n",
    "In the first row of the training DTM created by `TfidfVectorizer`, `'data'` now has value 0.489531 and `'learning'` has value 0.348306. So in this new matrix, `'data'` has more classification power than `'learning'`. The reason is that `'learning'` appears in both training texts and the total frequency of `'learning'` in the whole training text set(which is 4) is more than that of `'data'`(which is 3). So even though both words appear 3 times in the first training text, due to \"inverse document frequency\", the TF-IDF adjusted values are different; the value of `'data'` is greater than the value of `'learning'` due to the fact that `'data'` has less frequency in the whole text set. Using a more intuitive way to explain this: when a text has the word `'learning'` in it, it's hard to guess whether the text is about machine learning or special education because `'learning'` is a common word in both topics. On the other hand, if a text has the word `'data'`, it's more likely to be about machine learning than special education.\n",
    "\n",
    "The algorithm of how TF-IDF value is calculated is beyond the scope of this course.\n",
    "\n",
    "-----\n",
    "[tfd]: https://en.wikipedia.org/wiki/Tf–idf\n",
    "\n",
    "[tftd]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "\n",
    "[tfvd]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TFIDF DTM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>additional</th>\n",
       "      <th>algorithms</th>\n",
       "      <th>apply</th>\n",
       "      <th>appropriate</th>\n",
       "      <th>behavioral</th>\n",
       "      <th>catering</th>\n",
       "      <th>children</th>\n",
       "      <th>data</th>\n",
       "      <th>decision</th>\n",
       "      <th>designed</th>\n",
       "      <th>designing</th>\n",
       "      <th>difficulties</th>\n",
       "      <th>disabilities</th>\n",
       "      <th>education</th>\n",
       "      <th>educational</th>\n",
       "      <th>focuses</th>\n",
       "      <th>informed</th>\n",
       "      <th>learn</th>\n",
       "      <th>learned</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>make</th>\n",
       "      <th>needs</th>\n",
       "      <th>parsing</th>\n",
       "      <th>physical</th>\n",
       "      <th>predictions</th>\n",
       "      <th>problems</th>\n",
       "      <th>provide</th>\n",
       "      <th>resourced</th>\n",
       "      <th>school</th>\n",
       "      <th>schools</th>\n",
       "      <th>special</th>\n",
       "      <th>specifically</th>\n",
       "      <th>staffed</th>\n",
       "      <th>students</th>\n",
       "      <th>supervised</th>\n",
       "      <th>technique</th>\n",
       "      <th>unsupervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163177</td>\n",
       "      <td>0.163177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489531</td>\n",
       "      <td>0.163177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163177</td>\n",
       "      <td>0.163177</td>\n",
       "      <td>0.326354</td>\n",
       "      <td>0.163177</td>\n",
       "      <td>0.348306</td>\n",
       "      <td>0.326354</td>\n",
       "      <td>0.326354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163177</td>\n",
       "      <td>0.163177</td>\n",
       "      <td>0.163177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.306763</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.613527</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   additional  algorithms     apply  appropriate  behavioral  catering  \\\n",
       "0    0.000000    0.163177  0.163177     0.000000    0.000000  0.000000   \n",
       "1    0.153382    0.000000  0.000000     0.153382    0.153382  0.153382   \n",
       "\n",
       "   children      data  decision  designed  designing  difficulties  \\\n",
       "0  0.000000  0.489531  0.163177  0.000000   0.163177      0.000000   \n",
       "1  0.153382  0.000000  0.000000  0.153382   0.000000      0.153382   \n",
       "\n",
       "   disabilities  education  educational   focuses  informed     learn  \\\n",
       "0      0.000000   0.000000     0.000000  0.163177  0.163177  0.326354   \n",
       "1      0.153382   0.153382     0.153382  0.000000  0.000000  0.000000   \n",
       "\n",
       "    learned  learning   machine      make     needs   parsing  physical  \\\n",
       "0  0.163177  0.348306  0.326354  0.326354  0.000000  0.163177  0.000000   \n",
       "1  0.000000  0.109132  0.000000  0.000000  0.306763  0.000000  0.153382   \n",
       "\n",
       "   predictions  problems   provide  resourced    school   schools   special  \\\n",
       "0     0.163177  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "1     0.000000  0.153382  0.153382   0.153382  0.306763  0.153382  0.613527   \n",
       "\n",
       "   specifically   staffed  students  supervised  technique  unsupervised  \n",
       "0      0.000000  0.000000  0.000000    0.163177   0.163177      0.163177  \n",
       "1      0.153382  0.153382  0.153382    0.000000   0.000000      0.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define our TFIDF vectorizer\n",
    "tf_cv = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Build a vocabulary from our training text and transform training text\n",
    "training_dtm_tf = tf_cv.fit_transform(training_text)\n",
    "\n",
    "#transform testing set\n",
    "testing_dtm_tf = tf_cv.transform(testing_text)\n",
    "\n",
    "#create dataframe for training DTM\n",
    "training_data_tf = training_dtm_tf.todense().tolist()\n",
    "training_tf_df = pd.DataFrame(training_data_tf, columns=tf_cv.get_feature_names())\n",
    "print('Training TFIDF DTM')\n",
    "training_tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. Wikipedia article on [Bag of Words][wbow] model\n",
    "1. Wikipedia article on [Document Term Matrix][wdtm]\n",
    "1. Gentle Introduction (in Python 2) to text analysis with Python, [part 1][nctap1] and [part 2][nctap2]\n",
    "1. Kaggle tutorial on [Bag of Words][kbow]\n",
    "\n",
    "-----\n",
    "\n",
    "[inlp]: https://blog.monkeylearn.com/the-definitive-guide-to-natural-language-processing/\n",
    "\n",
    "[wnlp]: https://en.wikipedia.org/wiki/Natural_language_processing\n",
    "[wbow]: https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "[wdtm]: https://en.wikipedia.org/wiki/Document-term_matrix\n",
    "\n",
    "\n",
    "[nytnlp]: http://www.nytimes.com/2003/10/16/technology/circuits/16mine.html?pagewanted=print\n",
    "[nltk3]: http://www.nltk.org/book/ch01.html\n",
    "\n",
    "[nctap1]: http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-1/\n",
    "[nctap2]: http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-2/\n",
    "\n",
    "[kbow]: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2019: Gies College of Business at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
