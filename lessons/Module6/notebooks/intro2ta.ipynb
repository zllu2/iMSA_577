{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduce text analytics, which is one of the most exciting application areas for machine learning. Text analytics forms the basis for [natural language processing][nlp], and is explicitly used for [sentiment analysis][sa], for [language identification][lc], for [spelling and grammar correction][sc], as well as for mining text data contained in forms, such as [medical informatics][mi]. Before moving into text classification and text mining, in this notebook we focus on basic text analytics such as accessing data, tokenizing a corpus, and computing token frequencies. We demonstrate these tasks by using basic Python concepts and by using functionality from within the scikit learn library. Finally, we introduce the NLTK library, and use methods within this library to perform these tasks.\n",
    "\n",
    "---\n",
    "\n",
    "[nlp]: https://en.wikipedia.org/wiki/Natural_language_processing\n",
    "[sa]: https://en.wikipedia.org/wiki/Sentiment_analysis\n",
    "[lc]: https://en.wikipedia.org/wiki/Language_identification\n",
    "[sc]: http://norvig.com/spell-correct.html\n",
    "[mi]: https://en.wikipedia.org/wiki/Health_informatics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Data](#Data)\n",
    "\n",
    "[Bag of Words](#Bag-of-Words)\n",
    "\n",
    "[NLTK](#NLTK)\n",
    "\n",
    "[NLTK Corpus](#NLTK-Corpus)\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the rest of this notebook, we first include our standard notebook setup code and we define our _data_ directory.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display all plots inline\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We do this to ignore several specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set global fiugure properties\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({'axes.titlesize' : 20,\n",
    "                     'axes.labelsize' : 18,\n",
    "                     'legend.fontsize': 16})\n",
    "\n",
    "# Set default seaborn plotting style\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we find our HOME directory\n",
    "home_dir = !echo $HOME\n",
    "\n",
    "# Define data directory\n",
    "home = home_dir[0] +'/accy571/readonly/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Data\n",
    "\n",
    "To get started with text analysis, we need texts to analyze. To get started, we will analyze the [twenty newsgroup][tng] data set. We first download this data (scikit learn has built in methods for doing this, however, we have cached a copy locally on our server). The data are made available via a custom object, but we can access the data of interest by using dictionary keys. Before delving into text analysis, we first explore this data over several Code cells to understand more about the task at hand.\n",
    "\n",
    "In case you are unaware, one primary use of the early Internet (i.e., pre-Web) was to share information among interested groups via newsgroups, or bulletin boards. Users could subscribe to these groups to send and receive postings of interest. This text classification problem uses postings to twenty newsgroups, thus the individual newsgroup is the classification target and the text in the postings are used to create the features. These postings were similar to emails, thus each posting will have a header, the article body, which might quote all or part of a previous message, and  possibly a footer (like an email signature). While we use the entire posting in these notebooks, you can have the header, quoted text, and the footer removed by scikit learn by including the remove attribute, and indicating whether these sections should be removed. This attribute can take one or all of the values: header, footer, and quotes. For example, the following attribute would be used to remove both headers and footers.\n",
    "\n",
    "```remove =('headers', 'footers')```\n",
    "\n",
    "---\n",
    "\n",
    "[tng]: http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "text = fetch_20newsgroups(data_home=home + 'textdm')\n",
    "\n",
    "# To learn more about these data, either browse the relevant \n",
    "# scikit learn documentation, or enter help(text) in a Code cell\n",
    "\n",
    "# The data can be accessed via dictionary keys\n",
    "print(text.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  0 = alt.atheism\n",
      "Class  1 = comp.graphics\n",
      "Class  2 = comp.os.ms-windows.misc\n",
      "Class  3 = comp.sys.ibm.pc.hardware\n",
      "Class  4 = comp.sys.mac.hardware\n",
      "Class  5 = comp.windows.x\n",
      "Class  6 = misc.forsale\n",
      "Class  7 = rec.autos\n",
      "Class  8 = rec.motorcycles\n",
      "Class  9 = rec.sport.baseball\n",
      "Class 10 = rec.sport.hockey\n",
      "Class 11 = sci.crypt\n",
      "Class 12 = sci.electronics\n",
      "Class 13 = sci.med\n",
      "Class 14 = sci.space\n",
      "Class 15 = soc.religion.christian\n",
      "Class 16 = talk.politics.guns\n",
      "Class 17 = talk.politics.mideast\n",
      "Class 18 = talk.politics.misc\n",
      "Class 19 = talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "# Display target names, i.e., the names of the \n",
    "# twenty news groups\n",
    "\n",
    "for idx, label in enumerate(text['target_names']):\n",
    "    print(f'Class {idx:2d} = {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Newsgroup: sci.med\n",
      "--------------------------------------------------------------------------------\n",
      "From: geb@cs.pitt.edu (Gordon Banks)\n",
      "Subject: Re: sudden numbness in arm\n",
      "Reply-To: geb@cs.pitt.edu (Gordon Banks)\n",
      "Organization: Univ. of Pittsburgh Computer Science\n",
      "Lines: 48\n",
      "\n",
      "In article <C5u5LG.C3G@gpu.utcc.utoronto.ca> molnar@Bisco.CAnet.CA (Tom Molnar) writes:\n",
      ">I experienced a sudden numbness in my left arm this morning.  Just after\n",
      ">I completed my 4th set of deep squats.  Today was my weight training\n",
      ">day and I was just beginning my routine.  All of a sudden at the end of\n",
      ">the 4th set my arm felt like it had gone to sleep.  It was cold, turned pale,\n",
      ">and lost 60% of its strength.  The weight I used for squats wasn't that\n",
      ">heavy, I was working hard but not at 100% effort.  I waited for a few \n",
      ">minutes, trying to shake the arm back to life and then continued with\n",
      ">chest exercises (flyes) with lighter dumbells than I normally use.  But\n",
      ">I dropped the left dumbell during the first set, and experienced continued\n",
      ">arm weakness into the second.  So I quit training and decided not to do my\n",
      ">usual hour on the ski machine either.  I'll take it easy for the rest of\n",
      ">the day.\n",
      ">\n",
      ">My arm is *still* somewhat numb and significantly weaker than normal --\n",
      ">my hand still tingles a bit down to the thumb. Color has returned to normal\n",
      ">and it is no longer cold. \n",
      ">\n",
      ">Horrid thoughts of chunks of plaque blocking a major artery course through\n",
      ">my brain.  I'm 34, vegetarian, and pretty fit from my daily exercise\n",
      ">regimen.  So that can't be it.  Could a pinched nerve from the bar\n",
      ">cause these symptoms (I hope)?\n",
      "\n",
      "It likely has nothing to do with \"chunks of plaque\" but it sounds like\n",
      "you may have a neurovascular compromise to your arm and you need medical\n",
      "attention *before* doing any more weight lifting.  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- \n",
      "----------------------------------------------------------------------------\n",
      "Gordon Banks  N3JXP      | \"Skepticism is the chastity of the intellect, and\n",
      "geb@cadre.dsl.pitt.edu   |  it is shameful to surrender it too soon.\" \n",
      "----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display single message\n",
    "messageID = 251\n",
    "\n",
    "message = text['data'][messageID]\n",
    "target = text['target'][messageID]\n",
    "\n",
    "print(f'Target Newsgroup: {text[\"target_names\"][target]}')\n",
    "print(80*'-')\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can analyze text messages by using basic Python commands. For example, we can find how many times a word appears in a text by using Python `string` functions. One important item to consider, however, is that, by default, Python will search for sequences of characters in a text message. Thus, if a word is also part of larger words, we will over-count the occurrences, as demonstrated in the following Code cell.\n",
    "\n",
    "We overcome this limitation by explicitly splitting a text into tokens. By default, in Python this is done at whitespace, but this can be changed. The second Code cell demonstrates tokenizing a text string, and using a [`Counter`][pc] to accumulate the number of unique occurrences of each token. Finally, we employ regular expressions to split a text string, and use the resulting tokens to create another `Counter` that can be used to compare the results of regular expression parsing with the default string tokenization. One benefit of using regular expressions is that we can specifically indicate of what a token should be composed, in this case, we state a token is a sequence of one or more alphanumeric characters surrounded by white space. Thus the `>` token is removed.\n",
    "\n",
    "-----\n",
    "[pc]: https://docs.python.org/library/collections.html#collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expression Count: 14\n",
      "Isolated Token Count: 9\n"
     ]
    }
   ],
   "source": [
    "token = 'to'\n",
    "i_token = f' {token} '\n",
    "\n",
    "print(f'Expression Count: {message.count(token)}')\n",
    "print(f'Isolated Token Count: {message.count(i_token)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens = 213\n",
      "---------------------------------------------------------------------------\n",
      "Top 40 tokens:\n",
      "---------------------------------------------------------------------------\n",
      "[ ('the', 11), ('of', 10), ('to', 9), ('and', 8), ('a', 7), ('my', 7),\n",
      "  ('arm', 6), ('I', 6), ('it', 6), ('was', 4), ('is', 4), ('sudden', 3),\n",
      "  ('>I', 3), ('weight', 3), ('for', 3), ('with', 3), ('geb@cs.pitt.edu', 2),\n",
      "  ('(Gordon', 2), ('Banks)', 2), ('numbness', 2), ('in', 2), ('experienced', 2),\n",
      "  ('left', 2), ('4th', 2), ('set', 2), ('training', 2), ('at', 2), ('>the', 2),\n",
      "  ('like', 2), ('It', 2), ('>and', 2), ('that', 2), ('but', 2), ('not', 2),\n",
      "  ('continued', 2), ('than', 2), ('So', 2), ('do', 2), ('>', 2), ('normal', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Accumulate counts of tokens, using string functionality\n",
    "import collections as cl\n",
    "\n",
    "# Used to print out sequences\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "\n",
    "# Tokenize and create counter\n",
    "words = message.split()\n",
    "wc = cl.Counter(words)\n",
    "\n",
    "# Number of tokens to display from message\n",
    "wc_display = 40\n",
    "\n",
    "# Display results\n",
    "print(f'Total number of tokens = {len(wc):d}')\n",
    "print(75*'-')\n",
    "print(f'Top {wc_display} tokens:')\n",
    "print(75*'-')\n",
    "pp.pprint(wc.most_common(wc_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens = 207\n",
      "---------------------------------------------------------------------------\n",
      "Top 40 tokens:\n",
      "---------------------------------------------------------------------------\n",
      "[ ('the', 13), ('I', 12), ('of', 10), ('and', 10), ('my', 9), ('to', 9),\n",
      "  ('arm', 7), ('a', 7), ('it', 7), ('was', 4), ('is', 4), ('geb', 3),\n",
      "  ('pitt', 3), ('edu', 3), ('Gordon', 3), ('Banks', 3), ('sudden', 3),\n",
      "  ('set', 3), ('weight', 3), ('for', 3), ('with', 3), ('cs', 2),\n",
      "  ('numbness', 2), ('in', 2), ('experienced', 2), ('left', 2), ('4th', 2),\n",
      "  ('squats', 2), ('training', 2), ('day', 2), ('at', 2), ('like', 2), ('It', 2),\n",
      "  ('cold', 2), ('t', 2), ('that', 2), ('but', 2), ('not', 2), ('continued', 2),\n",
      "  ('than', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Use regular expressions to tokenize\n",
    "import re\n",
    "pattern = re.compile(r'[^\\w\\s]')\n",
    "\n",
    "# Tokenize and accumulate token counts\n",
    "words = re.sub(pattern, ' ', message).split()\n",
    "wc = cl.Counter(words)\n",
    "\n",
    "# Display results\n",
    "print(f'Total number of tokens = {len(wc):d}')\n",
    "print(75*'-')\n",
    "print(f'Top {wc_display} tokens:')\n",
    "print(75*'-')\n",
    "\n",
    "pp.pprint(wc.most_common(wc_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The previous cells tokenized a text document, but identical tokens with different case will be treated as distinct. In general, this is not a desirable result since it could undercount the occurrences of an otherwise important token. We can easily convert a text to all lowercase to prevent this, by using the string `lower` method. This is demonstrated in the following Code cell, where the total number of tokens has changed, as well as the counts of specific tokens (such as `in`).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens = 197\n",
      "---------------------------------------------------------------------------\n",
      "Top 40 tokens:\n",
      "---------------------------------------------------------------------------\n",
      "[ ('the', 14), ('i', 12), ('to', 10), ('of', 10), ('my', 10), ('and', 10),\n",
      "  ('it', 9), ('arm', 7), ('a', 7), ('was', 4), ('is', 4), ('from', 3),\n",
      "  ('geb', 3), ('pitt', 3), ('edu', 3), ('gordon', 3), ('banks', 3),\n",
      "  ('sudden', 3), ('in', 3), ('set', 3), ('weight', 3), ('for', 3), ('but', 3),\n",
      "  ('with', 3), ('cs', 2), ('numbness', 2), ('ca', 2), ('molnar', 2),\n",
      "  ('experienced', 2), ('left', 2), ('just', 2), ('4th', 2), ('squats', 2),\n",
      "  ('training', 2), ('day', 2), ('at', 2), ('like', 2), ('cold', 2), ('t', 2),\n",
      "  ('that', 2)]\n"
     ]
    }
   ],
   "source": [
    "# We can convert message to lower-case\n",
    "words = re.sub(pattern, ' ', message.lower()).split()\n",
    "wc = cl.Counter(words)\n",
    "\n",
    "# Display results\n",
    "print(f'Total number of tokens = {len(wc):d}')\n",
    "print(75*'-')\n",
    "print(f'Top {wc_display} tokens:')\n",
    "print(75*'-')\n",
    "pp.pprint(wc.most_common(wc_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Tokenizing a text document is interesting, but traditional machine learning algorithms operate directly on numerical data. One approach to analyze a text document is to generate a numerical representation of a text document by counting the number of times a word occurs (as we did with the `Counter` collection previously. Another approach is to normalize the token counts by the total number of tokens, which creates a term (or token) frequency. We demonstrate this in the following Code cell, where we display the _top_ terms and their frequency in the message.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term        : Frequency\n",
      "-------------------------\n",
      "the         : 0.042\n",
      "i           : 0.036\n",
      "to          : 0.030\n",
      "of          : 0.030\n",
      "my          : 0.030\n",
      "and         : 0.030\n",
      "it          : 0.027\n",
      "arm         : 0.021\n",
      "a           : 0.021\n",
      "was         : 0.012\n",
      "is          : 0.012\n",
      "from        : 0.009\n",
      "geb         : 0.009\n",
      "pitt        : 0.009\n",
      "edu         : 0.009\n",
      "gordon      : 0.009\n",
      "banks       : 0.009\n",
      "sudden      : 0.009\n",
      "in          : 0.009\n",
      "set         : 0.009\n",
      "weight      : 0.009\n",
      "for         : 0.009\n",
      "but         : 0.009\n",
      "with        : 0.009\n",
      "cs          : 0.006\n",
      "numbness    : 0.006\n",
      "ca          : 0.006\n",
      "molnar      : 0.006\n",
      "experienced : 0.006\n",
      "left        : 0.006\n",
      "just        : 0.006\n",
      "4th         : 0.006\n",
      "squats      : 0.006\n",
      "training    : 0.006\n",
      "day         : 0.006\n",
      "at          : 0.006\n",
      "like        : 0.006\n",
      "cold        : 0.006\n",
      "t           : 0.006\n",
      "that        : 0.006\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Term':12s}: {'Frequency'}\")\n",
    "print(25*'-')\n",
    "\n",
    "# Compute term counts\n",
    "t_wc = sum(wc.values())\n",
    "\n",
    "# Display counts and frequencies\n",
    "for wt in wc.most_common(wc_display):\n",
    "    print(f'{wt[0]:12s}: {wt[1]/t_wc:4.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we used the twenty newsgroup data to perform some basic text analysis. Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Remove the header, footer, and quoted material. How does this change the results? \n",
    "2. Change your regular expression so that tokens are sequences of two or more characters (i.e., no numbers). How does this change the results?\n",
    "3. Try timing the three technique (you might want a longer text document). Which is the fastest technique? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Bag of Words\n",
    "\n",
    "A simple question about text data mining that you might have is _How does one classify documents made up of words when machine learning algorithms work on numerical data?_ The simple answer is we need to build a numerical summary of a data set that our algorithms can manipulate. A conceptually easy approach  to implement this idea is to identify all possible words in the documents of interest and to track the number of times each words occurs in specific documents. This produces a (very) sparse matrix for our sample of documents, where the columns are the possible words (or tokens) and the rows are different documents. \n",
    "\n",
    "This concept, where one tokenizes documents to build these sparse matrices is more formally known as _bag of words_, because we effectively create the [bag of words][bwd] out of which are documents are constructed. In the bag of words model, each document can be mapped into a vector, where the individual elements correspond to the number of times the words (associated with the particular column) appears in the document.\n",
    "\n",
    "With scikit learn, we can use the [`CountVectorizer`][skcv] to break our document into tokens (in this case words), which are used to construct our _bag of words_ for the given set of documents. Given this tokenizer, we first need to construct the list of tokens, which we do with the `fit` method. Second, we need to transform our documents into this sparse matrix, which we do with the `transform` method. Since both steps use the same input data, there is a convenience method to perform both operations at the same time, called `fit_transform`.\n",
    "\n",
    "-----\n",
    "[bwd]: https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "[skcv]: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='word', lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Given the `CountVectorizer` we can see the number of words in our _bag_ as well as the number of documents on which we train, which in this case agrees with the values we obtained when we read in the data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a vocabulary from our data\n",
    "cv.fit(text['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples = 1\n",
      "Number of Tokens = 130107\n",
      "---------------------------------------------------------------------------\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "---------------------------------------------------------------------------\n",
      "Tuples from Document-Term Matrix[i, j] and c (Count)\n",
      "---------------------------------------------------------------------------\n",
      "[ (0, 2337, 1), (0, 11891, 1), (0, 14011, 1), (0, 14730, 2), (0, 16455, 1),\n",
      "  (0, 26605, 1), (0, 27436, 1), (0, 28146, 10), (0, 28601, 1), (0, 29376, 7),\n",
      "  (0, 29556, 1), (0, 29573, 1), (0, 30044, 2), (0, 30243, 1), (0, 31414, 1),\n",
      "  (0, 31767, 3), (0, 31793, 1), (0, 32311, 1), (0, 32517, 1), (0, 32542, 1),\n",
      "  (0, 33508, 1), (0, 33527, 1), (0, 33879, 1), (0, 34772, 1), (0, 35805, 3),\n",
      "  (0, 36206, 1), (0, 36955, 1), (0, 37219, 2), (0, 37295, 1), (0, 37565, 1),\n",
      "  (0, 37627, 1), (0, 38194, 1), (0, 39122, 1), (0, 39294, 1), (0, 39701, 2),\n",
      "  (0, 40938, 2), (0, 41049, 1), (0, 41485, 1), (0, 41571, 1), (0, 41614, 1),\n",
      "  (0, 42304, 2), (0, 42876, 1), (0, 42969, 1), (0, 43740, 2), (0, 44889, 1),\n",
      "  (0, 45295, 2), (0, 45645, 1), (0, 45824, 1), (0, 48351, 2), (0, 48479, 1),\n",
      "  (0, 48753, 1), (0, 49090, 1), (0, 49213, 1), (0, 49416, 1), (0, 49417, 1),\n",
      "  (0, 49506, 1), (0, 50174, 1), (0, 50527, 3), (0, 50654, 1), (0, 50881, 1),\n",
      "  (0, 51609, 1), (0, 53276, 1), (0, 53278, 1), (0, 53418, 2), (0, 54870, 1),\n",
      "  (0, 55011, 1), (0, 55525, 1), (0, 55569, 1), (0, 56011, 1), (0, 56283, 3),\n",
      "  (0, 56979, 3), (0, 58487, 3), (0, 59761, 1), (0, 59833, 3), (0, 59961, 1),\n",
      "  (0, 61546, 1), (0, 61803, 1), (0, 61959, 1), (0, 62123, 2), (0, 62221, 1),\n",
      "  (0, 62534, 1), (0, 63962, 1), (0, 64038, 1), (0, 64153, 1), (0, 66608, 3),\n",
      "  (0, 67700, 1), (0, 68003, 1), (0, 68532, 4), (0, 68766, 9), (0, 68857, 1),\n",
      "  (0, 71079, 2), (0, 75151, 2), (0, 75828, 1), (0, 75854, 1), (0, 75870, 1),\n",
      "  (0, 75901, 2), (0, 75906, 1), (0, 76032, 1), (0, 76377, 1), (0, 76685, 1),\n",
      "  (0, 76825, 1), (0, 78644, 1), (0, 79043, 1), (0, 80005, 1), (0, 80809, 1),\n",
      "  (0, 82308, 1), (0, 83441, 2), (0, 83706, 1), (0, 83743, 1), (0, 85354, 10),\n",
      "  (0, 85684, 1), (0, 86493, 1), (0, 86663, 1), (0, 86816, 1), (0, 87626, 1),\n",
      "  (0, 87873, 2), (0, 87879, 1), (0, 87949, 2), (0, 87974, 1), (0, 88361, 1),\n",
      "  (0, 88372, 2), (0, 89362, 10), (0, 89860, 1), (0, 90379, 1), (0, 91771, 1),\n",
      "  (0, 93915, 1), (0, 94037, 3), (0, 94048, 1), (0, 94275, 2), (0, 95834, 1),\n",
      "  (0, 98440, 1), (0, 99721, 1), (0, 100444, 1), (0, 101034, 1), (0, 101378, 1),\n",
      "  (0, 101549, 1), (0, 103084, 1), (0, 105252, 1), (0, 105742, 1),\n",
      "  (0, 106275, 3), (0, 106532, 1), (0, 106561, 1), (0, 107354, 1),\n",
      "  (0, 107811, 1), (0, 107828, 1), (0, 108047, 1), (0, 108558, 2),\n",
      "  (0, 108828, 1), (0, 108871, 1), (0, 108972, 1), (0, 109788, 2),\n",
      "  (0, 110697, 2), (0, 111010, 1), (0, 111322, 1), (0, 111553, 3),\n",
      "  (0, 112085, 1), (0, 112564, 1), (0, 113279, 1), (0, 114418, 2),\n",
      "  (0, 114440, 2), (0, 114455, 14), (0, 114520, 1), (0, 114625, 1),\n",
      "  (0, 114731, 1), (0, 114813, 1), (0, 114882, 1), (0, 114934, 1),\n",
      "  (0, 115200, 1), (0, 115475, 10), (0, 115508, 1), (0, 115579, 1),\n",
      "  (0, 115663, 1), (0, 116099, 2), (0, 116727, 1), (0, 117038, 1),\n",
      "  (0, 118952, 1), (0, 119737, 1), (0, 119740, 1), (0, 119832, 1),\n",
      "  (0, 119872, 1), (0, 119930, 1), (0, 121005, 1), (0, 123074, 1),\n",
      "  (0, 123292, 4), (0, 123310, 1), (0, 123586, 1), (0, 123592, 1),\n",
      "  (0, 123699, 3), (0, 124616, 3), (0, 125035, 1), (0, 125271, 1),\n",
      "  (0, 128402, 2), (0, 128420, 1)]\n"
     ]
    }
   ],
   "source": [
    "# We can now process documents.\n",
    "\n",
    "# We needd an iteratable to appply cv.transform()\n",
    "msg = []\n",
    "msg.append(message)\n",
    "\n",
    "# Transform one message, which is easier to comprehend.\n",
    "# By default, scikit learn uses sparse matrices for \n",
    "# text processing\n",
    "#\n",
    "# What is returned is a Document Term Matrix (dtm)\n",
    "dtm = cv.transform(msg)\n",
    "\n",
    "print(f'Number of Samples = {dtm.shape[0]}')\n",
    "print(f'Number of Tokens = {dtm.shape[1]}')\n",
    "print(75*'-')\n",
    "\n",
    "# We can convert from sparse to dense to explore the \n",
    "# document-term matrix. The range was manually chosen \n",
    "# to have several non-zero elements\n",
    "print(dtm.todense()[:,45141:45240])\n",
    "print(75*'-')\n",
    "\n",
    "# We can also print only nonzero DTM matrix elements\n",
    "print('Tuples from Document-Term Matrix[i, j] and c (Count)')\n",
    "print(75*'-')\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Find non-zero elements\n",
    "i, j, c = sp.find(dtm)\n",
    "dtm_list = list(zip(i, j, c))\n",
    "\n",
    "# Display list of non-zero elements\n",
    "pp.pprint(dtm_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can grab the words in our _bag of words_ by extracting the _vocabulary_. This allows us to see if words are present in the documents. We can also find which term occurs most frequently, least frequently, as well as the overall top terms.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arbitrary Word (the): Column = 114455\n",
      "Max Word (the): Column = 114455\n",
      "Min Word (100): Column = 2337\n"
     ]
    }
   ],
   "source": [
    "# We can explore the terms in our vocabulary\n",
    "terms = cv.vocabulary_\n",
    "\n",
    "# Look at single term\n",
    "my_word = 'the'\n",
    "print(f'Arbitrary Word ({my_word}): Column = {terms[my_word]}')\n",
    "\n",
    "from operator import itemgetter\n",
    "max_key = max(dtm_list, key=itemgetter(2))[1]\n",
    "min_key = min(dtm_list, key=itemgetter(2))[1]\n",
    "\n",
    "x_max = [key for key in terms.keys() if terms[key] == max_key]\n",
    "x_min = [key for key in terms.keys() if terms[key] == min_key]\n",
    "\n",
    "print(f'Max Word ({x_max[0]}): Column = {max_key}')\n",
    "print(f'Min Word ({x_min[0]}): Column = {min_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: Term in Vocabulary\n",
      "----------------------------------------\n",
      "   14: vocabulary[114455] = the\n",
      "   10: vocabulary[115475] = to\n",
      "   10: vocabulary[89362] = of\n",
      "   10: vocabulary[85354] = my\n",
      "   10: vocabulary[28146] = and\n",
      "    9: vocabulary[68766] = it\n",
      "    7: vocabulary[29376] = arm\n",
      "    4: vocabulary[123292] = was\n",
      "    4: vocabulary[68532] = is\n",
      "    3: vocabulary[31767] = banks\n"
     ]
    }
   ],
   "source": [
    "# Number of terms to display\n",
    "top_display = 10\n",
    "\n",
    "# Sort our document term list, and unzip\n",
    "dtm_list.sort(key=itemgetter(2), reverse=True)\n",
    "i, j, c = zip(*dtm_list)\n",
    "\n",
    "# Grab out the keys and values for top terms\n",
    "x_keys = [(k, v) for k, v in terms.items() \n",
    "          if terms[k] in j[:top_display]]\n",
    "x_keys.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "# Grab the data, including counts from DTM list\n",
    "x_counts = dtm_list[:top_display]\n",
    "x_counts.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "# Now we merge the two lists so we can sort to \n",
    "# display terms in order\n",
    "x_merged = []\n",
    "for idx in range(len(x_keys)):\n",
    "    if x_keys[idx][1] != x_counts[idx][1]:\n",
    "        print('Error: column mismatch!')\n",
    "\n",
    "    x_merged.append((x_keys[idx][0], \n",
    "                     x_keys[idx][1], \n",
    "                     x_counts[idx][2]))\n",
    "\n",
    "x_merged.sort(key=itemgetter(2), reverse=True)\n",
    "\n",
    "# Print results\n",
    "print('Count: Term in Vocabulary')\n",
    "print(40*'-')\n",
    "for x in x_merged:\n",
    "    print(f'{x[2]:5d}: vocabulary[{x[1]}] = {x[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding Code cells, we used scikit learn to parse our sample text message by using `CountVectorizer`. If you look carefully, you will notice the default results are different than our standard Python text analysis. For example, note how we do not have the single character word `a` in the list of most common items. The reason is the default tokenizer for `CountVectorizer` removes single character words (as they are assumed to be uninformative). You can change this by providing a new regular expression to the `CountVectorizer` via the `token_pattern` parameter. \n",
    "\n",
    "Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the `CountVectorizer` regular expression to the string `'(?u)\\\\b\\\\w+\\\\b'`, how do the results change?\n",
    "2. Try vectorizing a different message (i.e., transform a different message, or messages). How do the results change?\n",
    "3. You can have `CountVectorizer` text processing object remove accented words by setting the `strip_accents` parameter to `True`. Does setting this parameter change the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## NLTK\n",
    "\n",
    "The scikit learn library is a general purpose, Python machine learning library that does include some basic text analysis functionality. Text analysis, however is an extremely large and growing topic. As a result, we will want to explore an additional library for natural language processing. This library, known as [Natural Language ToolKit][nltk], or NLTK, enables a wide range of text analyses either on its own, or in conjunction with scikit learn. The NLTK library is extensive and includes [documentation][nltkd] covering many of the topics we have demonstrated previously.\n",
    "\n",
    "In the rest of this notebook, we will explore how to use NLTK to perform basic text analysis, in a similar manner as demonstrated earlier via standard Python and the scikit learn library. First we import the library, and tokenize the message. We create an NLTK frequency distribution by passing a list of words to the NLTK `FreqDist` method.\n",
    "\n",
    "-----\n",
    "[nltk]: http://www.nltk.org\n",
    "[nltkd]: http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ('--', 78), ('>', 22), ('.', 18), ('the', 14), ('i', 12), ('of', 10),\n",
      "  ('my', 10), ('and', 10), ('it', 9), ('to', 9)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Tokenize a text document\n",
    "words = [word.lower() for word in nltk.word_tokenize(message)]\n",
    "\n",
    "# Count number of occurances for each token\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can clean up the list of tokens by using a regular expression with the `word_tokenize` method. We can reuse our previously defined regular expression `pattern = re.compile(r'[^\\w\\s]')` to identify tokens as one or more alphanumeric characters followed by a whitespace character. Doing this removes the punctuation tokens, as shown below.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ('the', 14), ('i', 12), ('to', 10), ('of', 10), ('my', 10), ('and', 10),\n",
      "  ('it', 9), ('arm', 7), ('a', 7), ('was', 4)]\n"
     ]
    }
   ],
   "source": [
    "# Specify an RE to parse a text document\n",
    "words = [word.lower() for word \n",
    "         in nltk.word_tokenize(re.sub(pattern, ' ', message))]\n",
    "\n",
    "# Count and display token occurances\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "One measure of the use of tokens in a document is [lexical diversity][ld], which is the fraction of unique tokens, or terms, in a document to the total tokens, or terms in a document. \n",
    "\n",
    "-----\n",
    "[ld]: https://en.wikipedia.org/wiki/Lexical_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message has 197 tokens and 336 words for a lexical diversity of 1.706\n"
     ]
    }
   ],
   "source": [
    "# Compute and display lexical diversity\n",
    "num_words = len(words)\n",
    "num_tokens = len(counts)\n",
    "lexdiv  =  num_words / num_tokens\n",
    "print(f'Message has {num_tokens} tokens and {num_words} words', end='')\n",
    "print(f' for a lexical diversity of {lexdiv:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can compute the number of [unique sample values][b], [number of samples outcomes][n], and the [maximum occurring token][m] with simple NLTK statistical functions. We can also iterate through and display the most commonly occurring terms and their counts.\n",
    "\n",
    "-----\n",
    "[b]: http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist.B\n",
    "[n]: http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist.N\n",
    "[m]: http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bins(tokens) = 197\n",
      "Number of sample outcomes = 336\n",
      "Maximum occuring token = the\n",
      "\n",
      "Term        : Count\n",
      "-------------------------\n",
      "the         :  14.000\n",
      "i           :  12.000\n",
      "to          :  10.000\n",
      "of          :  10.000\n",
      "my          :  10.000\n",
      "and         :  10.000\n",
      "it          :  9.000\n",
      "arm         :  7.000\n",
      "a           :  7.000\n",
      "was         :  4.000\n"
     ]
    }
   ],
   "source": [
    "# Display number of unique tokens (or bins)\n",
    "print(f'Number of unique bins(tokens) = {counts.B()}')\n",
    "print(f'Number of sample outcomes = {counts.N()}')\n",
    "print(f'Maximum occuring token = {counts.max()}')\n",
    "\n",
    "print(f'\\n{\"Term\":12s}: {\"Count\"}')\n",
    "print(25*'-')\n",
    "\n",
    "for token, freq in counts.most_common(top_display):\n",
    "    print(f'{token:12s}:  {freq:4.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "For some machine learning applications, words that occur rarely are important. For example, in a classification process, words that are uniquely assigned to a particular message should carry considerable weight. Taken to the extreme, words that only occur once in an entire set of documents, or corpus, provide unique insight into the particular text document in which they occur. A word that only occurs once in an entire corpus is known as a [_hapax_][ha]. NLTK has a `hapaxes` method that can be used to quickly find _hapaxes_ in a corpus.\n",
    "\n",
    "-----\n",
    "[ha]: https://en.wikipedia.org/wiki/Hapax_legomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'subject', 're', 'reply', 'organization', 'univ', 'pittsburgh', 'computer',\n",
      "  'science', 'lines', '48']\n"
     ]
    }
   ],
   "source": [
    "# Hapaxes\n",
    "pp.pprint(counts.hapaxes()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can also use NLTK to quickly see the most commonly used tokens. First, we can use the `tabulate` method to display the top tokens and their frequency (i.e., what we did in multiple lines of code earlier). Second, we can visually plot the counts of the top tokens by using the `plot` method. Note, we decorate the traditional plot by using , but the `plot` method will generate a simple visualization with one function call.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the   i  to  of  my and  it arm   a was \n",
      " 14  12  10  10  10  10   9   7   7   4 \n"
     ]
    }
   ],
   "source": [
    "counts.tabulate(top_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAGOCAYAAABG9w00AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvERERESyRMFLREREJEsUvERERESy5G9drvZLgOuEegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f17d630aa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Below we replicate (in a cleaner figure) the nltk \n",
    "# term-frequency plot method\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# Collect data from frequency distribution\n",
    "terms = []\n",
    "freqs = []\n",
    "term_lbls = []\n",
    "\n",
    "for term, freq in counts.most_common(top_display):\n",
    "    terms.append(term)\n",
    "    freqs.append(freq)\n",
    "    \n",
    "    # For tick labels\n",
    "    term_lbls.append(str(term))\n",
    "\n",
    "# Plot term frequencies\n",
    "axs.grid(True)\n",
    "axs.plot(freqs)\n",
    "\n",
    "# Show labels\n",
    "axs.set(title='Term Count', \n",
    "        xlabel='Terms', \n",
    "        ylabel='Frequency')\n",
    "\n",
    "# Set x axis labels\n",
    "axs.set_xticks(range(top_display))\n",
    "axs.set_xticklabels(term_lbls, rotation=45)\n",
    "\n",
    "# Clean up plot\n",
    "axs.spines['top'].set_visible(False)\n",
    "axs.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## NLTK Corpus\n",
    "\n",
    "To this point, we have analyzed the twenty newsgroup data that are available from within scikit learn. The NLTK library includes a number of [data sets][nc] that can be downloaded and used directly from within NLTK. One data set that we will use repeatedly in this course will be the NLTK [movie review corpus][mrc]. These data should be available in your Docker container. If not, NLTK provides the `nltk.download()` method to download either all or one particular corpus.\n",
    "\n",
    "In the following Code cells, we access the movie review data set, display (part) of the data set's README (or general documentation), before we begin to process the words or terms in the corpus.\n",
    "\n",
    "-----\n",
    "[nc]: http://www.nltk.org/nltk_data/\n",
    "[mrc]: http://www.cs.cornell.edu/people/pabo/movie-review-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Access the NLTK movie review data set. \n",
    "mvr = nltk.corpus.movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Polarity Dataset Version 2.0\n",
      "Bo Pang and Lillian Lee\n",
      "\n",
      "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
      "\n",
      "Distributed with NLTK with permission from the authors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the data set README,\n",
    "# remove array bounds to see entire file\n",
    "print(mvr.readme()[:178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Review has 39768 tokens and 1583820 words for a lexical diversity of 39.826\n"
     ]
    }
   ],
   "source": [
    "# Extract words as tokens\n",
    "mvr_words = mvr.words()\n",
    "\n",
    "# Extract summary information\n",
    "counts  = nltk.FreqDist(mvr_words)\n",
    "num_words = len(mvr_words)\n",
    "num_tokens = len(counts)\n",
    "lexdiv  =  num_words / num_tokens\n",
    "\n",
    "# Display results\n",
    "print(f'Movie Review has {num_tokens} tokens ', end='')\n",
    "print(f'and {num_words} words for a ', end='')\n",
    "print(f'lexical diversity of {lexdiv:4.3f}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party',\n",
      "  ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an',\n",
      "  'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his',\n",
      "  'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',',\n",
      "  'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?',\n",
      "  'watch']\n"
     ]
    }
   ],
   "source": [
    "# Display first fifty words\n",
    "pp.pprint(mvr.words()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The data are organized into separate files for each movie review. Since these reviews have an associated sentiment: negative and positive, the reviews are categorized (via a directory structure) into `neg` or `pos` respectively. We can directly access a single review, which can be treated as a single text document. In the next few Code cells we directly access the number of files, which can be used to count the number of reviews (assuming one review per file). We also display the contents of a single file, before displaying a subset of the files in one particular category, in this case `neg`, or negative reviews.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of reviews = 2000\n"
     ]
    }
   ],
   "source": [
    "# Each article is in a separate file\n",
    "print(f'Total Number of reviews = {len(mvr.fileids())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example File: neg/cv000_29416.txt\n"
     ]
    }
   ],
   "source": [
    "a_filename = mvr.fileids()[0]\n",
    "print(f'Example File: {a_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('plot : two teen couples go to a church party , drink and then drive . \\n'\n",
      " 'they get into an accident . \\n'\n",
      " 'one of the guys dies , but his girlfriend continues to see him in her life , '\n",
      " 'and has nightmares . \\n'\n",
      " \"what's the d\")\n"
     ]
    }
   ],
   "source": [
    "# Print part of the file\n",
    "pp.pprint(mvr.raw(a_filename)[:211])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "# Display article assigned categories\n",
    "pp.pprint(mvr.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt',\n",
      "  'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt',\n",
      "  'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt',\n",
      "  'neg/cv009_29417.txt', 'neg/cv010_29063.txt', 'neg/cv011_13044.txt',\n",
      "  'neg/cv012_29411.txt', 'neg/cv013_10494.txt', 'neg/cv014_15600.txt',\n",
      "  'neg/cv015_29356.txt', 'neg/cv016_4348.txt', 'neg/cv017_23487.txt',\n",
      "  'neg/cv018_21672.txt', 'neg/cv019_16117.txt']\n"
     ]
    }
   ],
   "source": [
    "# Find articles that have specific category\n",
    "pp.pprint(mvr.fileids('neg')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Given the contents of a file, we can process the associated text in the same manner as before. In this case, we tokenize one review into sentences as opposed to the traditional word tokens. After this, we create a list of words that are much longer than normal. As this simple example demonstrates, this can be a useful technique to search for potential problems, since in this case, none of the example shown are actual words.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '9', ':', 'its', 'pathetic', 'attempt', 'at', '\"', 'improving', '\"', 'on',\n",
      "  'a', 'shakespeare', 'classic', '.']\n",
      "['8', ':', 'its', 'just', 'another', 'piece', 'of', 'teen', 'fluff', '.']\n",
      "['7', ':', 'kids', 'in', 'high', 'school', 'are', 'not', 'that', 'witty', '.']\n",
      "['6', ':', 'the', 'wittiness', 'is', 'not', 'witty', 'enough', '.']\n",
      "['5', ':', 'the', 'comedy', 'is', 'not', 'funny', '.']\n",
      "['4', ':', 'the', 'acting', 'is', 'poor', '.']\n",
      "['3', ':', 'the', 'music', '.']\n",
      "['2', ':', 'the', 'poster', '.']\n",
      "['1', ':', 'its', 'worse', 'than', 'she', \"'\", 's', 'all', 'that', '!']\n",
      "[ '10', '=', 'a', 'classic', '9', '=', 'borderline', 'classic', '8', '=',\n",
      "  'excellent', '7', '=', 'good', '6', '=', 'better', 'than', 'average', '5',\n",
      "  '=', 'average', '4', '=', 'disappointing', '3', '=', 'poor', '2', '=',\n",
      "  'awful', '1', '=', 'a', 'crap', 'classic']\n"
     ]
    }
   ],
   "source": [
    "# Display sentances from an article\n",
    "a_filename = 'neg/cv779_18989.txt'\n",
    "for sent in mvr.sents(a_filename):\n",
    "    pp.pprint(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'uuuuuuggggggglllllllyyyyy', 's_funniest_home_videos_',\n",
      "  '_the_last_days_of_disco_', '_i_know_what_you_did_last_summer_',\n",
      "  '_fear_and_loathing_in_las_vegas_', '_breakfast_of_champions_',\n",
      "  '_breakfast_of_champions_', '_a_night_at_the_roxbury_',\n",
      "  '_a_night_at_the_roxbury_',\n",
      "  '__________________________________________________________',\n",
      "  '____________________________________________', '==========================',\n",
      "  '========================', '=======================',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------',\n",
      "  '--------------------------------------------------------------']\n"
     ]
    }
   ],
   "source": [
    "# We can process the words with normal Python\n",
    "# For example, print out really long words.\n",
    "long_words = [word for word in mvr_words if len(word) > 22]\n",
    "long_words.sort(reverse=True)\n",
    "pp.pprint(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we used NLTK to access the movie review corpus. Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Tabulate the top tokens in the entire movie review corpus.\n",
    "2. Plot the top tokens in the entire movie review corpus.\n",
    "3. Search for words longer than eighteen characters. Did you find any real words?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. Introduction to [Natural Language Processing][inlp]\n",
    "1. Wikipedia article on [Bag of Words][wbow] model\n",
    "1. Wikipedia article on [Document Term Matrix][wdtm]\n",
    "1. Gentle Introduction (in Python 2) to text analysis with Python, [part 1][nctap1] and [part 2][nctap2]\n",
    "1. NY Times [article on NLP][nytnlp]\n",
    "1. Wikipedia article on [Natural Language Processing][wnlp]\n",
    "1. Kaggle tutorial on [Bag of Words][kbow]\n",
    "1. Sections 2, 3, 5, and 6 from Chapter 1 of the free [NLTK version 3.0][nltk3] book\n",
    "\n",
    "-----\n",
    "\n",
    "[inlp]: https://blog.monkeylearn.com/the-definitive-guide-to-natural-language-processing/\n",
    "\n",
    "[wnlp]: https://en.wikipedia.org/wiki/Natural_language_processing\n",
    "[wbow]: https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "[wdtm]: https://en.wikipedia.org/wiki/Document-term_matrix\n",
    "\n",
    "\n",
    "[nytnlp]: http://www.nytimes.com/2003/10/16/technology/circuits/16mine.html?pagewanted=print\n",
    "[nltk3]: http://www.nltk.org/book/ch01.html\n",
    "\n",
    "[nctap1]: http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-1/\n",
    "[nctap2]: http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-2/\n",
    "\n",
    "[kbow]: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2017: Robert J. Brunner at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
