{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bagging and Random Forests \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we introduce the concept of [_bagging_][wbag], which is shorthand for bootstrap aggregation, where random samples of the data are used to construct multiple decision trees. Since each tree only sees part of the data, each tree is less accurate than if it had been constructed over the full data set. Thus, each tree is known as a _weak learner_. A more powerful, meta-estimator is subsequently constructed by averaging over these many weak learners. The approach of constructing weak learners, and combining them into a more powerful estimator is at the heart of several, very powerful machine learning techniques, among which, the most popular one is the [random forest][wrf].\n",
    "\n",
    "In this notebook, we first introduce the formalism behind bagging, including a discussion of the concept of bootstrapping. Next, we move on to a discussion of the random forest algorithm, which will include its application to both classification and regression tasks.\n",
    "\n",
    "-----\n",
    "[wbag]: https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "[wrf]: https://en.wikipedia.org/wiki/Random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Formalism](#Formalism)\n",
    "\n",
    " - [Random Forest](#Random-Forest)\n",
    "\n",
    "[Random Forest: Classification](#Random-Forest:-Classification)\n",
    "\n",
    "- [Classification: Adult Income Data](#Classification:-Adult-Income-Data)\n",
    "- [Random Forest: Feature Importance](#Random-Forest:-Feature-Importance)\n",
    "\n",
    "[Random Forest: Regression](#Random-Forest:-Regression)\n",
    "\n",
    "- [Regression: Auto MPG Data](#Regression:-Auto-MPG-Data)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the _Formalism_ section of this Notebook, we first have our standard notebook setup code.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Formalism\n",
    "\n",
    "One of the simplest machine learning algorithms to understand is the decision tree. Often, a decision tree is made as large as possible to provide the best predictive model, as this produces a high purity in the leaf nodes. Doing so however, can lead to **overfitting**, where the model predicts very accurately on the training data but fails to generalize to the test data; the accuracy is, as a result, much lower. \n",
    "\n",
    "A simple approach to overcoming the overfitting problem is to train many decision trees on a subset of the data and to average the resulting predictions. This process is known as bootstrap aggregation, which is often shortened to bagging. Of these two terms, aggregation is simple to understand, one simply aggregates (averages) the predictions of the many trees. \n",
    "\n",
    "The term bootstrap is a statistical term that defines how a sample can be constructed from an original data set. Given a data set, there are two simple ways to construct a new sample. As a specific example, consider building a list of shows you wish to watch from an online provider like Netflix or Amazon by placing them in virtual cart. In the first approach, when you choose a show from the virtual shelf and place it in your cart, the show is no longer available on the shelf. This is known as sampling without replacement since the show is only present in your cart. In the second approach, you choose a show and place it in your cart, but there remains a copy of the show on the virtual shelf and others can still pick it. This is known as sampling with replacement, since we replace the original instance.\n",
    "\n",
    "Sampling with replacement has several advantages that make it important for machine learning. First, we can construct many large samples from our original data set, where each sample is not limited by the size of the original data set. For example, if our original data set contained 100 entries, sampling without replacement would mean we could only create ten new samples that each had ten entries. On the other hand, sampling with replacement means we could create 100 (or more) new samples that each have ten (or more) entries.\n",
    "\n",
    "Building many samples from a parent population allows us to build an estimator on each sample and average (or aggregate) the results. This is demonstrated in the following figure, where an original data set is used to train several decision trees. In this case, each tree is constructed from a bootstrap sample of the original data set. The predictions from these trees are aggregated at the end to make a final prediction.\n",
    "\n",
    "![Decision Trees Image](images/dt-rjb-2.png)\n",
    "\n",
    "The scikit-learn library provides a bagging meta-estimator, that can generate bootstrap samples, apply a standard estimator (including other algorithms beyond a decision tree), and aggregate the resulting predictions. This technique can be used for classification tasks ([`BaggingClassifier`][skbc]) or for regression ([`BaggingRegressor`][skbr])\n",
    "\n",
    "These estimators have several hyperparameters that control their performance:\n",
    "- `base_estimator`: The estimator to use on each sample; by default this is a decision tree.\n",
    "- `n_estimators`: The number of base estimators to create for the ensemble; by default this is ten.\n",
    "- `max_samples` : The number of instances to draw from the parent population to train each base estimator; by default this is one.\n",
    "- `max_features`: The number of features to draw from the  parent population to train each base estimator; by default this is one.\n",
    "\n",
    "This bagging estimator allows different basic algorithms, beyond the decision tree, to be used for ensemble learning. For the rest of the notebook, however, we will focus on a specific ensemble technique that efficiently implement bagging by using decision trees: the random forest.\n",
    "\n",
    "Beyond improved prediction, bagging algorithms provide an additional benefit. Since each tree (or other learning algorithm in the case of a _Bagging_ estimator) is constructed from a subsample of the original data, the performance of that tree can be tested on the data from the original data that were not used in its construction. These data are known as _out-of-bag_ data and provide a useful metric for the performance of each individual tree used in the ensemble. \n",
    "\n",
    "-----\n",
    "[skbc]:http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "[skbr]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Random Forest\n",
    "\n",
    "Bootstrap is a general procedure that can be used to reduce the variance for those algorithms that have high variances like decision tree. When bagging with decision trees, we are less concerned about individual tree overfitting the training data.\n",
    "\n",
    "A [random forest][wrf] employs bagging to create a set of decision trees from a given data set. Each tree is constructed from a bootstrap sample, and the final prediction is generated by aggregating the predictions of the individual trees, just like the previous code example demonstrated by using the mean of the sample means to estimate the population mean.\n",
    "\n",
    "Normally, when deciding on a split point during the construction of a decision tree, all features are evaluated and the one that has the highest impurity (or produces the largest information gain) is selected as the feature on which to split along with the value, at which to split that feature. In a random forest, a random subset of all features is used to make the split choice, and the best feature on which to split is selected form this subset. \n",
    "\n",
    "This extra randomness produces individual decision trees that are less sensitive to small scale fluctuations, which is known as under-fitting. As a result, each newly created decision tree is a weak learner since they are not constructed from all available information. Yet, since each decision tree is constructed from different sets of features, by aggregating their predictions, the final random forest prediction is improved and less affected by overfitting.\n",
    "\n",
    "Each tree in the random forest is constructed from a different combination of features. As a result, we can use the _out-of-bag_ performance from each tree to rank the importance of the features used to construct the trees in the forest. This allows for robust estimates of feature importance to be computed after constructing a random forest, which can provide useful insight into the nature of a training data set.\n",
    "\n",
    "-----\n",
    "\n",
    "[wrf]: https://en.wikipedia.org/wiki/Random_forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Random Forest: Classification\n",
    "\n",
    "Having completed the discussion on bootstrap aggregation, and introduced the random forest algorithm, we can now transition to putting this powerful ensemble algorithm to work. The scikit-learn library provides a robust implementation of the random forest algorithm. This implementation includes the _Bagging estimator's_ hyperparameters. The two most important hyperparameters for a random forest are\n",
    "- `n_estimators`, The number of decision trees that will be constructed to build the forest; the default value is 10 before sciket-learn v0.22 and 100 afterwards. \n",
    "- `max_features`, The number of features to examine when choosing the best split feature and value. By default this is `auto`, which means the square root of the total number of features. Other values can be an integer number of features, a floating point percentage of the total number of features (e.g., 25% of all features randomly selected), the square root of the total number of features, and the base two logarithm of the total number of features.\n",
    "\n",
    "You may run `help(RandomForestClassifier)` to view more details about the model and the hyper parameters.\n",
    "\n",
    "To demonstrate using a random forest with the scikit-learn library, we will use [Adult Income Dataset][uciad] to see the impact of a random forest on generating predictions from a complex data set.\n",
    "\n",
    "----\n",
    "[skdtc]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "[uciad]: https://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Classification: Adult Income Data\n",
    "\n",
    "We can now apply the Random Forest algorithm to the Adult Income data to create a classification model. The basic approach is simple, and follows the standard scikit-learn estimator philosophy:\n",
    "\n",
    "1. Prepare data. We first load data, then create label from `Salary` column, then encode categorical features. Note that for tree-based ensemble classifier, we don't have to create dummy variables for categorical features, and we also don't need to scale or normalize training features. Finally, we will split the data into training and testing sets.\n",
    "2. Import our estimator, [`RandomForestClassifier`][skdtc], from the proper scikit-learn module, `ensemble`.\n",
    "3. Create the estimator and specify the appropriate hyperparameters. For a random forest, we can accept the defaults, or specify values for specific hyperparameters such as `n_estimators` or `max_features`.\n",
    "4. Fit the model to the training data.\n",
    "5. Predict new classes with our trained model (or in the simple demonstration below, generate a performance metric via the `score` method).\n",
    "\n",
    "These steps are demonstrated in the following Code cell.\n",
    "\n",
    "In the first Code cell, we load the data and create label from `Salary`. \n",
    "\n",
    "In the second Code cell, we encode categorical features we are going to choose. We will only encode features with string values. We will define label and data, and then split them to 60% training and 40% testing.\n",
    "\n",
    "In the third Code cell, we create the estimator, the only hyperparameter that we specify at this time is  random_state in order to ensure reproducibility. Then we fit the estimator to our training data and generate a performance score on the testing data.\n",
    "\n",
    "In the next Code cell we plot the confusion matrix.\n",
    "\n",
    "-----\n",
    "[skdtc]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>FNLWGT</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Sex</th>\n",
       "      <th>CapitalGain</th>\n",
       "      <th>CapitalLoss</th>\n",
       "      <th>HoursPerWeek</th>\n",
       "      <th>NativeCountry</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1099</td>\n",
       "      <td>18</td>\n",
       "      <td>Private</td>\n",
       "      <td>256005</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3519</td>\n",
       "      <td>24</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>231473</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3753</td>\n",
       "      <td>23</td>\n",
       "      <td>Private</td>\n",
       "      <td>38251</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3876</td>\n",
       "      <td>30</td>\n",
       "      <td>Private</td>\n",
       "      <td>181992</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3244</td>\n",
       "      <td>41</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>190786</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age   Workclass  FNLWGT      Education  EducationLevel  \\\n",
       "1099   18     Private  256005        HS-grad               9   \n",
       "3519   24   State-gov  231473     Assoc-acdm              12   \n",
       "3753   23     Private   38251        HS-grad               9   \n",
       "3876   30     Private  181992   Some-college              10   \n",
       "3244   41   Local-gov  190786      Bachelors              13   \n",
       "\n",
       "            MaritalStatus       Occupation    Relationship    Race      Sex  \\\n",
       "1099        Never-married    Other-service       Own-child   White     Male   \n",
       "3519   Married-civ-spouse     Adm-clerical         Husband   White     Male   \n",
       "3753        Never-married    Other-service       Own-child   White   Female   \n",
       "3876        Never-married            Sales   Not-in-family   Black   Female   \n",
       "3244             Divorced   Prof-specialty   Not-in-family   White     Male   \n",
       "\n",
       "      CapitalGain  CapitalLoss  HoursPerWeek   NativeCountry  Salary  Label  \n",
       "1099            0            0            40   United-States   <=50K      0  \n",
       "3519            0            0            30   United-States   <=50K      0  \n",
       "3753            0            0            40   United-States   <=50K      0  \n",
       "3876            0            0            35   United-States   <=50K      0  \n",
       "3244            0            0            20   United-States   <=50K      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV data\n",
    "adult_data = pd.read_csv('data/adult_income.csv')\n",
    "\n",
    "# Create label column, one for >50K, zero otherwise.\n",
    "adult_data['Label'] = adult_data['Salary'].map(lambda x : 1 if '>50K' in x else 0)\n",
    "\n",
    "adult_data.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode categorical features with string value\n",
    "adult_data['Sex_code'] = LabelEncoder().fit_transform(adult_data.Sex)\n",
    "adult_data['Relationship_code'] = LabelEncoder().fit_transform(adult_data.Relationship)\n",
    "adult_data['Race_code'] = LabelEncoder().fit_transform(adult_data.Race)\n",
    "\n",
    "#pick training features\n",
    "data = adult_data[['Age', 'HoursPerWeek', 'EducationLevel', 'CapitalGain', 'CapitalLoss', 'Sex_code', 'Relationship_code', 'Race_code']]\n",
    "label = adult_data['Label']\n",
    "\n",
    "#split to training and testing\n",
    "d_train, d_test, l_train, l_test = train_test_split(data, label, test_size=0.4, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "adult_model = RandomForestClassifier(random_state=23)\n",
    "\n",
    "adult_model = adult_model.fit(d_train, l_train)\n",
    "\n",
    "# Classify test data and display score and report\n",
    "predicted = adult_model.predict(d_test)\n",
    "score = 100.0 * metrics.accuracy_score(l_test, predicted)\n",
    "print(f'Decision Tree Classification [Adult Data] Score = {score:4.1f}%\\n')\n",
    "print('Classification Report:\\n {0}\\n'.format(metrics.classification_report(l_test, predicted)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_code import mlplots as ml\n",
    "\n",
    "# Call confusion matrix plotting routine\n",
    "ml.confusion(l_test, predicted, ['Low', 'High'], 'Random Forest Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "### Random Forest: Feature Importance\n",
    "\n",
    "As the previous example demonstrated, the random forest is easy to use and often provides impressive results. In addition, by its very nature, a random forest provides an implicit measure of the importance of the individual features in generating the final predictions. While an individual decision tree provides this information, the random forest provides an aggregated result, which is generally more insightful and less sensitive to fluctuations in the training data that might bias the importance values determined by a decision tree. In the calculation of feature importance from a random forest, higher values indicate a more important feature. \n",
    "\n",
    "We demonstrate how to extract the feature importance for a random forest classifier in the following Code cell. We zip the training data column names with the model's feature_importances_ attribute, then convert it to a dataframe so that we can sort by the importance and print it out.\n",
    "\n",
    "From the feature importance we can see that Sex and Race are not very important in determining income, comparing to Age and Education Level.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display feature importance as computed from the random forest\n",
    "\n",
    "# Display name and importance\n",
    "feature_importance = pd.DataFrame(list(zip(d_train.columns, adult_model.feature_importances_)), columns=['Feature', 'Importance'])\n",
    "feature_importance.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the previous Code cell, we create random forest classifier with default hyperparamemter values. Try making the following changes and compare the different results.\n",
    "\n",
    "1. Set the values of hyperparameter `n_estimators` to 5, 10 and 15.\n",
    "2. Set hyperparameter `max_features` to different values to indicate how many features should be used in the splitting process.\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Random Forest: Regression\n",
    "\n",
    "A random forest can also be used to perform regression; however, in this case the goal is to create trees whose leaf nodes contain data that are nearby in the overall feature space. To make a prediction from a continuous value from a tree, we either have leaf nodes with only one feature, and use the relevant feature from that instance as our predictor, or we compute summary statistics from the instances in the appropriate leaf node, such as the mean or mode. In the end, the random forest aggregates the individual tree regression predictions into a final prediction.\n",
    "\n",
    "To perform regression with the scikit-learn library we employ the [`RandomForestRegressor`][skrfr] estimator in the tree module. One point, which was also true for classification, by specifying the `random_state` hyperparameter, we ensure reproducibility. This is because every time a tree is constructed, the features are randomly selected. Thus, even if we use the same set of hyperparameters and the same set of training data, we can end up with different trees, and thus a different forest, if the `random_state` hyperparameter is not fixed.\n",
    "\n",
    "In this section we employ a random forest to perform regression on the automotive fuel performance prediction as these data were fully described in the _Introduction to Decision Trees_ notebook. First, we will introduce these data, and prepare them for the regression task. Then we will employ the patsy module to use a regression formula to create our dependent and independent feature matrices. Finally, we will construct a decision tree regressor on these data and evaluate its performance.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "[skrfr]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Regression: Auto MPG Data\n",
    "\n",
    "Random forest can also be used to perform regression. To perform regression with the scikit-learn library we employ the RandomForestRegressor estimator in the ensemble module. This estimator employs the same set of hyperparameters as the RandomForestClassifier estimator, and is therefore, used in a similar manner. One other point, which is also true for classification, by specifying the random_state hyperparameter, we ensure reproducibility.\n",
    "\n",
    "In this section we employ decision trees to perform regression on the [automobile fuel performance prediction data][uciap]. The data contains nine features: mpg, cylinders, displacement, horsepower, weight, acceleration, model year, origin, and car name. Of these, the first is generally treated as the dependent variable (i.e., we wish to predict the fuel efficiency of the cars), while the next seven features are generally used as the independent variables. The last feature (car name) is a string that is unlikely to be useful when predicting on new, unseen data, and therefore, is not included in our analysis.\n",
    "\n",
    "In the first two Code cells, we load the data into a DataFrame, then encode origin column. After this, we select our independent and dependent variables. \n",
    "\n",
    "----\n",
    "[uciap]: https://archive.ics.uci.edu/ml/datasets/auto+mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data = sns.load_dataset('mpg')\n",
    "auto_data['origin_code'] = LabelEncoder().fit_transform(auto_data.origin)\n",
    "auto_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick dependent and independent variables\n",
    "y = auto_data['mpg']\n",
    "x = auto_data[['cylinders', 'displacement', 'weight', 'acceleration', 'model_year', 'origin_code']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "With the dependent variable(y) and independent variable(x) determined, we can now build a regressive model. First, we import the `RandomForestRegressor` before splitting our independent and dependent variables into training and testing samples. Next, we create our estimator, specifying a value for our `random_state` hyperparameter to enable reproducibility. Finally, we fit the model and display a predictive score. \n",
    "\n",
    "The second Code cell computes several different regression performance metrics and displays the results. \n",
    "\n",
    "This model predicts the fuel performance fairly well. You may compare the regression performance metrics with that of other machine learning algorithms we introduced in previous lessons.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Split data intro training:testing data set\n",
    "ind_train, ind_test, dep_train, dep_test = train_test_split(x, y, test_size=0.4, random_state=23)\n",
    "\n",
    "# Create Regressor with default properties\n",
    "auto_model = RandomForestRegressor(random_state=23)\n",
    "\n",
    "# Fit estimator and display score\n",
    "auto_model = auto_model.fit(ind_train, dep_train)\n",
    "print('Score = {:.1%}'.format(auto_model.score(ind_test, dep_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Regress on test data\n",
    "pred = auto_model.predict(ind_test)\n",
    "\n",
    "# Copute performance metrics\n",
    "mae = metrics.mean_absolute_error(dep_test, pred)\n",
    "mse = metrics.mean_squared_error(dep_test, pred)\n",
    "mr2 = metrics.r2_score(dep_test, pred)\n",
    "\n",
    "# Display metrics\n",
    "print(f'R^2 Score             = {mr2:5.3f}')\n",
    "print(f'Mean Absolute Error   = {mae:4.2f}')\n",
    "print(f'Mean Squared Error    = {mse:4.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the previous Code cells, we constructed a random forest for regression and applied it to the automobile fuel performance prediction task. The initial result was pretty good, but try making the following changes to see if you can do better.\n",
    "\n",
    "1. Change the features used in the regression; for example drop one column, such as `origin`.\n",
    "2. Try using different hyperparameter values, such as changing the number of estimators (`n_estimators`) or the number of features (`max_features`).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. The scikit-learn documentation provides a nice introduction to [_bagging_][1] and the estimators that implement this ensemble learning technique.\n",
    "2. A blog [article][2] on random forests in Python\n",
    "3. An article on [building random forests][3] from scratch in Python at the Machine Learning Mastery website\n",
    "4. An article on [random forests][3] at the Analytics Vidhya website\n",
    "5. A short [discussion][5] on the benefits of random forests\n",
    "-----\n",
    "[1]: http://scikit-learn.org/stable/modules/ensemble.html#bagging\n",
    "\n",
    "[2]: http://blog.yhat.com/posts/random-forests-in-python.html\n",
    "\n",
    "[3]: https://machinelearningmastery.com/implement-random-forest-scratch-python/\n",
    "\n",
    "[4]: https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/\n",
    "\n",
    "[5]: http://fastml.com/intro-to-random-forests/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
