{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bagging\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we introduce the concept of [_bagging_][wbag], which is shorthand for bootstrap aggregation, where random samples of the data are used to construct multiple decision trees. Since each tree only sees part of the data, each tree is less accurate than if it had been constructed over the full data set. Thus, each tree is known as a _weak learner_. A more powerful, meta-estimator is subsequently constructed by averaging over these many weak learners. The approach of constructing weak learners, and combining them into a more powerful estimator is at the heart of several, very powerful machine learning techniques, among which, the most popular one is the [random forest][wrf].\n",
    "\n",
    "In this notebook, we first introduce the formalism behind bagging, including a discussion of the concept of bootstrapping. Next, we move on to a discussion of the random forest algorithm, which will include its application to both classification and regression tasks.\n",
    "\n",
    "-----\n",
    "[wbag]: https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "[wrf]: https://en.wikipedia.org/wiki/Random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Formalism](#Formalism)\n",
    "\n",
    "- [Bootstrap](#Bootstrap)\n",
    "- [Random Forest](#Random-Forest)\n",
    "- [Extremely Randomized Trees](#Extremely-Randomized-Trees)\n",
    "\n",
    "[Random Forest: Classification](#Random-Forest:-Classification)\n",
    "\n",
    "- [Classification: Iris Data](#Classification:-Iris-Data)\n",
    "- [Random Forest: Decision Surface](#Random-Forest:-Decision-Surface)\n",
    "- [Random Forest: Hyperparameters](#Random-Forest:-Hyperparameters)\n",
    "- [Random Forest: Feature Importance](#Random-Forest:-Feature-Importance)\n",
    "- [Classification: Adult Data](#Classification:-Adult-Data)\n",
    "\n",
    "[Random Forest: Regression](#Random-Forest:-Regression)\n",
    "\n",
    "- [Regression: Auto MPG Data](#Auto-MPG-Data)\n",
    "- [Regression: Multi-Output](#Regression:-Multi-Output)\n",
    "\n",
    "[Extremely Randomized Trees](#Extremely-Randomized-Trees)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the _Formalism_ section of this Notebook, we first have our standard notebook setup code.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Formalism\n",
    "\n",
    "One of the simplest machine learning algorithms to understand is the decision tree. Often, a decision tree is made as large as possible to provide the best predictive model, as this produces a high purity in the leaf nodes. Doing so, however, can lead to **overfitting** where the model predicts very accurately on the training data but fails to generalize to the test data; and where the accuracy is, as a result, much lower. \n",
    "\n",
    "A simple approach to overcoming the overfitting problem is to train many decision trees on a subset of the data and to average the resulting predictions. This process is known as bootstrap aggregation, which is often shortened to bagging. Of these two terms, aggregation is simple to understand, one simply aggregates (or, in particular, averages) the predictions of the many trees. \n",
    "\n",
    "The term bootstrap is a statistical term that defines how a sample can be constructed from an original data set. Given a data set, there are two simple ways to construct a new sample. As a specific example, consider building a list of shows you wish to watch from an online provider like Netflix or Amazon by placing them in  virtual cart. In the first approach, you take a show off the virtual shelf and place it in your cart, the show is no longer available on the shelf. This is known as sampling without replacement since the show is only present in your cart. In the second approach, you take a show and place it in your cart, but there remains a copy of the show on the virtual shelf and others can still pick it. This is known as sampling with replacement, since we replace the original instance.\n",
    "\n",
    "Sampling with replacement has several advantages that make it important for machine learning. First, we can construct many large samples from our original data set, where each sample is not limited by the size of the original data set. For example, if our original data set contained 100 entries, sampling without replacement would mean we could only create ten new samples that each had ten entries. On the other hand, sampling with replacement means we could create 100 (or more) new samples that each have ten (or more) entries.\n",
    "\n",
    "Building many samples from a parent population allows us to build an estimator on each sample and average (or aggregate) the results. This is demonstrated in the following figure, where an original data set is used to train a number of decision trees. In this case, each tree is constructed from a bootstrap sample of the original data set. The predictions from these trees are aggregated at the end to make a final prediction.\n",
    "\n",
    "![Decision Trees Image](images/dt-rjb-2.png)\n",
    "\n",
    "The scikit learn library provides a bagging meta-estimator, that can generate bootstrap samples, apply a standard estimator (including other algorithms beyond a decision tree), and aggregate the resulting predictions. This technique can be used for classification tasks ([`BaggingClassifier`][skbc]) or for regression ([`BaggingRegressor`][skbr])\n",
    "\n",
    "These estimators have several hyperparameters that control their performance:\n",
    "- `base_estimator`: The estimator to use on each sample, by default this is a decision tree.\n",
    "- `n_estimators`: The number of base estimators to create for the ensemble, by default this is ten.\n",
    "- `max_samples` : The number of instances to draw from the parent population to train each base estimator, by default this is one.\n",
    "- `max_features`: The number of features to draw from the  parent population to train each base estimator, by default this is one.\n",
    "\n",
    "This bagging estimator allows different basic algorithms, beyond the decision tree, to be used for ensemble learning. For the rest of the notebook, however, we will focus on two specific ensemble techniques that efficiently implement bagging by using decision trees: the random forest and extremely randomized trees.\n",
    "\n",
    "Beyond improved prediction, bagging algorithms provide an additional benefit. Since each tree (or other learning algorithm in the case of a _Bagging_ estimator) is constructed from a subsample of the original data, the performance of that tree can be tested on the data from the original data that were not used in its construction. These data are known as _out-of-bag_ data, and provide a useful metric for the performance of each individual tree used in the ensemble. \n",
    "\n",
    "Before introducing the random forest, we first explore the construction and use of bootstrap samples.\n",
    "\n",
    "-----\n",
    "[skbc]:http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "[skbr]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Bootstrap\n",
    "\n",
    "Formally, a bootstrap refers to any statistical process that relies on the generation of random samples with replacement. To demonstrate the benefit of the bootstrap, we will bootstrap the `size` feature from the _tips_ data set, which is the number of patrons served by the restaurant for a meal. We will try to estimate the population mean and variance of the `size`.\n",
    "\n",
    "The first Code cell below loads the tips data and shows 5 samples of the `size` feature.\n",
    "\n",
    "In the next cell, we will calculate the sample mean and variance of `size` feature based on the whole `size` column.\n",
    "\n",
    "In the third cell, we will generate 1000 random samples with replacemen, each random sample has 50 data points. we will compute the mean for each sample, creating an array of means. In this case, we can consider each sample mean to be an estimate of the mean of the parent population. We can average these means (i.e., aggregate) these sample means to provide an estimate of the population mean, along with a measure of the uncertainty in this estimate, by computing the standard deviation of our sample means.\n",
    "\n",
    "-----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44    4\n",
       "54    4\n",
       "8     2\n",
       "90    2\n",
       "26    2\n",
       "Name: size, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data Set\n",
    "tdf = sns.load_dataset('tips')\n",
    "\n",
    "# Display several random 'size' features\n",
    "tdf['size'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The next Code cell extract `size` feature as a NumPy array, and displays the number of instances, which is 244. We also compute the mean and standard deviation of this sample. These values will be compared to the same metrics computed from the random samples we will generate.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances = 244\n",
      "Mean (original sample) = 2.570\n",
      "Standard deviation (original sample) = 0.949\n"
     ]
    }
   ],
   "source": [
    "# Extract size feature as an array\n",
    "sizes = tdf['size'].as_matrix()\n",
    "\n",
    "# Display number of instances\n",
    "print(f'Number of instances = {sizes.shape[0]}')\n",
    "\n",
    "# Compute and display population statistics\n",
    "print(f'Mean (original sample) = {np.mean(sizes):5.3f}')\n",
    "print(f'Standard deviation (original sample) = {np.std(sizes):5.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can now generate 1000 random samples, with replacement from these data. In the following Code cell, we create a NumPy `RandomState` to ensure reproducibility of our results, and define our initial sample size as 50. This value can be changed, and since we are using replacement, can actually be larger than the size of the parent population (although this is generally not a good idea). Finally, we employ the NumPy `choice` method to sample `sample_size` instances with replacement, which is indicated by passing the parameter `replace=True`. \n",
    "\n",
    "The output is a NumPy array that is randomly sampled with replacement from the parent array, `sizes`. Note, if we pass the parameter `replace=False` we produce a sample without replacement, which can be useful in other contexts.\n",
    "\n",
    "We then calculate the mean and standard deviation of the random sample means.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50)\n",
      "Mean (1000 samples) = 2.571\n",
      "Standard Deviation (1000 samples) = 0.131\n"
     ]
    }
   ],
   "source": [
    "sample_size = 50\n",
    "num_samples = 1000\n",
    "\n",
    "# Define random seed for reproducability\n",
    "rng = np.random.RandomState(23)\n",
    "\n",
    "the_sample = np.zeros((num_samples, sample_size))\n",
    "\n",
    "#for idx in range(num_samples):\n",
    "#    for j in range(sample_size):\n",
    "#        the_sample[idx][j] = rng.choice(sizes, 1, replace=True)\n",
    "\n",
    "for idx in range(num_samples):\n",
    "    the_sample[idx] = rng.choice(sizes, sample_size, replace=True)\n",
    "\n",
    "print(the_sample.shape)\n",
    "# Compute and display sample statistics\n",
    "the_means = np.mean(the_sample, axis=1)\n",
    "print(f'Mean ({num_samples} samples) = {np.mean(the_means):5.3f}')\n",
    "print(f'Standard Deviation ({num_samples} samples) = {np.std(the_means):5.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The mean of the random sample means is nearly identical to the mean value of our sample mean with 244 data points, but the uncertainty(standard deviation) on this estimate is remarkably low.\n",
    "\n",
    "This simple example has demonstrated how bootstrap aggregation, in this case of the sample means, can provide a powerful estimator of a population statistic. In each case, we generate multiple samples with replacement, and compute a statistics across these samples, and aggregate the result at the end. This concept underlies all bagging estimators.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "Bootstrap is a general procedure that can be used to reduce the variance for those algorithm that have high variance. An algorithm that has high variance are decision trees. When bagging with decision trees, we are less concerned about individual trees overfitting the training data.\n",
    "\n",
    "A [random forest][wrf] employs bagging to create a set of decision trees from a given data set. Each tree is constructed from a bootstrap sample, and the final prediction is generated by aggregating the predictions of the individual trees, just like the previous code example demonstrated by using the mean of the sample means to estimate the population mean.\n",
    "\n",
    "Normally, when deciding on a split point during the construction of a decision tree, all features are evaluated and the one that has the highest impurity (or produces the largest information gain) is selected as the feature on which to split along with the value at which to split that feature. In a random forest, a random subset of all features are used to make the split choice, and the best feature on which to split is selected form this subset. \n",
    "\n",
    "This extra randomness produces individual decision trees that are less sensitive to small scale fluctuations, which is known as under-fitting. As a result, each newly created decision tree is a weak learner since they are not constructed from all available information. Yet, since each decision tree is constructed from different sets of features, by aggregating their predictions, the final random forest prediction is improved and less affected by overfitting.\n",
    "\n",
    "Each tree in the random forest is constructed from a different combination of features. As a result, we can use the _out-of-bag_ performance from each tree to rank the importance of the features used to construct the trees in the forest. This allows for robust estimates of feature importance to be computed after constructing a random forest, which can provide useful insight into the nature of a training data set.\n",
    "\n",
    "-----\n",
    "\n",
    "[wrf]: https://en.wikipedia.org/wiki/Random_forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Random Forest: Classification\n",
    "\n",
    "Having completed the discussion on bootstrap aggregation, and introduced the random forest algorithm, we can now transition to putting this powerful ensemble algorithm to work. The scikit learn library provides a robust implementation of the random forest algorithm. This implementation includes the _Bagging estimator's_ hyperparameters. The two most important hyperparameters for a random forest are\n",
    "- `n_estimators`, which is the number of decision trees that will be constructed to build the forest, the default value is ten, and \n",
    "- `max_features`, which is the number of features to examine when choosing the best split feature and value. By default this is `auto`, which means the square root of the total number of features. Other values can be an integer number of features, a floating point percentage of the total number of features (e.g., 25% of all features randomly selected), the square root of the total number of features, and the base two logarithm of the total number of features.\n",
    "\n",
    "You may run `help(RandomForestClassifier)` to view more details about the model and the hyper parameters.\n",
    "\n",
    "To demonstrate using a random forest with the scikit-learn library, we will use [Adult Income Dataset][uciad] to see the impact of a random forest on generating predictions from a complex data set.\n",
    "\n",
    "----\n",
    "[skdtc]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "[uciad]: https://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Classification:  Adult Income Data\n",
    "\n",
    "We can now apply the Random Forest algorithm to the Adult Income data to create a classification model. The basic approach is simple, and follows the standard scikit-learn estimator philosophy:\n",
    "\n",
    "1. Prepare data. We first load data from UCI repository, then create label from `Salary` column, then encode categorical features. Note that for tree-based ensemble classifier, we don't have to create dummy variables for categorical features, and we also don't need to scale or normalize training features. Finally, we will split the data into training and testing sets.\n",
    "2. Import our estimator, [`RandomForestClassifier`][skrfc], from the proper scikit-learn module, `ensemble`.\n",
    "3. Create the estimator and specify the appropriate hyperparameters. For a random forest, we can accept the defaults, or specify values for specific hyperparameters such as `n_estimators` or `max_features`.\n",
    "4. Fit the model to the training data.\n",
    "5. Predict new classes with our trained model (or in the simple demonstration below, generate a performance metric via the `score` method).\n",
    "\n",
    "These steps are demonstrated in the following code cell.\n",
    "\n",
    "In the first code cell, we load the data and create label from `Salary`. \n",
    "\n",
    "In the second code cell, we encode categorical features we are going to choose. We will only encode features with string values. Then we will define label and data, and split them to 60% training and 40% testing.\n",
    "\n",
    "In the third code cell, we create the estimator, the only hyperparameter that we specify at this time is  random_state in order to ensure reproducibility. Then we fit the estimator to our training data, and generate a performance score on the testing data.\n",
    "\n",
    "In the next code cell we plot the confusion matrix.\n",
    "\n",
    "-----\n",
    "[skdtc]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>FNLWGT</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Sex</th>\n",
       "      <th>CapitalGain</th>\n",
       "      <th>CapitalLoss</th>\n",
       "      <th>HoursPerWeek</th>\n",
       "      <th>NativeCountry</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>104457</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>?</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7130</th>\n",
       "      <td>69</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>128206</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14070</th>\n",
       "      <td>60</td>\n",
       "      <td>Private</td>\n",
       "      <td>145664</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23964</th>\n",
       "      <td>38</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>58972</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14126</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>187584</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Canada</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age          Workclass  FNLWGT      Education  EducationLevel  \\\n",
       "12172   27            Private  104457      Bachelors              13   \n",
       "7130    69   Self-emp-not-inc  128206        HS-grad               9   \n",
       "14070   60            Private  145664   Some-college              10   \n",
       "23964   38   Self-emp-not-inc   58972        HS-grad               9   \n",
       "14126   44            Private  187584        HS-grad               9   \n",
       "\n",
       "             MaritalStatus          Occupation    Relationship  \\\n",
       "12172        Never-married   Machine-op-inspct   Not-in-family   \n",
       "7130    Married-civ-spouse        Tech-support         Husband   \n",
       "14070             Divorced               Sales   Not-in-family   \n",
       "23964   Married-civ-spouse     Exec-managerial         Husband   \n",
       "14126   Married-civ-spouse     Protective-serv         Husband   \n",
       "\n",
       "                      Race      Sex  CapitalGain  CapitalLoss  HoursPerWeek  \\\n",
       "12172   Asian-Pac-Islander     Male            0            0            40   \n",
       "7130                 White     Male            0            0            30   \n",
       "14070                White   Female            0            0            48   \n",
       "23964                White     Male            0            0            50   \n",
       "14126                White     Male            0            0            40   \n",
       "\n",
       "        NativeCountry  Salary  Label  \n",
       "12172               ?   <=50K      0  \n",
       "7130    United-States   <=50K      0  \n",
       "14070   United-States   <=50K      0  \n",
       "23964   United-States   <=50K      0  \n",
       "14126          Canada   <=50K      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = \"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "col_names = ['Age', 'Workclass', 'FNLWGT', 'Education', \n",
    "             'EducationLevel', 'MaritalStatus', 'Occupation', \n",
    "             'Relationship', 'Race', 'Sex', 'CapitalGain', 'CapitalLoss', \n",
    "             'HoursPerWeek', 'NativeCountry', 'Salary']\n",
    "\n",
    "# Read CSV data from URL return Pandas\n",
    "adult_data = pd.read_csv(data_file, index_col=False, names = col_names)\n",
    "\n",
    "# Create label column, one for >50K, zero otherwise.\n",
    "adult_data['Label'] = adult_data['Salary'].map(lambda x : 1 if '>50K' in x else 0)\n",
    "\n",
    "adult_data.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode categorical features with string value\n",
    "adult_data['Sex_code'] = LabelEncoder().fit_transform(adult_data.Sex)\n",
    "adult_data['Relationship_code'] = LabelEncoder().fit_transform(adult_data.Relationship)\n",
    "adult_data['Race_code'] = LabelEncoder().fit_transform(adult_data.Race)\n",
    "\n",
    "#pick training features\n",
    "data = adult_data[['Age', 'HoursPerWeek', 'EducationLevel', 'CapitalGain', 'CapitalLoss', 'Sex_code', 'Relationship_code', 'Race_code']]\n",
    "label = adult_data['Label']\n",
    "\n",
    "#split to training and testing\n",
    "d_train, d_test, l_train, l_test = train_test_split(data, label, test_size=0.4, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification [Adult Data] Score = 84.0%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.90      9811\n",
      "           1       0.71      0.60      0.65      3214\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     13025\n",
      "   macro avg       0.79      0.76      0.77     13025\n",
      "weighted avg       0.83      0.84      0.84     13025\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "adult_model = RandomForestClassifier(random_state=23)\n",
    "\n",
    "adult_model = adult_model.fit(d_train, l_train)\n",
    "\n",
    "# Classify test data and display score and report\n",
    "predicted = adult_model.predict(d_test)\n",
    "score = 100.0 * metrics.accuracy_score(l_test, predicted)\n",
    "print(f'Decision Tree Classification [Adult Data] Score = {score:4.1f}%\\n')\n",
    "print('Classification Report:\\n {0}\\n'.format(metrics.classification_report(l_test, predicted)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper_code import mlplots as ml\n",
    "\n",
    "# Call confusion matrix plotting routine\n",
    "ml.confusion(l_test, predicted, ['Low', 'High'], 'Random Forest Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Random Forest:  Feature Importance\n",
    "\n",
    "As the previous example demonstrated, the random forest is easy to use and often provides impressive results. In addition, by its very nature, a random forest provides an implicit measure of the importance of the individual features in generating the final predictions. While an individual decision tree provides this information, the random forest provides an aggregated result, that is generally more insightful and less sensitive to fluctuations in the training data that might bias the importance values determined by a decision tree. In the calculation of feature importance from a random forest, higher values indicate a more important feature. \n",
    "\n",
    "We demonstrate how to extract the feature importance for a random forest classifier in the following Code cell. We zip the training data column names with the model's feature_importances_ attribute, then convert it to a dataframe so that we can sort by the importance and print it out.\n",
    "\n",
    "From the feature importance we can see that Sex and Race are not very important in determining income, comparing to Age and Education Level.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.258865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EducationLevel</td>\n",
       "      <td>0.185578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Relationship_code</td>\n",
       "      <td>0.170906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HoursPerWeek</td>\n",
       "      <td>0.146041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CapitalGain</td>\n",
       "      <td>0.140547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CapitalLoss</td>\n",
       "      <td>0.053635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sex_code</td>\n",
       "      <td>0.022270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Race_code</td>\n",
       "      <td>0.022159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature  Importance\n",
       "0                Age    0.258865\n",
       "2     EducationLevel    0.185578\n",
       "6  Relationship_code    0.170906\n",
       "1       HoursPerWeek    0.146041\n",
       "3        CapitalGain    0.140547\n",
       "4        CapitalLoss    0.053635\n",
       "5           Sex_code    0.022270\n",
       "7          Race_code    0.022159"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display feature importance as computed from the random forest\n",
    "\n",
    "# Display name and importance\n",
    "feature_importance = pd.DataFrame(list(zip(d_train.columns, adult_model.feature_importances_)), columns=['Feature', 'Importance'])\n",
    "feature_importance.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with Decision Tree\n",
    "\n",
    "Random forests consist of multiple single decision trees each based on a random sample of the training data. They are typically more accurate than single decision trees.\n",
    "\n",
    "In the following code cell, we will apply a decision tree classifier on same data set created above and compare the performance with that of the random forest.\n",
    "\n",
    "We can see that random forest classifier model achieves better score, performs better in both precision and recall on both classes(low and high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification [Adult Data] Score = 82.0%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88      9811\n",
      "           1       0.66      0.56      0.61      3214\n",
      "\n",
      "   micro avg       0.82      0.82      0.82     13025\n",
      "   macro avg       0.76      0.73      0.74     13025\n",
      "weighted avg       0.81      0.82      0.82     13025\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=23)\n",
    "\n",
    "dtc = dtc.fit(d_train, l_train)\n",
    "# Classify test data and display score and report\n",
    "predicted = dtc.predict(d_test)\n",
    "score = 100.0 * metrics.accuracy_score(l_test, predicted)\n",
    "print(f'Decision Tree Classification [Adult Data] Score = {score:4.1f}%\\n')\n",
    "print('Classification Report:\\n {0}\\n'.format(metrics.classification_report(l_test, predicted)))\n",
    "ml.confusion(l_test, predicted, ['Low', 'High'], 'Random Forest Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the previous Code cell, we create random forest classifier with default hyperparamemter values. Try making the following changes and compare the different results.\n",
    "\n",
    "1. Set the values of hyperparameter `n_estimators` to 5, 10 and 15.\n",
    "2. Set pyperparameter `max_features` to different values to indicate how many features should be used in the splitting process.\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Random Forrest: Regression\n",
    "\n",
    "A random forest can also be used to perform regression; however, in this case the goal is  to create tress whose leaf nodes contain data that are nearby in the overall feature space. To make a predict a continuous value from a tree we either have leaf nodes with only one feature, and use the relevant feature from that instance as our predictor, or we compute summary statistics from the instances in the appropriate leaf node, such as the mean or mode. In the end, the random forest aggregates the indivudla tree regreession predictions into a final prediction.\n",
    "\n",
    "To perform regression with the scikit-learn library we employ the [`RandomForestRegressor`][skrfr] estimator in the tree module. This estimator employs the same set of hyperparameters as the `RandomForestClassifier` estimator, and is, therefore, used in a similar manner. One point, which was also true for classification, by specifying the `random_state` hyperparameter, we ensure reproducibility. This is because every time a tree is constructed, the features are randomly selected. Thus, even if we use the same set of hyperparameters and the same set of training data, we can end up with different trees, and thus a different forest, if the `random_state` hyperparameter is not fixed.\n",
    "\n",
    "In this section we employ a random forest to perform regression on the automotive fuel performance prediction, these data were fully described in the _Introduction to Decision Trees_ notebook. First, we will introduce these data, and prepare them for the regression task. We will employ the patsy module to use a regression formula to create our dependent and independent feature matrices. Finally, we will construct a decision tree regressor on these data and evaluate its performance.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "[skrfr]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "### Regression: Auto MPG Data\n",
    "\n",
    "Random forest can also be used to perform regression. To perform regression with the scikit-learn library we employ the RandomForestRegressor estimator in the ensemble module. This estimator employs the same set of hyperparameters as the RandomForestClassifier estimator, and is, therefore, used in a similar manner. One other point, which is also true for classification, by specifying the random_state hyperparameter, we ensure reproducibility.\n",
    "\n",
    "In this section we employ decision trees to perform regression on the [automobile fuel performance prediction data][uciap]. The data contains nine features: mpg, cylinders, displacement, horsepower, weight, acceleration, model year, origin, and car name. Of these, the first is generally treated as the dependent variable (i.e., we wish to predict the fuel efficiency of the cars), while the next seven features are generally used as the independent variables. The last feature(car name) is a string that unlikely to be useful when predicting on new, unseen data; and is, therefore, not included in our analysis.\n",
    "\n",
    "Of these features, three are discrete: cylinders, year, and origin; and four are continuous: displacement, horsepower, weight, and acceleration. A careful examination of these data indicate that horsepower has missing values, encoded as the string `?`. This causes Pandas to treat this entire column as a string, and the patsy module will, therefore, turn the column into a categorical feature. As a result, we will drop this column in our subsequent analysis; an alternative would be to drop or impute the missing values and include this column as a numerical feature.\n",
    "\n",
    "In the first two Code cells, we first load the data into a DataFrame, using our provided column names and indicating that the features are delimited by whitespace. After this, we select our independent and dependent variables. \n",
    "\n",
    "----\n",
    "[uciap]: https://archive.ics.uci.edu/ml/datasets/auto+mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Year</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>13.0</td>\n",
       "      <td>8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4735.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>chrysler new yorker brougham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>?</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>ford pinto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>29.5</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>68.00</td>\n",
       "      <td>2135.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>honda accord lx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>23.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>88.00</td>\n",
       "      <td>2957.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>peugeot 504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>34.5</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>?</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>renault 18i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MPG  Cylinders  Displacement Horsepower  Weight  Acceleration  Year  \\\n",
       "94   13.0          8         440.0      215.0  4735.0          11.0    73   \n",
       "32   25.0          4          98.0          ?  2046.0          19.0    71   \n",
       "279  29.5          4          98.0      68.00  2135.0          16.6    78   \n",
       "178  23.0          4         120.0      88.00  2957.0          17.0    75   \n",
       "354  34.5          4         100.0          ?  2320.0          15.8    81   \n",
       "\n",
       "     Origin                          Name  \n",
       "94        1  chrysler new yorker brougham  \n",
       "32        1                    ford pinto  \n",
       "279       3               honda accord lx  \n",
       "178       2                   peugeot 504  \n",
       "354       2                   renault 18i  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "# Names for our columns\n",
    "col_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Year', 'Origin', 'Name']\n",
    "\n",
    "# Create DataFrame and sample the result\n",
    "auto_data = pd.read_csv(data_file, index_col=False, names = col_names, \n",
    "                  delim_whitespace=True)\n",
    "auto_data.sample(5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = auto_data['MPG']\n",
    "x = auto_data[['Cylinders', 'Displacement', 'Weight', 'Acceleration', 'Year', 'Origin']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "With the dependent variable(y) and independent variable(x) determined, we can now build a regressive model. First, we import the `RandomForestRegressor` before splitting our independent and dependent variables into training and testing samples. Next, we create our estimator, specifying a value for our `random_state` hyperparameter to enable reproducibility. Finally, we fit the model and display a predictive score. \n",
    "\n",
    "The second Code cell computes a number of different regression performance metrics and displays the results. \n",
    "\n",
    "This model predicts the fuel performance pretty well. You may compare the regression performance metrics with that of other machine learning algorithms we introduced in previous lessons.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score = 84.7%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Split data intro training:testing data set\n",
    "ind_train, ind_test, dep_train, dep_test = train_test_split(x, y, test_size=0.4, random_state=23)\n",
    "\n",
    "# Create Regressor with default properties\n",
    "auto_model = RandomForestRegressor(random_state=23)\n",
    "\n",
    "# Fit estimator and display score\n",
    "auto_model = auto_model.fit(ind_train, dep_train)\n",
    "print('Score = {:.1%}'.format(auto_model.score(ind_test, dep_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score             = 0.847\n",
      "Mean Absolute Error   = 2.16\n",
      "Mean Squared Error    = 8.27\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Regress on test data\n",
    "pred = auto_model.predict(ind_test)\n",
    "\n",
    "# Copute performance metrics\n",
    "mae = mean_absolute_error(dep_test, pred)\n",
    "mse = mean_squared_error(dep_test, pred)\n",
    "mr2 = r2_score(dep_test, pred)\n",
    "\n",
    "# Display metrics\n",
    "print(f'R^2 Score             = {mr2:5.3f}')\n",
    "print(f'Mean Absolute Error   = {mae:4.2f}')\n",
    "print(f'Mean Squared Error    = {mse:4.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the previous Code cells, we constructed a random forest for regression and applied it to the automobile fuel performance prediction task. The initial result was pretty good, but try making the following changes to see if you can do better.\n",
    "\n",
    "1. Change the features used in the regression, for example drop one column, such as `origin`. Do the results change? \n",
    "2. Try using different hyperparameter values, such as changing the number of estimators (`n_estimators`) or the number of features (`max_features`).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "4. The scikit learn documentation provides a nice introduction to [_bagging_][1] and the estimators that implement this ensemble learning technique.\n",
    "1. A blog [article][2] on random forests in Python\n",
    "2. An article on [building random forests][3] from scratch in Python at the Machine Learning Mastery website\n",
    "2. An article on [random forests][3] at the Analytics Vidhya website\n",
    "3. A short [discussion][5] on the benefits of random forests\n",
    "4. A long [discussion][6] on random forests\n",
    "-----\n",
    "[1]: http://scikit-learn.org/stable/modules/ensemble.html#bagging\n",
    "\n",
    "[2]: http://blog.yhat.com/posts/random-forests-in-python.html\n",
    "\n",
    "[3]: https://machinelearningmastery.com/implement-random-forest-scratch-python/\n",
    "\n",
    "[4]: https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/\n",
    "\n",
    "[5]: http://fastml.com/intro-to-random-forests/\n",
    "\n",
    "[6]: http://www.cip-labs.net/2013/01/17/introduction-to-random-forests/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
