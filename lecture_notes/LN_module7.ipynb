{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7 Introduction to Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "### Slide 1\n",
    "#### Module 7 Introduction to Clustering\n",
    "- Unsupervised Learning\n",
    "- K-Means\n",
    "- DB-SCAN\n",
    "- Case Study-Credit Card Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction Script\n",
    "Hello and welcome. \n",
    "\n",
    "This module is about clustering, which is a type of unsupervised learning. \n",
    "\n",
    "Unitl now, all the machine learning algorithms we've learned are supervised learning. Supervised learning uses training data to make connections between the inputs and the outputs. \n",
    "\n",
    "Unsupervised learning, on the other hand, does not use outputs. The most common unsupervised learning method is clustering, which is used to find hidden patterns in the data and split the data into different groups.\n",
    "\n",
    "We will introduce two types of clustering algorithms, K-means and DBSCAN.\n",
    "\n",
    "One of the most popular applications of clustering is customer segmentation, which divides customers into sub-groups based on some type of shared characteristics, like purchase pattern. In this module, we will have a case study that uses k-means clustering to segment credit card users into different groups.\n",
    "\n",
    "As I mentioned before, please watch the video to learn the concepts behind the algorithms, and more importantly, go through the lesson notebooks and practice as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 1: Introduction to K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1(Just the first image in clustering)\n",
    "#### K-means\n",
    "\n",
    "1. Choose k centers randomly\n",
    "2. Form k groups by centers\n",
    "3. Recalculate centers of k groups\n",
    "4. Repeat 2-4 until no change in groups or reach max_iter\n",
    "5. Calculate total distance D\n",
    "6. Repeat 1-6 pre-defined times(n_init)\n",
    "7. Choose groups with smallest D as final clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "(face)\n",
    "\n",
    "In this lesson, we introduce the simplest clustering method, k-means. K-means tries to divide a dataset into k groups. We need to provide the value of k or number of groups to the model.\n",
    "\n",
    "Let's start with an example.\n",
    "\n",
    "In this example, the dots are the datapoints. Our goal is to divide the dataset into two different groups. By visualizing the data, we can easily identify the two groups, one group at the top right corner and one group at bottom left corner.\n",
    "\n",
    "Let's see how k-means algorithm divids this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "\n",
    "#### K-means\n",
    "\n",
    "##### Image need to change\n",
    "<img src='https://miro.medium.com/max/638/0*rrzG3LyOnAvOepbJ.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "\n",
    "(slide1)\n",
    "\n",
    "The first step in k-means algorithms is to randomly pick k data points as centers, in this case, since we are going to divide the dataset into two groups, we pick two data points randomly as initial centers, represented by the blue dot and the red dot.\n",
    "\n",
    "(slide2)\n",
    "\n",
    "We then divide the dataset to two subgroups based on the distance between each datapoint and the two centers. Now we have two subgroups, blue and red, all datapoints in subgroup blue are closer to the blue center, all datapoints in subgroup red are closer to the red center.\n",
    "\n",
    "(slide3)\n",
    "\n",
    "We then calculate the true center of each group, represented by the new blue and red dots.\n",
    "\n",
    "(slide4)\n",
    "\n",
    "With the two new centers, we again divide the dataset into two groups based on the distance of datapoints to the two new centers. \n",
    "\n",
    "(slide5)\n",
    "\n",
    "We repeat step 3 and step 4 untill the centers don't change any more. The two subgroups determined by the final certers are the final result.\n",
    "\n",
    "(slide 6)\n",
    "This is the whole process.\n",
    "\n",
    "Since we pick original centers randomly, it's possible that the final result is not optimal. To ensure the best result, we can repeat the whole process with different original centers. For each iteration, we can calculate the sum of the distance between each datapoint to its center, then choose the one with the least total distance as our final result.\n",
    "\n",
    "We will use the iris dataset to demonstrate k-means in this lesson. The iris dataset has 4 features. To make visualization easier, we use the principla component analysis to convert the iris dataset to two features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "\n",
    "#### Principal Component Analysis(PCA)\n",
    "- Reduce Dimension\n",
    "- Retain major characteristics of original features\n",
    "- Help visualize clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "Principla component analysis or PCA is an algorithm to reduce dataset features while retaining the majority of the characteristics of orignal features. PCA itself is a type of unsupervised learning.\n",
    "\n",
    "It'a common practice in clustering analysis to visualize dataset with the help of PCA. But please note that we still use the original features to do clustering. PCA is just used to help us visualize the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Sklearn KMeans\n",
    "\n",
    "- Scale features\n",
    "- No train-test split\n",
    "- Sklean KMeans Hyperparameters\n",
    "    - `n_clusters`: number of clusters, $k$\n",
    "    - `n_init`: number of times to choose different initial centers, default 10.\n",
    "    - `max_iter`: maximum number of iterations for the algorithm in any given run, default 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "\n",
    "Now let's look at some import aspects in the k-means model.\n",
    "\n",
    "First, since k-means relies on distance calculation, scaling feature is critical. \n",
    "\n",
    "Secondly, train-test split is not necessary because kmeans is an unsupervised learning. \n",
    "\n",
    "Scikit learn cluster module has kmeans model which accepts many hyperparamters. I'll explain the three most important hyperparameters.\n",
    "\n",
    "The first one is n_clusters, which is the number of clusters or the value of k; \n",
    "\n",
    "The second one is n_init, which is the number of times to choose different initial centers, the default value is 10.\n",
    "\n",
    "The thrid one is max_iter, this hyperparameter set a limit on how many times it can repeat the step 3 and 4, or recalculating centers and regroup the dataset. In k-means, after the initial centers are picked, the algorithm will repeat re-grouping and recalculating centers until there's no change in the sub groups. But sometimes, the subgroups never stop changing and the process falls into an infinite loop. To avoid this, we set max_iter. The process will end after max_iter is reached even if the subgroups still change. The default value of max_iter is 300.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "#### Sklearn KMeans\n",
    "```\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# We build our model assuming three clusters\n",
    "k_means = KMeans(n_clusters=3, n_init=25, random_state=23)\n",
    "\n",
    "# We fit our original data\n",
    "k_means.fit(x)\n",
    "\n",
    "# Obtain the predictions\n",
    "y_cluster = k_means.predict(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "The steps to apply kmeans is pretty similar to that of the supervised learning models. We first create the kmeans model, then train the model with the dataset, then call predict function to divide the dataset into subgroups.\n",
    "\n",
    "In the lesson notebook, we plot the clusters and compare it with the true clusters because for iris dataset, we have the output which is the species column. You are encouraged to understand the python code that is used to make the plot but it's not required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "#### Determine K - Elbow Method\n",
    "- Use KMeans\n",
    "- Get total distance D for each $k$\n",
    "- Plot D against k\n",
    "- Find elbow\n",
    "<img src='images/elbow.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "K-means is pretty easy to understand. But there's a challenge. How do we know the proper value of k?\n",
    "\n",
    "Sometimes, we can get this information with prior knowledge, like with the iris dataset, we know there are three different species of iris in the dataset. But when we don't have the prior information, we can find k programatically with the Elbow Method.\n",
    "\n",
    "Elbow method actually uses the k-means algorithm itself to find the best value of k. We perform k-means on the data set for a range of k values, for example from 1 to 10. In each round, we calculate the sum of the distance from each data point to its cluster center. We then plot the total distance against k values.\n",
    "\n",
    "The plot in this slide is the elbow plot on iris data set for k from 1 to 10. The total distance decreases when k increases. The extreme case is when k equals to the total number of data points in the dataset. Every data point is its own cluster center and the distance from a data point to its center is always 0, so the total distance is also 0.\n",
    "\n",
    "The best value for k is selected at the elbow point within this plot. In another word, we will choose a k so that increases in k won't reduce total distance significantly.\n",
    "\n",
    "For iris data set, as shown in the plot, the elbow appears at k equals to 3, which matches our prior knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8\n",
    "#### Explore Cluster Characteristics\n",
    "<img src='images/cluster_pairplot.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8 Script\n",
    "\n",
    "After the data set is divided to different clusters, we want to explore the characteristics of each cluster. You can calculate various statistics of the clusters, or explore the clusters visually. One of the simplest ways is to plot a pairplot on the dataset and the cluster labels.\n",
    "\n",
    "The plot in this slide is the pairplot of the iris dataset and the cluster labels assigned by the k-means model.\n",
    "\n",
    "From the last column the pairplot, the 3rd and 4th row indicate that cluster 0 has shortest petal width and petal lenght. From the second row, we can see that cluster 0 has largest sepal width.\n",
    "\n",
    "The scatter plots in the pairplot also clearly shows that cluster 0 is easily separated from other clusters. On the other hand, there's no clear separation between cluster 1 and cluster 2.\n",
    "\n",
    "The scatter plots are a good indicator of how good the clusters are, but we still need some metrics to evaluate the quality of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9\n",
    "#### Clustering Evaluation\n",
    "- Adjusted Rand Index\n",
    " - -1 to 1\n",
    " - Compare with true class\n",
    "- Silhouette\n",
    " - -1 to 1\n",
    " - Don't use true class\n",
    " - Compare distance to assigned center and next closest center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9 Script\n",
    "\n",
    "We've learned many differenct regression and classification evaluation metrics. But they're all for supervised learning and the metrics are based on the comparison of the predictions and the true outputs.\n",
    "\n",
    "Clustering is unsupervised learning where the learning process doesn't need the true outputs. When true outputs are available in the dataset, like the iris data set, we can compare the predicted clusters with the true outputs and get a score. \n",
    "\n",
    "Most of the scikit learn clustering evaluation metrics need true outputs. Adjusted Rand index is one of them. The range of adjusted rand index is from -1 to 1, 1 means the clusters match the true outputs 100%.\n",
    "\n",
    "The problem is, most clustering datasets don't have outputs at all. We need to have a metric that doesn't rely on the true outputs. There's only one such metric in the scikit learn metrics module, the Silhouette score. Instead of comparing to the true outputs, the silhouette score is calculated by comparing the total distance of all datapoints to their cluster centers with the total distance of the datapoints to their next closest cluster centers. If the distance to the cluster centers are much less than the distances to the next nearest centers, it means the clusters have good separations. The silhouette score also ranges from -1 to 1, the larger value indicates the better clustering result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 2: K-means Case Study\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Credit Card Data\n",
    "<img src='images/credit_card_dataset.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "(face)\n",
    "\n",
    "In this lesson, we will have a k-means case study on the credit card dataset. \n",
    "\n",
    "(slide)\n",
    "\n",
    "The credit card dataset is downloaded from kaggle, which is an online community of data scientists. You can find a lot of datasets, data analytics articles and source code on kaggle.\n",
    "\n",
    "The credit card dataset summarizes the usage behavior of about 9000 active credit card holders during 6 months period. The original dataset has 18 features. We will only pick 5 features in this lesson. The five features are:\n",
    "\n",
    "Our goal is to divide the credit card users to different sub groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Determine K\n",
    "<img src='images/elbow_creditcard.png' width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "\n",
    "Since we don't have an idea how many subgroups there should be in the dataset, we will the elbow plot to help us determine the value of k.\n",
    "\n",
    "Unfortunately, there's no clear elbow in the plot. This is very common in clustering analysis. Real world datasets rarely have clear cluster separations like what we saw in the iris dataset.\n",
    "\n",
    "In this case, we will pick k equals to 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Clusters Member Counts\n",
    "\n",
    "Cluster 0     : 3436 members  \n",
    "Cluster 1     : 2572 members  \n",
    "Cluster 2     : 1270 members  \n",
    "Cluster 3     : 1233 members  \n",
    "Cluster 4     :   56 members  \n",
    "Cluster 5     :   69 members  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "Once we determine the value of k, it's very simple to apply kmeans model on the dataset.\n",
    "\n",
    "This is the count of each cluster identified by the kemans model. Cluster 4 and 5 are negalectable since the counts are too small comparing to the total size of the dataset. \n",
    "\n",
    "In the lesson notebook, we explore the clusters with the help of the pairplot. The pairplot indicates that there's no clear separation between clusters. But we can still find some useful information. Please refer to the notebook for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4(Not needed)\n",
    "#### Explore Clusters with Pair Plot\n",
    "<img src='images/creditcard_pairplot.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script(Not needed)\n",
    "\n",
    "The pairplot indicates that there's no clear separation between clusters. But we can still find some useful information. For example, the first row and last column shows that most of the purchases in cluster 1 are cash-in-advance purchases, which means the users in this cluster made a lot of online purchases. These users are good marketing target of online busineses.\n",
    "\n",
    "I'm not going to spend too much time on this plot since the individual plots are too small to see in the slide. Please refer to the lesson notebook for more coding and analysis details of this case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5\n",
    "#### Visualize Data\n",
    "<img src='images/pca_creditcard.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5 Script\n",
    "\n",
    "This is a 2-dimensional scatter plot of the credit card dataset. We first need to apply pca to reduce the dataset to two features, then plot this scatter plot. From the plot we can see that there isn't any clear separation in the dataset. One possible reason could be that we don't have enough information in the dataset. You may try download the orignal credit card data from kaggle and repeat the case study to see if you can get better clustering result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 3: Introduction to Density-Based Clustering\n",
    "\n",
    "- DBSCAN\n",
    "- Scikit Learn DBSCAN Model\n",
    "- Hyperparameter Estimation\n",
    "- Kmeans vs. DBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 3 Script\n",
    "(face)\n",
    "In the previous two lessons, we introduced k-means algorithm, which is a simple and fast clustering method. The biggest challenge in k-means is that it requires knowledge of the number of clusters, or value of k ahead of time.\n",
    "\n",
    "In this lesson, we will introduce a density based clustering method, db-scan, which stands for Density-Based Spatial Clustering of Applications with Noise. To understand this confusing name, let's first take a look at some basic concpets in dbscan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### DBSCAN\n",
    "- Hyperparameters\n",
    " - `eps`: Max distance between two points to be considered as in same neighbor\n",
    " - `min_samples`: Number of points that must lie within the neighborhood of the current point in order for it to be considered a _core point_. \n",
    "\n",
    "- Point Categories\n",
    " - **Core point**: at least `min_samples` of points within distance `eps`.\n",
    " - **Border point**: not a core point, but within `eps` to at least one core point.\n",
    " - **Noise point**: Neither core point nor border point.\n",
    "\n",
    "- Clusters\n",
    " - All core points that are reachable to each other form a cluster. All border points of these core points also belong to the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "\n",
    "In k-means model, the most import hyperparameter is the value of k. DBSCAN doesn't requir value of k, but it requires the other two hyperparameters, epsilon and min_samples.\n",
    "\n",
    "epsilon is a distance value, if the distance between two data points are less than epsilon, they are considered as in the same neighbor.\n",
    "\n",
    "min_samples is an integer which is used to define a core point. \n",
    "\n",
    "a core point is a datapoint that has at least min_samples neighbors.\n",
    "\n",
    "If a datapoint is not a core point, but it's within the distance epsilon to any of the core points, it's a border point.\n",
    "\n",
    "If a datapoint is neither core point nor border point, it's a noise point.\n",
    "\n",
    "Two datapoints are reachable when their distance is less than epsilon. All core points that are reachable to each other forms a cluster. Border points belong to the cluster of their closest core points.\n",
    "\n",
    "Let's use an image to understand these concepts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2\n",
    "### DBSCAN\n",
    "<img src='https://3.bp.blogspot.com/-rDYuyg00Z0w/WXA-OQpkAfI/AAAAAAAAI_I/QshfNVNHD_wXJwXEipRIVzDSX5iOEAy2wCEwYBhgL/s1600/DBSCAN_Points.PNG'>\n",
    "<img src='https://miro.medium.com/max/2174/0*bUyZlx3rbNneiUA_'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Slide 2 Script\n",
    "\n",
    "In this image, min_samples is 4, epsilon is the radius of the circles. In the image, the red dot is a core point because it has 6 neighbors within the distance epsilon. There are 6 neighbors becaues the datapoint itself is counted as one neighbor. \n",
    "\n",
    "The blue dot is a border point because it is not a core point since it only has two neighbors, but it's reachable to a core point. The yellow dot is neither core point nor border point, so it's a noise point.\n",
    "\n",
    "(next slide)\n",
    "\n",
    "In this iamge, all the red dots are core points and they are reachable to each other. The bule dots are border points because they are reachable to at least one core point. All the reachable core points and their border points form a cluster.\n",
    "\n",
    "So in the image, these dots are in the same cluster and this dot is a noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3\n",
    "#### Sklearn DBSCAN\n",
    "```\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Apply DBSCAN\n",
    "db = DBSCAN(eps=0.575, min_samples=10)\n",
    "db.fit(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script\n",
    "\n",
    "Scikit learn cluster module defines the DBSCAN model. It's very simple to permform a DBSCAN clustering. As shown in this slide, you just need to provide epsilon and min_samples values and call fit function to train on the dataset.\n",
    "\n",
    "The problem is, how do we determine the values of epsilon and min_samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4\n",
    "#### Hyperparameter Estimation\n",
    "\n",
    "- `min_samples`\n",
    " - Greater than number of features. ie. 2 times\n",
    "- `eps`\n",
    " - small `eps` preferable\n",
    " - k-distance graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "\n",
    "There are couple of general rules in choosing the hyperparameter values. \n",
    "\n",
    "min_samples can't be too small, a rule of thumb is two times of number of features in the dataset.\n",
    "\n",
    "For epsilon, a samller value is prefered. And we can estimate the proper epsilon with the k-distance graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "#### K-distance Graph\n",
    "\n",
    "- Plot distance to nearest neighbor for all data points\n",
    "- Choose a distance at elbow that covers majority of data points\n",
    "\n",
    "<img src='images/k_distance.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "\n",
    "This is the k-distance graph. The y axis is the distance of each datapoint to its nearest neighbor. We sort the distances of all datapoints to their nearest neighbors, then make the k-distance plot. The x axis in the plot represents the sorted index of the datapoints. \n",
    "\n",
    "For example, the plot in this slide is the k-distance plot on the iris dataset. There are 150 datapoints in the iris dataset, so there are 150 points on the plot line. On the plot line, when x equals to 0, y is also about 0, it means that the distance between this datapoint to its nearest neighbor is 0; when x equals to 140, y is about 0.6, it means that the distance of this datapoint to its nearest neighbor is about 0.6, and there are 140 datapoints that are within 0.6 to their nearest neighborts.\n",
    "\n",
    "The elbow in the k-distance graph indicates the distance within which the majority data points are to their nearest neighbors. This elbow is the estimate of the epsilon value. In this case, the epsilon is between 0.5 to 0.6.\n",
    "\n",
    "You can find the python code to plot the k-distance graph in the lesson notebook. It's fairly straightforward, please try to understand it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "#### k-Means vs. DBSCAN\n",
    "\n",
    "#### k-Means works well:\n",
    "<img src='images/k_means_good.png' width=400>\n",
    "\n",
    "#### k-Means works poorly but DBSCAN works well:\n",
    "<img src='images/dbscan_good.png' width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "\n",
    "Now we've learned two clustering algorithms, k-means and dbscan. A natural question is which one we should use?\n",
    "\n",
    "While, the answer to this question total depends on the dataset.\n",
    "\n",
    "In the left image in this slide, the clusters in the dataset have spherical-like shapes, the clusters can be constructed nicely around the centers. For this kind of dataset, k-means works better.\n",
    "\n",
    "In the right image, however, the clusters in the dataset have a special shape. It's not possible to separate the two clusters with fixed centers. But dbscan can easily separate the clusters since the data points in the same cluster are reachable to each other. In this case, dbscan is better.\n",
    "\n",
    "So visualizing the dataset can help us make the choice. But to visualize a dataset, we will have to first reduce the dataset  to two features, which we can do by using principle component analysis introduced in lesson 1.\n",
    "\n",
    "Another notable issue is that dbscan performs better when the clusters have similar density, because there's just one epsilon value for all clusters.\n",
    "\n",
    "If you recall the case study on the credit card data we did in lesson 2, the scatter plot of the credit card data look like this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7\n",
    "<img src='images/pca_creditcard.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Script\n",
    "\n",
    "The density of the data points are apparently not consistant. There's also no special shapes. So even though k-means doesn't do a good job to separate the clusters, it's still considered as the better choice between the two clustering algorithms for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1\n",
    "- K-means\n",
    " - n_clusters\n",
    " - elbow method\n",
    "- DBSCAN\n",
    " - eps, min_samples\n",
    " - k-distance graph\n",
    "- Visualize dataset\n",
    " - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Review Script\n",
    "\n",
    "In this module we learned two popular unsupervised learning algorithms, k-means and dbscan.\n",
    "\n",
    "You need to understand how they work and the difference between the two algorithms.\n",
    "\n",
    "k-means forms clusters with predetermined k value of number of clusters. It works better with dataset that have spherical-like cluster shapes.\n",
    "\n",
    "The elbow plot helps to determine value of k.\n",
    "\n",
    "dbscan forms clusters by connection core points and border points. It doesn't require number of clusters in advance, but it requires epsilon which is the distance to define neighbor points, and min_samples that is used to define core points. dbscan words better when clusters in a dataset have speical shapes and have similar density.\n",
    "\n",
    "The k-distance graph helps to determine value of epsilon.\n",
    "\n",
    "A scatter plot with the help of PCA is a good way to visualize the dataset and will help to determine which algorithm to choose.\n",
    "\n",
    "Now I'd like to talk about the assignment a little bit.\n",
    "\n",
    "One problem in the assignment asks you to prepare data for an elbow plot. You don't need to write the plot code, just create data for the plot. There's complete elbow plot code in lesson 1 notebook. If you understand the code in the lesson notebook and follow the problem instruction closely, you should have no trouble to finish the problem.\n",
    "\n",
    "Other problems are fairly straightforward. Just remember to work on the problems in order.\n",
    "\n",
    "Good luck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
