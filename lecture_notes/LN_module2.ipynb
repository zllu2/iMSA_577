{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Fundamental Algorithms I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction Slide\n",
    "\n",
    "#### Module 2: Fundamental Algorithms I\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction Script\n",
    "Hello and welcome. \n",
    "\n",
    "In this module, you will learn 3 supervised machine learning algorithms. Lesson1 introduces linear regression, lesson 2 introduces logistic regression, lesson 3 is about decision tree. \n",
    "\n",
    "In the lesson notebooks, we often include some python code that create plots to help you understand some concepts. You are not required to understand those plotting code.\n",
    "\n",
    "You should, however, understand how the algorithms work in general, and more importantly, know how to apply those algorithms using python scripts. As we learned from module 1, the steps to apply machine learning models are standard for all models defined in python scikit learn module, so it's pretty easy to understand.\n",
    "\n",
    "You also need to know the key hyperparameters of each machine learning model and the value options for those key hyperparameters.\n",
    "\n",
    "For each of the 3 lessons in this module, please go through the lesson notebook briefly, then watch the lesson video with questions in mind. The lesson videos explain the concept of the machine learning algorithms. To fully understand how to apply those algorithms in python, you need to go through the lesson notebooks again after watching the video. \n",
    "\n",
    "I encourage you to run the code in the lesson notebooks, make modifications and rerun the code to see different results. Learning by doing is the best way to learn data analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 1: Introduction to Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Linear Regression\n",
    "\n",
    "- Supervised Learning\n",
    "- Predicts continuous output\n",
    "- Assumes linear relationships between input and output\n",
    "- Feature scaling is normally not needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "\n",
    "In this lesson, we will introduce linear regression.\n",
    "\n",
    "Regression is a modeling technique used to explore the relationship between independent and dependent variables. Regression is used to predict continous output, like interest of a loan, or premium of an auto insurance policy.\n",
    "\n",
    "Linear regression assumes linear relationship between independent and dependent variables. In another word, linear regression suggests that the relationship between dependent and independent variables can be expressed  by a straight line. We normally don't need to scale the continuous features in linear regression.\n",
    "\n",
    "When we use only one independent variable to predict an outcome of a dependent variable, it's called simple linear regression. If there're more than one independent variables in a linear regression model, it's called multiple linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Simple Linear Regression\n",
    "$\\hat{y} = \\beta_0 + \\beta_1  x$  \n",
    "\n",
    "$y = \\hat{y} + \\epsilon$ = $\\beta_0 + \\beta_1  x  + \\epsilon$\n",
    "\n",
    "###### Multiple Linear Regression\n",
    "$\\hat{y} = \\beta_0 + \\beta_1  x_1 + ... + \\beta_n  x_n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2\n",
    "\n",
    "##### Simple Linear Regression\n",
    "$\\hat{y} = \\beta_0 + \\beta_1  x$  \n",
    "\n",
    "$y = \\hat{y} + \\epsilon$ = $\\beta_0 + \\beta_1  x  + \\epsilon$\n",
    "\n",
    "###### Multiple Linear Regression\n",
    "$\\hat{y} = \\beta_0 + \\beta_1  x_1 + ... + \\beta_n  x_n$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "In a simply linear regression, when there's only one indepedent variable and one dependent variable, the relationship can be represented by equation $y = \\beta_0 + \\beta_1  x$ where x is the independent variable and y is the dependent variable. This is also the quation of a straight line in a two-dimensional space.\n",
    "\n",
    "\n",
    "We use y hat in the first equation because this is the predicted outcome of the dependent variable, which is usually not same as the observed value of dependent variable which is represented by y. The difference between y and y hat is called error, represented by epsilon in the formula.\n",
    "\n",
    "Our task is to find the best $\\beta_0 and \\beta_1$ so that we can minimize the overall error. This approach of minimizing errors is often used in machine learning where we define a cost function and determine the model parameters through the process of minimizing the cost function.\n",
    "\n",
    "We'll demonstrate cost function with simple linear regression in the next slide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "\n",
    "#### Cost Function\n",
    "$\\epsilon_i^2 = \\left( \\ y_i - \\hat{y}_i \\ \\right)^2$  \n",
    "$cost = \\sum \\epsilon_i^2$\n",
    "\n",
    "<img src='images/linear_regression.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "This is the plot of a simple linear regression. The blue dots are observed values. The red line is the estimated regression line. The slope of the regression line is beta1, and the intercept of the regression line and y axis is beta 0. For any given x, for example, x3, the observed output is y3, and the predicted output which is on the regression line, is y hat 3. The difference between y3 and y hat 3, epsilon 3, is the error for this data point x3.\n",
    "\n",
    "We wish to find the combination of beta 0 and beta 1 to minmize the overal the errors. So we will first define a cost function to represents the overall errors. In linear regression, the most common cost function is defined as a sum of squared errors. This kind of linear regression is also called ordinary least square regression or OLS. The square term is necessary to remove any negative errors like epsilon 10 in the image. It also gives more weight to larger errors which usually results a better fit. But it's not always true, if a dataset has some extreme outlies, the outliers may have big impact on the linear regression due to the square term. You may want to deal with the outliers in the data before applying OLS linear regression.\n",
    "\n",
    "Ok, enough of theory. Python scikit learn module defines a linear regression model, which only requires several lines of code to construct a linear regression. I'll demonstate this with iris dataset. Let's take a look at the dataset first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Data\n",
    "<img src='images/tips_dataset.png' width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "This is the tips dataset, which is built-in in the seaborn module. The tips dataset records the data of people visiting a resturant. \n",
    "\n",
    "The columns are pretty self-explanatory. Our goal is to build a linear regression model to predict amount of tips. So the dependent variables is tip in the dataset, and other varaibles are independent variables which can be used to predict tip. \n",
    "\n",
    "To construct a linear regression model, we need to preprocess the data first. We will separate dependent and independent variables, we will also encode the categorical features. But we don't need to scale the features. we will discuss why we don't need to scale features in linear regression in a few moments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "#### Data Preparation with patsy\n",
    "```\n",
    "import patsy as pts \n",
    "y, x = pts.dmatrices('tip ~ total_bill + size + C(time)', data=tdf, return_type='dataframe')\n",
    "```\n",
    "\n",
    "#### Independent Variables(x)\n",
    "<img src='images/tips_data_encode.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "Python patsy module defines a function dmatrices which makes data pre-processing very simple when we don't need to scale the features. We simply define a formula of our linear regression as a string and pass it to the dmatrices function. The function returns dependent variable and encoded independent variables directly.\n",
    "\n",
    "The first item in the formula string is the column name of the dependent variable, in this case it's 'tip'. Then we use telda to separate dependent variable from independent variables, you can imagine it as a equal sign in a linear regression equation. The column names of indepenpent variables are connected by + sign. Notice that there's a capical C and parenthesis around time column. This indicates that time is a categorical feature and need encoding. dmatrices will encode all columns wrapped by capital C. Here we only choose three features as independent variable, two continuous and one categorical. \n",
    "\n",
    "The table below displays some random rows of the resulting independent variables. In the table, instead of time, there's C(time)(T.Dinner] column. dematrices encodes categorical features similar to one-hot encoding, but with one less dummy feature. For example, there are two unique values in time feature, dinner and lunch. With one-hot encoding, two dummy features will be created, one for dinner one for lunch. But actually one dummy is good enough, since a 0 in dinner column indicates it's lunch.\n",
    "\n",
    "In the table, there's an extra feature created, intercept, with value 1 for all data points. This can be used to estimate the intercept of the linear regression model.\n",
    "\n",
    "Now with the dependent variable and the pre-processed independent variable, we can apply linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 6\n",
    "#### Scikit Learn LinearRegression model\n",
    "```\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ind_train, ind_test, dep_train, dep_test = \\\n",
    "    train_test_split(x, y, test_size=0.4, random_state=23)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(ind_train, dep_train)\n",
    "\n",
    "score = model.score(ind_test, dep_test)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "\n",
    "We first split the data to train and test. The independent variable x is splitted into independent train and independent test, and the dependet variable y is splitted to dependent train and dependent test.\n",
    "\n",
    "Then we create a linear regression model object and fit the model with the independent and depdendent train set. Then we call the score function in the model to evaluate the model with the test set. The score function will first make prediction on the test data set, then compare the prediction with observed output of test set to calculate an accuracy score.\n",
    "\n",
    "This is the standard way to apply all machine learning models defined in the scikit learn module.\n",
    "\n",
    "Now let's explore the parameters or the value of betas in this fitted linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7\n",
    "\n",
    "#### Linear Regression Formula\n",
    "```\n",
    "# Display model fit parameters for training data\n",
    "print(f\"tip = {model.intercept_[0]:4.2f} + \" + \\\n",
    "      f\"{model.coef_[0][1]:4.2f} Dinner + \" + \\\n",
    "      f\"{model.coef_[0][2]:4.2f} total_bill + \" + \n",
    "      f\"{model.coef_[0][3]:4.2f} size\")\n",
    "```\n",
    "\n",
    "`tip = 0.83 + -0.23 Dinner + 0.09 total_bill + 0.22 size`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Script\n",
    "This piece of code contructs the equation of the fitted linear model. We will use intercept_ attribute in the model to get the intercept, or beta 0, and attribute coef_ to get values of other betas, or the coeffecients of all indpendent variables.\n",
    "\n",
    "We mentioned above that we don't need to scale features for linear regression. This is because the scale of features can be adjusted by the coefficients. For example, if we convert the unit of total_bill column to cent from dollar, and still keep dollar as unit in tip column. We will get same linear model except that the coefficient of total_bill becomes 0.0009 to accomodate the unit change in total_bill.\n",
    "\n",
    "The equation indicates that larger total bill and size of the group result more tip, which is understandable. But tip from dinner is less than that from lunch, which is kind of counter intuitive. Can we trust these results?\n",
    "\n",
    "Before we discuss the reliablity of the regression results, let's look at another way to do linear regression in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 8\n",
    "#### Statsmodels Linear Regression\n",
    "```\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "result = smf.ols(formula='tip ~ total_bill + size + C(time)', data=tdf).fit()\n",
    "result.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the machine learning models we introduce in this course are defined in the scikit learn module. But for linear regression, there's an ols implementation in the statsmodels module, which in my opinion, is more convenient to use than the scikit learn linear regression model.\n",
    "\n",
    "Only one line of code is needed to construct an ols linear regression with statsmodels module. We don't need to separate dependent and independent variable or encode categorical features. We simply pass a string formula as we did above when we prepare data with patsy module.\n",
    "\n",
    "We can print the linear regression information nicely with the summary function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9\n",
    "#### Linear Regression Result\n",
    "<img src='images/lin_reg_result.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9 Script\n",
    "This is the result. We can ingore the bottom part of the result. The top part shows the general information of this linear model. For examle, the dependent variable is tip, and the model use OLS method. The R-squared score is 0.468, it's a score we can use to evaluate the accuracy of the model. R-squared is normally between 0 and 1, larger r-squared indicates a better model. We will discuss evaluation metrics in more details in the following lessons.\n",
    "\n",
    "The middle part the result shows the model parameters. It not only shows the coefficient values, but also shows some statistics of the coefficients. For example, t statistics. If the absolute value of a coefficient t statistics is greater than 2, we can say that the value is significant at 95% confidence level. Notice that the t statistics of the coefficient of C time Dinner is only -0.028, which means the value is not significant. In another word, time is a good indicator in determining tips.\n",
    "\n",
    "Notice that the coefficient returned by statsmodels are different to that returned by the scikit learn linear regression model. The reason is that we don't split the dataset and use the whole dataset when we use statsmodels linear model. The r-squared score is also calculated with the whole dataset, which makes it less reliable. Since we usually want to evaluate a model with data that's unseen by the model. Despite of this, statsmodels is still a good choice for linear regression just because it's so easy to use. The scikit learn linear regression model doesn't return coefficient statistics directly. You will have to calculate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 2: Introduction to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Logistic Regression\n",
    "- Supervised learning\n",
    "- For classification problems\n",
    "- Feature scaling is normally not needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "\n",
    "In this lesson we will learn logistic regression which is a supervised learning. Despite its name, logistic regression is for classification problems, which is used to predict discrete output, like true or false, yes or no. We learned in the first lesson that a regression is normally used to predict continous value, but if we apply some function to map continous value into a probability with a range from 0 to 1, we can then apply a threshold on the probability to predict binary output. We will use an example to explain this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Challenger Disaster Investigation\n",
    "<img src='images/oring.png' width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "In 1986, after the space shuttle challenger disaster, the investigation commission determined that the disaster is cuased by the failure of an O-ring seal in the solid rocket motor due to the cold temperature. The figure in this slide is a report of o-ring test which was actually done before challenger launch. The x axis is temperature, the y axis is the number of o-ring failures. Green dots in the figure indicate no failure, while red dots indicate at least one o-ring failure. We can create a dataset with this figure with two variables, the independent varialbe is temperature, and the dependent variable is whether there's oring failure, 0 if there's no failure and 1 if there's at least one failure.\n",
    "\n",
    "Through a logistic function transformation, we can fit the logistic regression model with this data and get following result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3\n",
    "#### Map to Probability\n",
    "<img src='images/sigmoid.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dots in the figure are observed data. For any give independent variable or temperature in this case, the output is either 0 or 1. When we apply a logistic transformation, we convert a linear model into a logistic model which can map temperature to oring failure probability. The relationship is represented by the s shaped curve in the figure which is called a sigmoid curve. The shape and location of thesigmoid curve is determined by the model paramters, or betas, just like in linear regression. And like in linear regression, we get the best betas by minizing the overall errors of the predictions.\n",
    "\n",
    "The figure shows the best fit based on the oring test data, we can see that based on this logistic regression model, the oring failure probability at 65 degree Fahrenheit is about 50%, and the probability is close to 100% when temperate is below 50 degree. And guess what's the temperature at challenger launch time? It is 31 degree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Adult Income Dataset\n",
    "\n",
    "<img src='images/adult_dataset.png' width=800>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "\n",
    "Now let's learn how to create a logistic regression model with python. We will use adult income data to demonstate this. The adult income data is extracted from the 1994 Census database. The dataset contains employee information like education, race, sex, weekly work hour etc. The Salary column, which has two values greater than 50 thousand or less or equal to 50 thousand. This column will be our label which we are trying to predict with employee infomation.\n",
    "\n",
    "We first need to pre-process the data, which includes encoding the label, map greater than 50 thousand to 1 and otherwise 0, encoding categorical features, and splitting the dataset to train and test.\n",
    "\n",
    "After that, we apply the scikit learn logistic model same way as we did with linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5\n",
    "#### Scikit Learn LogisticRegression model\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "adult_model = LogisticRegression(C=1E6)\n",
    "adult_model = adult_model.fit(x_train, y_train)\n",
    "\n",
    "predicted = adult_model.predict(x_test)\n",
    "score = metrics.accuracy_score(y_test, predicted)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "This is the code we use to apply scikit learn logistic regression model. There are couple of things worth mentioning. One is that we set a hyperparmater C when constructing logisticRegression model. This hyperparameter is used to redcue the risk of overfitting. We will discuss how to choose proper value for C in future lessons. Another thing is we call predict function of the model to get prediction on test set, then use accuracy_score function in scikit learn metrics module to calculate accuracy score of our model. In the previous lesson, for the linear regression model, we directly call the score function of the model to calculate the r-squared score. We can also do the same thing for logistic regression model by calling the score function of the model to get accuracy score. The score function combines predicting and calculating accuracy score into one function. To separate the two steps, allows us to calculate other evaluation metrics, which we will discuss briefly in the next slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 6\n",
    "### Classification Metrics\n",
    "#### Classification Report\n",
    "```\n",
    "metrics.classification_report(y_test, predicted)\n",
    "```\n",
    "\n",
    "<img src='images/classification_report.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "The accuracy score is a common classification evaluation metric. It indicates the overall proportion of correct predictions. But accuracy score is often not able to reflect the quality of a classification model. Especially when the dataset is imbalanced. For example, if a dataset has two outcomes, positive and negative, and 99% of the dataset has negative outcome and only 1% has positive outcome. A zero model, which predicts all data with majority outcome, which is negative in this case, will have accuracy score at 99%. But zero model is esentially useless since it's not able to identify any positive outcome.\n",
    "\n",
    "So in addition to accuracy score, we also use some other metrics to evaluate a classification model. Scikit learn metrics module has classfication_report function which returns a classification report with more comprehensive metrics.\n",
    "\n",
    "The figure in this slide is the classification report for our logistic regression model fitted on the adult dataset. The first line is the scores on predicting low income, or income below 50 thousand dollars and the second line is the score on predicting high income or incomes above 50 thousand dollars.\n",
    "\n",
    "Precision is the proportion of predictions that is correct. For example, the first line shows that among all low income predictions, 80% of them are correct.\n",
    "\n",
    "Recall rate is the proportion of an actual class that are predicted correctly. For example, the second line in recall column indicates that our model only identifies 25% of all high income cases correctly.\n",
    "\n",
    "We can also plot a confusion matrix to show actual number of correct and wrong predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 7\n",
    "### Classification Metrics\n",
    "#### Confusion Matrix\n",
    "<img src='images/confusion_matrix.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the confusion matrix. The lesson 2 notebook has the code to plot this matrix. We will also use this code to plot confusion matrix in future lessons. You don't have to understand the plotting code, but you need to know how to use the function to plot a confusion matrix.\n",
    "\n",
    "In this matrix, the rows indicate observed outcomes or number of actual classes, the columns indicate the predicted outcomes. The first row indicates that there are total 1182 plus 29 low income cases, and 1182 of them are predicted correctly as low income by our model. The second row shows that there are 307 plus 82 high income cases, among which only 82 are classified correctly by our model. \n",
    "\n",
    "We can calcualte precision and recall with confusion matrix. For example, for high income recall, it's 82 divided by all high income cases which is 82 plus 307, the result is 25% as shown in the classfication report.\n",
    "\n",
    "We will discuss the evaluation matrix in more detail in future lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 3: Introduction to Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Decision Tree\n",
    "- Supervised learning\n",
    "- For both classification and regression problems\n",
    "- Can handle categorical feature(numerical)\n",
    "- Feature scaling is not needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "In this lesson we will discuss decision tree, which is a simple algorithm that is easy to understand. Decision tree is a tree-like decsion making model. It can be used in both classification and regression problems. \n",
    "\n",
    "For linear regression and logistic regression, we normally want to create dummy variables for categorical features because we don't want to introduce artificial ranking information into the dataset. Decesion tree, on the other hand, can handle categorical features directly as long as they have numeric values. For text categorical values, we can simply apply label encoding to map them to numbers.\n",
    "\n",
    "Just like in linear or logistic regression, we normally don't need to scale continous features with decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Decition Tree\n",
    "<img src=\"images/dt_sunglasses.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "This is a simply decision tree. Decision tree use conditional statements to split the dataset into different groups. In this image, the blue boxes with conditions are decision nodes, the top most node is the root node. The tree splitted into branches based on the conditions. The end of branches represented by green ovals that don't split any more are called leaves, or decision nodes. \n",
    "\n",
    "The decision tree in this slide determines whether you should wear sunglasses based on time and location. The root node has a condition on time, if it's night, directly go to decision no sunglasses. If it's day, check location feature and eventually make decision based on location.\n",
    "\n",
    "In this decision tree, the two features are all categorical features. If there're continuous features, the conditions will be comparisons, for example, check if temperature is greater than 90 degree.\n",
    "\n",
    "We mentioned above that decision tree can be used for both classification and regression problems. In this lesson, we will use iris dataset to demonstrate how to apply decsion tree classfier with scikit learn decision tree module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3\n",
    "#### Iris Dataset\n",
    "<img src='images/iris_dataset.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script\n",
    "\n",
    "As tips dataset we used in lesson one, iris dataset is also a built-in dataset in seaborn module. The data set consists of 50 samples from each of three species of Iris, Iris setosa, Iris virginica and Iris versicolor. Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
    "\n",
    "The species column will be our label and other four columns will be the features. All 4 features are continuous features. For a decision tree model, we don't need to scale continuous features. We do need to encode the label, or the species column because they have text values.\n",
    "\n",
    "After we encode the species column and split the dataset to train and test, we can use the standard way to apply scikit learn decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Decision Tree Classifier\n",
    "```\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=23)\n",
    "dtc = dtc.fit(d_train, l_train)\n",
    "score = dtc.score(d_test, l_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "By now you should be very familiar with this piece of code. It is very simple to apply scikit learn machine learning models. That's why modeling doesn't really take too much time, most of your time and effort will be put in data understanding and data preparation.\n",
    "\n",
    "Here we use score function of the model directly to get accuracy score. If you recall what we did in previous lesson, we first get predictions with predict function, then compare the predictions with true label to get accuracy score. Both ways do the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "<img src='images/dt_image.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "A big advantage of decision tree is its interpretability. We can plot the tree from the scikit learn decision tree classifier. This is the decision tree that's created on the iris dataset.\n",
    "\n",
    "The first line in the decision nodes is the condition. For example, in the root node, the condition is petal width less than or equal to 0.8.\n",
    "\n",
    "samples is the total number of data points in the node. In the root node, there're 90 samples which is total count of the training dataset. \n",
    "\n",
    "value is a list which shows the count of each class in the node. There are three different species of iris in the dataset, so value is a list of three numbers. The root node indicates that there are 29 setosa, 32 vercicolor and 29 verginica iris in the training dataset.  \n",
    "\n",
    "gini is a measurement of the homogeneity of the node. A smaller gini indicates higher homogeneity. For example, in the first decision node at 2nd row, the gini is zero, which means all of the 29 samples in this node are same species, or setosa. We reach this node with the condition petal width less than or equal to 0.8. This means with just one condition, we can already identify all setosa iris.\n",
    "\n",
    "A decision tree can have many layers and eventually reach to a point that all leaves have 0 gini, or all data points in a leaf node have same class. The tree we show in this slide has some impure leaves, because we limit the depth of the tree to make sure the tree is not too big to fit in a slide.\n",
    "\n",
    "We can feed new iris data into this tree and we will reach to one of the leaf nodes base on the features. The majority class in the leaf node will be the classification of the new data.\n",
    "\n",
    "For a regression problem, the mean of all outputs in the leaf nodes will be the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "#### Auto MPG Dataset\n",
    "<img src='images/mpg_dataset.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side 6 Script\n",
    "For decision tree regression, we will use another seaborn built-in dataset, the auto mpg dataset. The dependent variable is mpg, or mile per gallon. We'd like to predict a vehicle's fuel efficiency based on its features like cylinders, horsepower, model year etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 7\n",
    "#### Decision Tree Regressor\n",
    "```\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "auto_model = DecisionTreeRegressor(random_state=23)\n",
    "auto_model = auto_model.fit(ind_train, dep_train)\n",
    "score = auto_model.score(ind_test, dep_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Script\n",
    "\n",
    "The preprocessing is similar to what we did in the desicion tree classification. To apply the scikit learn decision tree regressor, we again, just need these several lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "### Slide 1\n",
    "#### Module 2 Review\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "\n",
    "We learned three machine learning algorithms in this module. Linear regression is for regression problems, logistic regression is used for classification problems despite of its name. Decision tree can be use on both classification and regression problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2\n",
    "#### Module 2 Review\n",
    "- Continuous Features\n",
    " - Scaling not needed\n",
    "- Categorical Features\n",
    " - Create dummy variables\n",
    "   - Linear Regression\n",
    "   - Logistic Regression\n",
    " - Label encoding\n",
    "   - Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "For continuous features, all three algorithms don't require feature scaling. \n",
    "\n",
    "For categorical features, creating dummy variables is normally prefered for linear regression and logistic regression. Decision tree, on the other hand, can deal with categorical features directly as long as they have numeric values. For categories with string value, label encoding is good enough for a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script without slide\n",
    "The first module's assignment is fairly straightforward. Just remember to work on the problems in order.\n",
    "\n",
    "Good luck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
