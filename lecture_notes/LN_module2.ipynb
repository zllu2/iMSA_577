{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Fundamental Algorithms I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction Slide\n",
    "\n",
    "#### Module 2: Fundamental Algorithms I\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction Script\n",
    "Hello and welcome. \n",
    "\n",
    "This module introduces three supervised machine learning algorithms. \n",
    "\n",
    "In lesson1, you will learn linear regression, which is one of the most well known and well understood algorithms. Linear regression is a linear approach to modeling the relationship between a dependent variable and one or more  independent variables.\n",
    "\n",
    "In lesson 2, you will learn logistic regression. Despite its name, logistic regression is a classification algorithm.\n",
    "\n",
    "In lesson 3, you will learn the decision tree algorithsm which can be used on both classification and regression problems.\n",
    "\n",
    "In the lesson notebooks, we often include some python code that create plots to help you understand some concepts. You are not required to understand those plotting code.\n",
    "\n",
    "You should, however, understand how the algorithms work in general, and more importantly, know how to apply those algorithms using python scripts. As we learned from module 1, the steps to apply machine learning models are standard for all models defined in the python scikit learn module, so it's pretty easy to understand.\n",
    "\n",
    "You also need to know the key hyperparameters of each machine learning model and the value options for those key hyperparameters.\n",
    "\n",
    "We introduce some basic evaluation metrics for regression and classfication in this module. You need to understand the concepts of these metrics and how to get these metrics with python code.\n",
    "\n",
    "For each of the 3 lessons in this module, please go through the lesson notebook briefly, then watch the lesson video with questions in mind. The lesson videos explain the concept of the machine learning algorithms. To fully understand how to apply those algorithms in python, you need to go through the lesson notebooks again after watching the video. This time, play with the code, make modifications and rerun the code to see different results. Remember that learning by doing is the best way to learn data analytics with python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 1: Introduction to Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Linear Regression\n",
    "\n",
    "- Supervised Learning\n",
    "- Predicts continuous output\n",
    "- Assumes linear relationships between input and output\n",
    "- Feature scaling is normally not needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "\n",
    "In this lesson, we will introduce linear regression.\n",
    "\n",
    "Regression is a supervised modeling technique that is used to explore the relationship between one dependent variable and one or more independent variables.\n",
    "\n",
    "Linear regression assumes linear relationship between the dependent and the independent variables. In another word, linear regression suggests that the relationship between the dependent and independent variables can be expressed  by a straight line. \n",
    "\n",
    "We normally don't need to scale the continuous features in linear regression.\n",
    "\n",
    "When we use only one independent variable to predict an outcome of a dependent variable, it's called simple linear regression. If there're more than one independent variables in a linear regression model, it's called multiple linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Simple Linear Regression\n",
    "$\\hat{y} = \\beta_0 + \\beta_1  x$  \n",
    "\n",
    "$y = \\hat{y} + \\epsilon$ = $\\beta_0 + \\beta_1  x  + \\epsilon$\n",
    "\n",
    "###### Multiple Linear Regression\n",
    "$\\hat{y} = \\beta_0 + \\beta_1  x_1 + ... + \\beta_n  x_n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2\n",
    "\n",
    "##### Simple Linear Regression\n",
    "$\\hat{y} = \\beta_0 + \\beta_1  x$  \n",
    "\n",
    "$y = \\hat{y} + \\epsilon$ = $\\beta_0 + \\beta_1  x  + \\epsilon$\n",
    "\n",
    "###### Multiple Linear Regression\n",
    "$\\hat{y} = \\beta_0 + \\beta_1  x_1 + ... + \\beta_n  x_n$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "In a simply linear regression, when there's only one indepedent variable and one dependent variable, the relationship can be represented by equation y hat equals to beta 0 + beta 1 times x, where x is the independent variable and y hat is the dependent variable. This is also the quation of a straight line in a two-dimensional space.\n",
    "\n",
    "The y hat in the first equation is the predicted outcome of the dependent variable, which is usually not same as the observed outcome. The difference between the observed value and the predicted value is called error. \n",
    "\n",
    "In the second equation, y is the observed outcome, which equals to the predicted outcome plus the error, represented by epsilon.\n",
    "\n",
    "Our task is to find the best $\\beta_0 and \\beta_1$ so that we can minimize the overall error. This approach of minimizing errors is often used in machine learning where we define a cost function and determine the model parameters through the process of minimizing the cost function.\n",
    "\n",
    "We'll demonstrate cost function with simple linear regression in the next slide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "\n",
    "#### Cost Function\n",
    "$\\epsilon_i^2 = \\left( \\ y_i - \\hat{y}_i \\ \\right)^2$  \n",
    "$cost = \\sum \\epsilon_i^2$\n",
    "\n",
    "<img src='images/linear_regression.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "This is the plot of a simple linear regression. The blue dots are observed values. The red line is the estimated regression line. The slope of the regression line is beta1, and the intercept of the regression line and the y axis is beta 0. For any given x, for example, x3, the observed outcome is y3, and the predicted outcome which is on the regression line, is y hat 3. The difference between y3 and y hat 3, epsilon 3, is the error for this data point x3.\n",
    "\n",
    "We will first define a cost function to represent the overall errors. In linear regression, the most common cost function is defined as a sum of squared errors. This kind of linear regression is also called ordinary least square regression or OLS. The square term is necessary to remove any negative errors like epsilon 10 in the image. It also gives more weight to larger errors which usually results a better fit.\n",
    "\n",
    "The linear regression model will find beta 0 and beta1 in the process of minizing the cost function.\n",
    "\n",
    "Python scikit learn module defines a linear regression model, which only requires several lines of code. I'll demonstate this with the tips dataset. Let's take a look at the dataset first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Data\n",
    "<img src='images/tips_dataset.png' width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "This is the tips dataset, which is built-in in the seaborn module. The tips dataset records the data of people visiting a resturant. \n",
    "\n",
    "There are 7 columns in the dataset. Among them, total_bill, tip and size have continous values. Sex, smoker, day and time have categorical values. For example, time column has two categories, either lunch or dinner. Our goal is to build a linear regression model to predict amount of tips. So the dependent variables is the tip column in the dataset, and other varaibles are independent variables which can be used to predict tip. \n",
    "\n",
    "To construct a linear regression model, we need to preprocess the data first. We will separate dependent and independent variables, we will also encode the categorical features. But we don't need to scale the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "#### Data Preparation with patsy\n",
    "```\n",
    "import patsy as pts \n",
    "y, x = pts.dmatrices('tip ~ total_bill + size + C(time)', data=tdf, return_type='dataframe')\n",
    "```\n",
    "\n",
    "#### Independent Variables(x)\n",
    "<img src='images/tips_data_encode.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "Python patsy module defines a function dmatrices which makes categorical encoding very simple. We just need define a formula of the linear regression as a string and pass it to the dmatrices function. The function returns dependent variable and encoded independent variables. Here we save dependent variable to y and independent variables to x.\n",
    "\n",
    "The first item in the formula string is the column name of the dependent variable, in this case it's 'tip'. Then we use telda to separate the dependent variable from the independent variables, you can imagine it as a equal sign in a linear regression equation. The column names of indepenpent variables are connected by + signs. Notice that there's a capical C around time column. This indicates that time is a categorical feature and needs encoding. dmatrices will encode all columns wrapped by capital C. Here we only choose three features as independent variable, two continuous and one categorical. \n",
    "\n",
    "The table below displays some random rows of the resulting independent variables. In the table, instead of time, there's C(time)(T.Dinner] column. dematrices encodes categorical features similar to one-hot encoding, but with one less dummy feature. For example, there are two unique values in time feature, dinner and lunch. With one-hot encoding, two dummy features will be created, one for dinner one for lunch. But actually one dummy feature is enough, since a 0 in c time t dinner column indicates it's lunch.\n",
    "\n",
    "In the table, there's an extra feature created, intercept, with constant value 1 for all data points. This can be used to estimate the intercept of the linear regression model.\n",
    "\n",
    "Now with the dependent variable and the independent variables, we can apply linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 6\n",
    "#### Scikit Learn LinearRegression model\n",
    "```\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ind_train, ind_test, dep_train, dep_test = \\\n",
    "    train_test_split(x, y, test_size=0.4, random_state=23)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(ind_train, dep_train)\n",
    "\n",
    "score = model.score(ind_test, dep_test)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "\n",
    "We first split the data to train and test. The independent variable x is splitted into independent train and independent test, and the dependet variable y is splitted to dependent train and dependent test.\n",
    "\n",
    "Then we create a linear regression model object and fit the model with the independent and depdendent train set. Then we call the score function in the model to evaluate the model with the test set. The score function will first make prediction on the test data set, then compare the prediction with observed outcome which is dependent test to calculate an accuracy score.\n",
    "\n",
    "This is the standard way to apply all machine learning models defined in the scikit learn module.\n",
    "\n",
    "Now let's explore the value of betas in this linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7\n",
    "\n",
    "#### Linear Regression Formula\n",
    "```\n",
    "# Display model fit parameters for training data\n",
    "print(f\"tip = {model.intercept_[0]:4.2f} + \" + \\\n",
    "      f\"{model.coef_[0][1]:4.2f} Dinner + \" + \\\n",
    "      f\"{model.coef_[0][2]:4.2f} total_bill + \" + \n",
    "      f\"{model.coef_[0][3]:4.2f} size\")\n",
    "```\n",
    "\n",
    "`tip = 0.83 + -0.23 Dinner + 0.09 total_bill + 0.22 size`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Script\n",
    "\n",
    "We can get the intercept or beta 0 from the intercept attribute in the linear regression model. and we'll get the three beta values for the three independent variables from the coef attribute of the model. \n",
    "\n",
    "This equation shows the relationship between tip and total bill, size and time.\n",
    "\n",
    "Total bill and size have positive coefficient, which indicates that larger total bill and larger group of customers result more tip, which is understandable. But the coefficient of time dinner is negative, which indicatas that tip from dinner is less than that from lunch, which is kind of counter intuitive. Can we trust these results?\n",
    "\n",
    "Before we discuss the reliablity of the regression results, let's look at another way to do linear regression in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 8\n",
    "#### Statsmodels Linear Regression\n",
    "```\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "result = smf.ols(formula='tip ~ total_bill + size + C(time)', data=tdf).fit()\n",
    "result.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the machine learning models we introduce in this course are defined in the scikit learn module. But for linear regression, there's an ols implementation in the statsmodels module, which in my opinion, is more convenient to use than the scikit learn linear regression model.\n",
    "\n",
    "Only one line of code is needed to construct an ols linear regression with statsmodels. We simply pass a formula string to the ols function and call fit function to train the ols model.\n",
    "\n",
    "We can then print the linear regression information nicely with the summary function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9\n",
    "#### Linear Regression Result\n",
    "<img src='images/lin_reg_result.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9 Script\n",
    "This is the result of the ols linear regression. We can ingore the bottom part of the result. The top part shows the general information of this linear model. For examle, the dependent variable is tip, and the model uses OLS method. The R-squared score is 0.468, it's a score we can use to evaluate the accuracy of the model. R-squared is normally between 0 and 1, larger r-squared indicates a better model. We will discuss evaluation metrics in more details in the following modules.\n",
    "\n",
    "The middle part of the result shows the model parameters. It not only shows the coefficient values, but also shows some statistics of the coefficients. For example, t statistics. If the absolute value of the t statistics is greater than 2, we can say that the value is significant at 95% confidence level. Notice that the t statistics of the coefficient of C time Dinner is only -0.028, which means the value is not significant. In another word, time is not a good indicator in determining tips.\n",
    "\n",
    "Notice that the coefficient returned by statsmodels are different to that returned by the scikit learn linear regression model. The reason is that we don't split the dataset and use the whole dataset to train the model when we use statsmodels linear model. The r-squared score is also calculated with the whole dataset, which makes it less reliable. Since we usually want to evaluate a model with data that's unseen by the model. Despite of this, statsmodels is still a good choice for linear regression just because it's so easy to use. The scikit learn linear regression model doesn't return coefficient statistics directly. You will have to calculate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 2: Introduction to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Logistic Regression\n",
    "- Supervised learning\n",
    "- For classification problems\n",
    "- Feature scaling is normally not needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "\n",
    "In this lesson we will learn logistic regression which is a supervised learning. Despite its name, logistic regression is for classification problems, which is used to predict discrete output, like true or false, yes or no. As with linear regression, feature scaling is not needed for logistic regression.\n",
    "\n",
    "We learned in the first lesson that a regression is normally used to predict continous value, but if we apply some function to map continous value into a probability , we can then apply a threshold on the probability to predict binary output. We will use an example to explain this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Challenger Disaster Investigation\n",
    "<img src='images/oring.png' width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "In 1986, after the space shuttle challenger disaster, the investigation commission determined that the disaster is cuased by the failure of an O-ring seal in the solid rocket motor due to the cold temperature. The figure in this slide is a report of o-ring test which was actually done before challenger launch. The x axis is temperature, the y axis is the number of o-ring failures. Blue dots in the figure indicate no failure, while red dots indicate at least one o-ring failure. We can create a dataset with this figure with two variables, the independent varialbe is the temperature, and the dependent variable is whether there's oring failure, 0 if there's no failure and 1 if there's at least one failure.\n",
    "\n",
    "Through a logistic function transformation, we can fit the logistic regression model with this data and get following result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3\n",
    "#### Map to Probability\n",
    "<img src='images/sigmoid.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dots in the figure are observed outcome. For any give independent variable or temperature in this case, the output is either 0 or 1. When we apply a logistic transformation, we convert a linear model into a logistic model which can map temperature to oring failure probability. The relationship is represented by the s shaped curve in the figure which is called a sigmoid curve. \n",
    "\n",
    "The figure shows the best fit based on the oring test data, we can see that based on this logistic regression model, the oring failure probability at 65 degree Fahrenheit is about 50%, and the probability is close to 100% when temperate is below 50 degree. And guess what's the temperature at challenger launch time? It is only 31 degree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Adult Income Dataset\n",
    "\n",
    "<img src='images/adult_dataset.png' width=800>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "\n",
    "Now let's see how to create a logistic regression model with python. We will use adult income data to demonstate this. The adult income data is extracted from the 1994 Census database. The dataset contains employee information like education, race, sex, weekly work hour etc. The Salary column, which has two values, either greater than 50 thousand or less or equal to 50 thousand. This column will be our label which we are trying to predict from the employee infomation.\n",
    "\n",
    "We first need to pre-process the data, which includes encoding the label. We will map greater than 50 thousand to 1 and less or equal to 50thousand to 0. So 1 indicates high income and 0 indicates low income. We then encode other categorical features, and split the dataset to train and test.\n",
    "\n",
    "After that, we apply the scikit learn logistic model same way as we did with linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5\n",
    "#### Scikit Learn LogisticRegression model\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "adult_model = LogisticRegression(C=1E6)\n",
    "adult_model = adult_model.fit(x_train, y_train)\n",
    "\n",
    "predicted = adult_model.predict(x_test)\n",
    "score = metrics.accuracy_score(y_test, predicted)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "This is the code we use to apply scikit learn logistic regression model. There are couple of things worth mentioning. \n",
    "\n",
    "One is that we set a hyperparmater C when constructing logisticRegression model. This hyperparameter is used to redcue the risk of overfitting. We will discuss how to choose proper value for C in the future lessons. \n",
    "\n",
    "The other one is, in the lesson one, we call the linear regresion model's score function to get the model's accuracy score.\n",
    "Here we use the model's predict function to get prediction on test dataset first. Then with the accuracy_score function in the scikit learn metrics module, we compare the prediction with the true outcome of the test dataset to get the accuracry score of the model. The combination of these two lines does the same thing as the model's score function does. Separating the process to two steps, allows us to calculate other evaluation metrics, which we will discuss briefly in the next slide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 6\n",
    "### Classification Metrics\n",
    "#### Classification Report\n",
    "```\n",
    "metrics.classification_report(y_test, predicted)\n",
    "```\n",
    "\n",
    "<img src='images/classification_report.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "The Scikit learn metrics module has classfication_report function which returns a classification report with more comprehensive metrics.\n",
    "\n",
    "The table in this slide is the classification report for our logistic regression model. The first line is the scores on predicting class 0, or or salary below 50 thousand dollars and the second line is the score on predicting class 1 or salary above 50 thousand dollars.\n",
    "\n",
    "Precision is the proportion of predictions that is correct. For example, the first line shows that among all low income predictions, 81% of them are correct.\n",
    "\n",
    "Recall rate is the proportion of an actual class that are predicted correctly. For example, the second line in recall column indicates that the model only identifies 34% of all high income cases correctly.\n",
    "\n",
    "We will discuss the classification report in more detail in the future lessons. In this lesson we'll focus on precision and recall.\n",
    "\n",
    "The classification report displays percentage of precision and recall rate, to see actual count of correct and incorrect predictions, we can use confusion matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 7\n",
    "### Classification Metrics\n",
    "#### Confusion Matrix\n",
    "<img src='images/confusion_matrix.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the confusion matrix. The lesson 2 notebook has the code to plot this matrix. You don't have to understand the plotting code, but you need to know how to use the function to plot a confusion matrix.\n",
    "\n",
    "In this matrix, the rows indicate the true outcomes or number of actual classes, the columns indicate the predicted outcomes. The first row indicates that there are total 1092 plus 119 low income cases, and 1092 of them are predicted correctly as low income by the model. The second row shows that there are 255 plus 134 high income cases, among which only 134 are classified correctly by the model. \n",
    "\n",
    "The confusion matrix is very intuitive and informative, We can easily calculate accuracy score, procision and recall rate from the confusion matrix. We will discuss the confusion matrix in more detail in future lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 3: Introduction to Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Decision Tree\n",
    "- Supervised learning\n",
    "- For both classification and regression problems\n",
    "- Can handle categorical feature(numerical)\n",
    "- Feature scaling is not needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "In this lesson we will discuss the decision tree algorithm, which is a simple algorithm that is easy to understand. Decision tree is a tree-like decsion making model. It can be used in both classification and regression problems. \n",
    "\n",
    "For linear regression and logistic regression, we normally want to create dummy variables for categorical features because we don't want to introduce artificial ranking information into the dataset. Decesion tree, on the other hand, can handle categorical features directly as long as they have numeric values. For string categorical values, we can simply apply label encoding to map them to numeric values.\n",
    "\n",
    "Just like in linear or logistic regression, for a decision tree, we normally don't need to scale the continous features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Decition Tree\n",
    "<img src=\"images/m2_dt_sunglasses.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "This is a simply decision tree. Decision tree uses conditional statements to split the dataset into different groups. In this image, the black boxes with conditions are decision nodes. The tree splitted into branches based on the conditions. The end of branches that don't split any more are called leaves, or decision nodes. \n",
    "\n",
    "The decision tree in this slide determines whether you should wear sunglasses based on time and location. The root node has a condition on time, if it's night, directly go to decision no sunglasses. If it's day, check location and eventually make decision based on location.\n",
    "\n",
    "In this decision tree, the two features are all categorical features. If there're continuous features, the conditions will be comparisons, for example, check if temperature is greater than 90 degree.\n",
    "\n",
    "In this lesson, we will use iris dataset to demonstrate how to apply decsion tree classfier with scikit learn decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3\n",
    "#### Iris Dataset\n",
    "<img src='images/iris_dataset.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script\n",
    "\n",
    "The iris dataset is a seaborn built-in dataset. The data set consists of 50 samples from each of three species of Iris, Iris setosa, Iris virginica and Iris versicolor. Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
    "\n",
    "For the classification model, the species column will be our label. For a decision tree model, we don't need to scale continuous features. We do need to encode the label, or the species column because they have string values.\n",
    "\n",
    "After we encode the species column and split the dataset to training and test, we can use the standard way to apply a scikit learn decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Decision Tree Classifier\n",
    "```\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=23)\n",
    "dtc = dtc.fit(d_train, l_train)\n",
    "score = dtc.score(d_test, l_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "By now you should be very familiar with this piece of code. \n",
    "\n",
    "Here we use the score function of the model directly to get accuracy score. If you recall what we did in previous lesson, we first get predictions with predict function, then compare the predictions with true label to get accuracy score. Both ways do the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "<img src='images/dt_image.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "A big advantage of decision tree is its interpretability. We can plot the tree from the decision tree model. This is the decision tree that's created on the iris dataset.\n",
    "\n",
    "The first line in the decision nodes is the condition. For example, in the root node, the condition is petal width less than or equal to 0.8.\n",
    "\n",
    "samples is the total number of data points in the node. In the root node, there're 90 samples which is total count of the training dataset. \n",
    "\n",
    "value is a list which shows the count of each class in the node. There are three different species in the dataset. The root node indicates that there are 29 setosa, 32 versicolor and 29 verginica in the training dataset.  \n",
    "\n",
    "gini is a measurement of the homogeneity of the node. A smaller gini indicates higher homogeneity. For example, in the first decision node at 2nd row, the gini is zero, which means all of the 29 samples in this node are same species, or setosa. We reach this node with the condition petal width less than or equal to 0.8. This means with just one condition, we can already identify all setosa iris.\n",
    "\n",
    "A decision tree can have many layers and eventually reach to a point that all leaves have 0 gini, or all data points in a leaf node have same class. The tree we show in this slide has some impure leaves, because we limit the depth of the tree to make sure the tree is not too big to fit in a slide.\n",
    "\n",
    "With this decsion tree constructed, we can feed new iris data into this tree and we will reach to one of the leaf nodes base on the features. The majority class in the leaf node will be the classification of the new data.\n",
    "\n",
    "For a regression problem, the mean of all outputs in the leaf nodes will be the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "#### Auto MPG Dataset\n",
    "<img src='images/mpg_dataset.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side 6 Script\n",
    "For decision tree regression, we will use another seaborn built-in dataset, the auto mpg dataset. The dependent variable is mpg, or mile per gallon. We'd like to predict a vehicle's fuel efficiency based on its features like cylinders, horsepower, model year etc.\n",
    "\n",
    "For the categorical features with string values like orign, we need to encode them to numeric values, and we don't need to scale the continuous features. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 7\n",
    "#### Decision Tree Regressor\n",
    "```\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "auto_model = DecisionTreeRegressor(random_state=23)\n",
    "auto_model = auto_model.fit(ind_train, dep_train)\n",
    "score = auto_model.score(ind_test, dep_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Script\n",
    "\n",
    "After splitting the dataset to training and test, we can apply the scikit learn decision tree regressor with the standard approach with these several lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "### Slide 1\n",
    "#### Module 2 Review\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "\n",
    "We learned three machine learning algorithms in this module. Linear regression is for regression problems, logistic regression is used for classification problems despite of its name. Decision tree can be use on both classification and regression problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2\n",
    "#### Module 2 Review\n",
    "- Continuous Features\n",
    " - Scaling not needed\n",
    "- Categorical Features\n",
    " - Create dummy variables\n",
    "   - Linear Regression\n",
    "   - Logistic Regression\n",
    " - Label encoding\n",
    "   - Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "For continuous features, all three algorithms don't require feature scaling. \n",
    "\n",
    "For categorical features, creating dummy variables is normally prefered for linear regression and logistic regression. Decision tree, on the other hand, can deal with categorical features directly as long as they have numeric values. For categories with string value, label encoding is good enough for a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script without slide\n",
    "The first module's assignment is fairly straightforward. Just remember to work on the problems in order.\n",
    "\n",
    "Good luck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
