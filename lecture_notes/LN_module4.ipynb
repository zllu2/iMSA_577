{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction Script\n",
    "Hello and welcome. \n",
    "\n",
    "\n",
    "As I mentioned before, please watch the video to learn the concepts behind the algorithms, and more importantly, go through the lesson notebooks and practice as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 1: Introduction to Regression Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Evaluation Metrics for Regression\n",
    "\n",
    "- Mean Absolute Error (MAE)\n",
    "- Mean Squared Error (MSE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- R-squared ( ùëÖ2 )\n",
    "\n",
    "<img src='images/linear_regression.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "<img src='images/linear_regression.png' width=400>\n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum |y_i - \\hat{y}_i|$\n",
    "\n",
    "or\n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum |\\epsilon_i|$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "<img src='images/linear_regression.png' width=400>\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "or\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum \\epsilon_i^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$RMSE = \\sqrt{MSE}$\n",
    "\n",
    "or\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5\n",
    "### R-squared ($R^2$)\n",
    "\n",
    "$MSE_{base} = \\frac{1}{n} \\sum (\\bar{y} - \\hat{y}_i)^2$\n",
    "\n",
    "Where $\\bar{y}$ is the mean of observed outputs.\n",
    "\n",
    "$R^2 = 1- \\frac{MSE_{model}}{MSE_{base}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 6\n",
    "#### Metrics Calculation\n",
    "```\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "dtr = DecisionTreeRegressor(random_state=23)\n",
    "dtr = dtr.fit(ind_train, dep_train)\n",
    "pred = dtr.predict(ind_test)\n",
    "\n",
    "# Copute performance metrics\n",
    "mae = mean_absolute_error(dep_test, pred)\n",
    "mse = mean_squared_error(dep_test, pred)\n",
    "rmse = math.sqrt(mse)\n",
    "mr2 = r2_score(dep_test, pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7\n",
    "#### Residual vs. Observed Plot\n",
    "<img src='images/residual.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 2: Introduction to Classification Evaluation I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Evaluation Metrics for Classification\n",
    "\n",
    "- Accuracy Score\n",
    "- Classification Report\n",
    "- Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Accuracy Score\n",
    "\n",
    "\n",
    "Accuracy Score = $\\frac{Correct Prediction}{All Prediction}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Classification Report\n",
    "\n",
    "<img src=\"./images/classification_report.png\" width=\"600\">\n",
    "\n",
    "#### Precision\n",
    "The proportion of the prediction that is correct. \n",
    "\n",
    "\n",
    "#### Recall\n",
    "Proportion of actual class that is predicted correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "Precision is the proportion of the prediction that is actually correct. \n",
    "\n",
    "The above classification report shows that, the precision of class 0 or negative is 0.8, it means that out of all negative predictions by the model, 80% of them are correct. Likewise, 71% of all positive predictions are actually positive.\n",
    "\n",
    "Recall is the proportion of actual class of a label that is identified correctly.\n",
    "\n",
    "The above classification report shows that, the recall of class 0 or negative is 0.97, it means that out of all observed negative classes, 97% of them are predicted as negative by the model. On the other hand, out of all observed positive classes, only 25% of them are classified correctly as positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Confusion Matrix\n",
    "<img src=\"images/confusion_matrix_raw.png\" width='500'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5\n",
    "#### Confusion Matrix\n",
    "\n",
    "- **True Positive (TP)**: Total number of predicted positives that are actually positive.\n",
    "\n",
    "- **True Negative (TN)**:  Total number of predicted negatives that are actually negative.\n",
    "\n",
    "- **False Positive (FP)** aka **Type 1 Error**: Total number of predicted positives that are actually negative.\n",
    "\n",
    "- **False Negative (FN)** aka **Type 2 Error**: Total number of predicted negatives that are actually positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5 Script\n",
    "\n",
    "When there are two outcomes, 0 and 1, It is a table with 4 different combinations of predicted and actual values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "#### Metrics Calculation\n",
    "\n",
    "$Accuracy Score = \\frac{Correct Predictions}{All Predictions} = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "$Positive Precision = \\frac{True Positive}{Predicted Positive} = \\frac{TP}{TP + FP}$\n",
    "\n",
    "$Negative Precision = \\frac{True Negative}{Predicted Negative} = \\frac{TN}{TN + FN}$\n",
    "\n",
    "$Positive Recall = \\frac{True Positive}{Actual Positive} = \\frac{TP}{TP + FN}$\n",
    "\n",
    "$Negative Recall = \\frac{True Negative}{Actual Negative} = \\frac{TN}{TN + FP}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Scrip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7\n",
    "#### When to Avoid Type 1 Error(False Positive)\n",
    "- Criminal trial\n",
    "\n",
    "#### When to Avoid Type 2 Error(False Negative)\n",
    "- Cancer screening test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Scrip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8\n",
    "#### Case Study: Direct Mail Marking\n",
    "- Case 1: Reach to as many target cusotmers as possible(high positive recall)\n",
    "- Case 2: Reduce mails to wrong customers(high positive precision)\n",
    "<img src='https://eugeneloj.typepad.com/.a/6a00d834516e6369e20120a977b5af970b-pi' width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9\n",
    "#### Adust class_weight Hyperparamter\n",
    "\n",
    "##### class_weight\n",
    "- **None**(defaul): give all classes same weight\n",
    "- **balanced**: give weights inversely proportional to class frequencies in the input data\n",
    "- **custom weight**: `{0:0.8, 1:0.2}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 3: Introduction to Classification Evaluation II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1\n",
    "\n",
    "#### ROC\n",
    "Receiver operating characteristic\n",
    "#### AUC\n",
    "Area under curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### ROC & AUC\n",
    "<img src='images/roc.png' width=350>\n",
    "\n",
    "**True positive rate(TPR)** aka. positive recall rate.  \n",
    "The ratio of true positive over all actual positive.  \n",
    "$TPR = \\frac{True Positive}{All Actural Positive}= \\frac{TP}{TP + FN}$\n",
    "\n",
    "**False positive rate(FPR)**  \n",
    "The ratio of false positive over all actual negative.  \n",
    "$FPR = \\frac{False Positive}{All Actual Negative} = \\frac{FP}{TN + FP}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "ROC stands for receiver operating characteristic. Originally developed during World War Two to predict the performance of an individual using a radar system The ROC curve displays the relationship between the false positives rate and the true positives rate.\n",
    "\n",
    "In this image, the y axis represents true positive rate or TPR, which is calculated by dividing true positive by all actual positive. This is actually the positive recall rate, or the percentage of all positive classes that are predicted correctly.\n",
    "\n",
    "The x axis in the image represents false positive rate, or FPR, which is calculate by deviding false positive  by all negatives.\n",
    "\n",
    "As we saw in the last lesson's case study, we can set a hyperparamter class_weight to a classifier to adjust how likely a model classifies a data point as positive.\n",
    "\n",
    "Now let's assume we config our classifier so that it classifies all data points as negative. Then both true positive and false positive are 0, because there's no positive prediction at all. In the ROC plot, this is the starting point of an ROC curve, point (0,0). To make things simple, let's assume the dataset is a balanced dataset, which means about 50% have positive class. Now let's adjust the model so that our model predict 10% of all datapoints as positive. First, let's examine a random model, which predicts with random guess, so it predicts correctly 50% of the time. For the 10% datapoints that are predicted as positive, 5% is correctly predicted, so true positive rate is 5% divided by actual positive which is 50%, since we assume the dataset is balanced, 50% of the dataset have positive class. So TRP is 5% divided by 50% which is 0.1, similarly, false positive rate is 5% divided by all actual negatives which is also 50%, the FPR is also 0.1. If we increase the random model's positive prediction, both TPR and FPR increase at same rate. Eventually, when all datapoints are predicted as positive, now TPR is 1, because all actually positive are predicted as positive, on the other hand, FPR is also 1, becuase all actual negative are all predicted as positive, so false positive is same as actual negtives. So for a random model, the ROC curve is a straight line from (0,0) to (1,1), which is the dashed diagonal line in the plot.\n",
    "\n",
    "Now let's look at a perfect model, which means it predicts perfectly. The roc curve still starts from (0,0), when we adjust class_weight so that the model predicts 10% of the dataset as positive, since the model is a perfect model, all positive predictions are correct, now TPR equals to 10% divided by 50%, or 0.2, and FPR is 0 because false positive is 0. When we increase positive predictions, TPR increases and FPR remains as 0, to the point when  all actual positives are predicted as positive, ROC reaches to the point (0, 1). From there, when we increase positive prediction, TPR remains as 1, and FPR starts to increase, since all new positive predictions are actual negatives. When the model predicts all datapoints as positive, ROC is now at point(1,1). So the ROC curve for a perfect model is the vertical and horizental blue line in the image.\n",
    "\n",
    "The ROC cuver of an actual classifier should normally fall in between the roc of the random model and that of the perfect model. In the image, the green curve line represents an ROC curve of a real model.\n",
    "\n",
    "The area under the ROC curve, or AUC, is a metric that indicate how good a model is. As shown in the image, AUC of a perfect model is 1X1 which is 1, AUC of a random model is 0.5. For a real model's AUC, the closer to 1 the better the model.\n",
    "\n",
    "AUC is generally considered a better metric than the accuracy rate, especially with imbalanced dataset. For example, assume only 10% of a dataset have positive class, for a model that always predicts with major class, or negative, the accuracy is 90%, but the AUC is only 0.5, or same as a random model. You may try this out in the lesson notebook by setting class_weight so that negative class has value 1 and positive class has value 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Plot ROC\n",
    "#### Classifiers that have `decision_function`:\n",
    "- Logistic Regression\n",
    "- Support Vector Machine\n",
    "\n",
    "#### Classifiers that have `predict_proba`:\n",
    "- K-nearest Neighbors\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4\n",
    "#### Compare Models with ROC&AUC\n",
    "<img src='images/roc_auc.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Review Script\n",
    "\n",
    "\n",
    "The first module's assignment is fairly straightforward. Just remember to work on the problems in order.\n",
    "\n",
    "Good luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
