{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction Script\n",
    "(cover)\n",
    "Hello and welcome. \n",
    "\n",
    "So far, we've learned six supervised machine learning algorithms. For supervised learning, we split the dataset to training and test. \n",
    "\n",
    "(split slide)\n",
    "\n",
    "We fit the models with the training set. The test set is used to evaluate the model. \n",
    "\n",
    "Model evaluation is an important part of data analytics process. \n",
    "\n",
    "(crips-dm)\n",
    "\n",
    "In the crisp-dm framework, evaluation is the fifth step after modeling. \n",
    "\n",
    "In the previous lessons, we mainly used r-squared to evaluate regression models and accuracy score to evaluate classification models. There are other evaluation metrics for both regression and classfication. \n",
    "\n",
    "In this lesson, we will discuss those metrics in detail. \n",
    "\n",
    "(module content)\n",
    "\n",
    "We will cover regression evaluation metrics in lesson 1.\n",
    "\n",
    "In lesson2 and 3, we will discuss classification evaluation metrics. We will also have a case study to demonstrate how to improve a model with the help of evaluation metrics.\n",
    "\n",
    "As I mentioned before, please watch the video to learn the concepts behind the algorithms, and more importantly, go through the lesson notebooks thoroughly and practice as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 1: Introduction to Regression Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Evaluation Metrics for Regression\n",
    "\n",
    "- Mean Absolute Error (MAE)\n",
    "- Mean Squared Error (MSE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- R-squared ( ùëÖ2 )\n",
    "\n",
    "<img src='images/linear_regression.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "In this lesson we will learn four evaluation metrics for regression models.  \n",
    "the mean absolute error,  \n",
    "the mean squared error,  \n",
    "the root mean squared error, and  \n",
    "the r-squared.  \n",
    "\n",
    "All metrics are calculated with residuals, or the difference between the predicted outcomes and the observed outcomes.\n",
    "\n",
    "We will use the simple linear regression to explain the concepts.\n",
    "\n",
    "In the simple linear regression, as shown in this image, residual or epsilon is the difference between the observed outcome y and the predicted outcome y hat of each data point x. All regression evaluation metrics are different combinations of the residuals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "<img src='images/linear_regression.png' width=400>\n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum |y_i - \\hat{y}_i|$\n",
    "\n",
    "or\n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum |\\epsilon_i|$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "The mean absolute error (MAE) is the simplest regression error metric to understand. We'll calculate the residual for every data point, taking only the absolute value of each so that negative and positive residuals do not cancel out. We then take the average of all these residuals.\n",
    "\n",
    "Mean absolute error gives all residuals same weight. But In many circumstances it makes sense to give more weigt to  larger errors. This brings up the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "<img src='images/linear_regression.png' width=400>\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "or\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum \\epsilon_i^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "The difference between mean sqaured error and mean absolutie error is, instead of using absolute value of residuals, mean squared error squares the residuals before summing them up. The square term esentially gives larger errors more weight in calcluating the metric.\n",
    "\n",
    "The sum of the squared errors is the cost function of the popular linear regression method ols. ols fits a linear function with the data by minimizing the cost function.\n",
    "\n",
    "The disadvantage of the mean squared error as a evaluation metric is that the square term changes the unit of the metric, which makes interpretation of mean squared error difficult. \n",
    "\n",
    "For example, for a regression model on mpg dataset, if the mean absolute error is 2, we can say that the prediction of the regression model is off by 2 miles per gallon on average. But if the mean squared error is 5, it's hard to interpret the value.\n",
    "\n",
    "To fix this problem, we can take a square root of the metric, which brings up the root mean squared error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$RMSE = \\sqrt{MSE}$\n",
    "\n",
    "or\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "\n",
    "The root mean squared error is just the square root of the mean squared error. It has advantages of both mean squared error and mean absolute error. It gives larger errors more weigth due to the square term. It also has same unit as the dependent variable due to the square root, which makes it easier to interpret.\n",
    "\n",
    "All three regression metrics we learned so far have range from 0 to infinity. Smaller values indicate better model. But it's hard to judge a regression model with this metrics alone. For example, if a model's root mean squared error is 10, we don't really know how good the model is. We have to compare it with another model, for examole,we know that it's better than the other model that has root mean squared error 20.\n",
    "\n",
    "It would be nice if we can come up with a metric that can be used to judge a model by itself. So we need a benchmark or a base model to compare to determine how good a regression model is. This brings up the next metric, the R-squared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5\n",
    "### R-squared ($R^2$)\n",
    "\n",
    "$MSE_{base} = \\frac{1}{n} \\sum (y_i - \\bar{y})^2$\n",
    "\n",
    "Compare model with base: $\\frac{MSE_{model}}{MSE_{base}}$\n",
    "\n",
    "$R^2 = 1- \\frac{MSE_{model}}{MSE_{base}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "R-sqaured is also called Coefficient of Determination. It compares a model's Mean Squared Error with that of a base model. \n",
    "\n",
    "So what is the base model? The base model simply predict with the mean value of all observed values. For example, for the mpg dataset, the base model calculates the average mpg of all vechicles in the training dataset, then predict mpg  of any new vechicle with the mean mpg.\n",
    "\n",
    "To get the r-squared score, we first calculate the mean squared error of the base model as shown in the first equation. In the equation, y-bar is the mean of training data outcomes, yi is the observed outcome of the test set. \n",
    "\n",
    "Then we compare our model's Mean Squared Error with the base model's Mean Squared Eerror by calculating the fraction of the two mean square errors, or MSE model divided by MSE basemodel\n",
    "\n",
    "If we have a perfect model, which means the model's mean squared error is 0, then the fraction will be 0. If our model predicts as poorly as the base model, then the fraction will be 1 since the model's mean squared error is same to the base model's mean squared error. It's possible that the model's mean squared error is even greater than the base model's, which means the model is simply useless since its predictions are even worse then predicting with the mean value. So normally this fraction has the range from 0 to 1. A smaller value indicates a better model.\n",
    "\n",
    "Since we are used to use bigger number to represent better result, we define R-sqaured as 1 minus the fraction. A perfect model will have r-squared 1, a model predicts as poorly as the base model will has r-quared 0.\n",
    "\n",
    "With r-squared, we can evaluate a regression model with just one value, the closer a model's r-sqaured to 1, the better the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 6\n",
    "#### Metrics Calculation\n",
    "```\n",
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "mae = metrics.mean_absolute_error(dep_test, pred)\n",
    "mse = metrics.mean_squared_error(dep_test, pred)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = metrics.r2_score(dep_test, pred)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "Scikit learn metrics module defines functions to calculate the regression metrics except for the root mean sqaured error since it's simply the square root of the mean squared error.\n",
    "\n",
    "To calculate these metrics, we need to use the observed values and the predicted values. In the code example in this slide, dep_test is the observed outcome of the test dataset and pred is the model prediction on the test dataet. You get predictions by using regression model's predict function. Please refer to the lesson notebook for more coding details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7\n",
    "#### Residual vs. Observed Plot\n",
    "<img src='images/residual.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Script\n",
    "\n",
    "We can also evaluate a model by visualing the residuals. The image in this slide is the residual vs observed for a random forest regressor trained on the mpg dataset. The x axis represents the observed outputs, or mile per gallon, the y axis represent the residuals. Ideally, the residuals should be evenly distributed around zero for all outcomes. \n",
    "\n",
    "In this plot, we can see that the random forest model only has negative residuals when mpg is greater than 35, which means the model tends to under-estimate mile per gallon for vehicles that have high fuel efficiency.\n",
    "\n",
    "We define the function for this plot in our lesson notebook. You can use the function to make this plot for all regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 2: Introduction to Classification Evaluation I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Evaluation Metrics for Classification\n",
    "\n",
    "- Accuracy Score\n",
    "- Classification Report\n",
    "- Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "\n",
    "In this lesson we will discuss evaluation metrics for a classfication model. We actually already used all the 3 metrics listed in this slide in previous lessons. We will discuss them in more detail in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Accuracy Score\n",
    "\n",
    "\n",
    "Accuracy Score = $\\frac{Correct Prediction}{All Prediction}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "\n",
    "Accuracy score is the simplest classification evaluation metric. It's simply the proportion of the correct predictions. \n",
    "\n",
    "Scikit learn classification models have function that returns the accuracy score, the function name is score. You can also get accuracy score with accuracy_score function in the scikit learn metrics module. \n",
    "\n",
    "The accuracy score is often the first metric we use to evaluate a model since it's so intuitive. But this metric can be misleading especially when the dataset has imbalanced outcome.\n",
    "\n",
    "For example, if we have a cancer screening test dataset, there's only 1% in the whole dataset has positive outcome. A zero model, which always predict with majority class, in this case, always predicts negative regardless the test result. The zero model will actually achieve 99% of accuracy score because 99% of time, the outcome is negative. But this model is useless since it can't identify any postive case from the test results, which is the purpose of the screening test.\n",
    "\n",
    "It will be helpful if we can evaluate different outcomes separately. And we can achive this by using the classification report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Classification Report\n",
    "\n",
    "<img src=\"./images/classification_report.png\" width=\"600\">\n",
    "\n",
    "#### Precision\n",
    "The proportion of the prediction that is correct. \n",
    "\n",
    "\n",
    "#### Recall\n",
    "Proportion of actual class that is predicted correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "The scikit learn metrics module has a function classification_report that returns a classficication report. \n",
    "\n",
    "This is a classification report for a classifier trained on the adult income dataset. The dataset has two outcomes or classes, less than $50,000 per year which is represented by 0, or higher than 50,000 per year which is represented by 1. In the classification report, the first line is the statistics for less than 50,000 per year, or low income prediction; the second line is for high income prediction.\n",
    "\n",
    "Precision is the proportion of the prediction that is actually correct. This is similar to the accuracy score\n",
    "\n",
    "In this classification report, the precision of class 0 or low income is 0.81, it means that out of all low income predictions by the model, 81% of them are correct. Likewise, 53% of all high income predictions are actual high income.\n",
    "\n",
    "Recall is the proportion of actual class that is identified correctly.\n",
    "\n",
    "The classification report shows that, the recall of class 0 is 0.90, it means that out of all low income cases, 90% of them are predicted correctly as low income by the model. On the other hand, out of all high income cases, only 34% of them are identified correctly.\n",
    "\n",
    "So this model may have 80% accuracy score, but it does a poor job to identify high income individuals.\n",
    "\n",
    "The f1-score is calculated from the precision and recall with a special kind of average. The micro, macro and weighted average are different averages of the scores for all outcomes.\n",
    "\n",
    "Support indicates the count of each classes in the dataset. In this example, there are 1211 low income cases and 389 high income cases in the dataset. That 1600 is the sum of all caese, which equals to 1211 + 389.\n",
    "\n",
    "\n",
    "The classification report shows the percentages of the correct predictions. To see the actual number of correct and incorrect predictions, we can use the confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Confusion Matrix\n",
    "<img src=\"images/confusion_matrix.png\" width='500'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "\n",
    "This is the confusion matrix for the same classifier trained on the adult income dataset. \n",
    "\n",
    "Here we use low for low annual income that's below 50,000 dollars per year, or class 0 in the dataset label. High represents high annual income that's over $50,000.\n",
    "\n",
    "The rows in the matrix are actual count of each classes. The first row indicates there are 1092+119 low income cases in the dataset. The second row indicates there are 255 + 134 high income cases in the dataset.\n",
    "\n",
    "The columns in the matrix are count of predicted classes. So any individual number in the matrix has two meanings. The number in the top left cell of the matrix, or 1092 indicates that this 1092 cases are actual low income and also predicted correctly by the model as low income. The number in the top right cell, or 119, indicates that this 119 cases are actual low income but predicted by the model as high income. Similarly, 255 high income cases are wrongly predicted as low income and 134 high income cases are predicted correctly.\n",
    "\n",
    "For a dataset that only has two classes, normally represented by 0 and 1, we normally call class 0 as negative and class 1 as positive. We can then name the 4 cells in the confusion matrix as shown in the next slide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "<img src=\"images/confusion_matrix_raw.png\" width='500'>\n",
    "\n",
    "- **True Positive (TP)**: Total number of predicted positives that are actually positive.\n",
    "\n",
    "- **True Negative (TN)**:  Total number of predicted negatives that are actually negative.\n",
    "\n",
    "- **False Positive (FP)** aka **Type 1 Error**: Total number of predicted positives that are actually negative.\n",
    "\n",
    "- **False Negative (FN)** aka **Type 2 Error**: Total number of predicted negatives that are actually positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "The top left cell is called True Negative, because their actual class is negative, and they are predicted as negative. The bottom left cell is called False negative, because they are predicted as negative, but their true class is positive. Similarly, the bottom right cell is called true positive since they are correctly predicted as positive, and the top right cell are wrongly predicted as positive so it's called false positive.\n",
    "\n",
    "False positive is also called type 1 error, and false negative is also called type 2 error. We will discuss these two types of errors later in a case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "#### Metrics Calculation\n",
    "\n",
    "<img src=\"images/confusion_matrix_raw.png\" width='500'>\n",
    "\n",
    "$Accuracy Score = \\frac{Correct Predictions}{All Predictions} = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "$Positive Precision = \\frac{True Positive}{Predicted Positive} = \\frac{TP}{TP + FP}$\n",
    "\n",
    "$Negative Precision = \\frac{True Negative}{Predicted Negative} = \\frac{TN}{TN + FN}$\n",
    "\n",
    "$Positive Recall = \\frac{True Positive}{Actual Positive} = \\frac{TP}{TP + FN}$\n",
    "\n",
    "$Negative Recall = \\frac{True Negative}{Actual Negative} = \\frac{TN}{TN + FP}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Scrip\n",
    "With the confusion matrix, we can easily calculate the classification metrics we introduced before. \n",
    "\n",
    "For example, accuracy score which is the proportion of the correct predictions, equals to true positive + true negative dvivdided by the count of all numbers in the confusion matrix.\n",
    "\n",
    "You can pause the video and study other equations by yourself, since they are all pretty intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7\n",
    "#### When to Avoid Type 1 Error(False Positive)\n",
    "- Criminal trial\n",
    "\n",
    "#### When to Avoid Type 2 Error(False Negative)\n",
    "- Cancer screening test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Scrip\n",
    "We mentioned above that False positive is also called type 1 error and false negative is also called type 2 error.\n",
    "\n",
    "It is greate if we can reduce both errors at the same time. But it's normally very hard to achieve. We will often have to accept higher error of one type in order to reduce the error the other type.\n",
    "\n",
    "Let's take criminal trial as an example. In a criminal trial, guilty conviction is positive outcome. If an innocent person is wrongly convicted, it's called false positive, or type 1 error. If a criminal is acquitted, it's false negative, or type 2 error.\n",
    "\n",
    "It's clear that we'd like to avoid type 1 error by any means in a criminal trial since we don't want to convict an innocent person. To achive this goal, we can accept the fact that we relase some true criminals in the process.\n",
    "\n",
    "Now let's consider another exmaple, cancer screening test. In a cancer screening test positive means has cancer. if a patient who has cancer but tested negative, it's false negative, or type 2 error. If a healthy person is tested poistive, it's false positive or type 1 error. \n",
    "\n",
    "In this case, we want to avoid type 1 error or false negative as much as possible, since misdiagnosing a cancer patient would delay proper treatment. We can accept some false positive, since we can have extra tests to confirm the result.\n",
    "\n",
    "In data analytics, we also always make decision on this trade off. Let's look at a case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8\n",
    "#### Case Study: Direct Mail Marketing\n",
    "- Case 1: Reach to as many target cusotmers as possible(high positive recall)\n",
    "- Case 2: Reduce mails to wrong customers(high positive precision)\n",
    "<img src='https://eugeneloj.typepad.com/.a/6a00d834516e6369e20120a977b5af970b-pi' width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8 Script\n",
    "The company in this case sells luxury goods and they use direct mail marketing to reach out to their target customers, who are wealthy individuals. Their marketing director wants to build a classification model with adult income dataset to help him identify high income customers.\n",
    "\n",
    "Now consider two cases:\n",
    "\n",
    "In the first case, the company has enough budget for marketing, the director wants to reach out to as many high income individuals as possible. He's ok that some mails are sent to low income individuals in the process. This is similar to the cancer screening case, which we want to avoid false negative or type 2 error at the cost of higher false positive or type 1 error.\n",
    "\n",
    "In the second case, the company has limited marketing budget, the company can't afford to send out too many marketing mails. So the director hopes to send mails only to those true high income individuals. This is like in the criminal trial, He wants to avoid false positive, or type 1 error. In the process he's willing to accept the fact that he may miss some high income individuals, or have higher type 2 error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9\n",
    "#### Adjust class_weight Hyperparamter\n",
    "\n",
    "##### class_weight\n",
    "- **None**(default): give all classes same weight\n",
    "- **balanced**: give weights inversely proportional to class frequencies\n",
    "- **custom weight**: `{0:0.8, 1:0.2}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9 Script\n",
    "In scikit learn classification models, there's a hyperparamter class_weight, which is used to determine the weights associated to each outcome class. For example, we can set class_weight to a dictionary, in which we assign weight on negative class to 0.8, and weight on positive class to 0.2. This tells the model to bias toward negative outcome. In another word, the model is more likely to classify a data point as negative. A data point will be classified as positive only when the model is highly confident that it's positive.\n",
    "\n",
    "The default value of class_weight is None, or all classes have same weight. Another value is balanced, which gives class weight based on class distribution in the training dataset. For example, if there are more negative classes in the dataset, less weight will be given to the negative class, or make it harder to classify a datapoint as negative.\n",
    "\n",
    "(next slide)\n",
    "In our case studay, for case 1, we set class_weight of class 0 to 0.25, and class one to 0.75, this way the model is more likely to classify an individual as positive. This increases false positive, but reduces false negative.\n",
    "\n",
    "For case 2, we can set class_weight of class 0 to 0.8, and class one to 0.2, this way the model is less likely to classify an individual as positive. This decreases false positive, but increase false negative.\n",
    "\n",
    "By adjusting the class_weight of a classification model, we can achieve our goal to reduce one type of error at the cost of increasing the other type of error. Please refer to the lesson notebook for more coding details of this case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 10(not needed)\n",
    "<img src='images/knn2.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 10 Script\n",
    "~~Let's use k-nearest neighbor as an example. For an imbalanced dataset, for example, 80% of the training dataset has negative outcome. For a new data point, its neighbors are more likely to be negative, if we assign same weight to all classes, assume we use 9 neighbors, we'll classfiy a new datapoint with simple majority of the neighbors. Since 80% of the dataset of the training data has negative outcome,  it's really hard to identify positive cases because we need at least 5 positive neighbors to classify it as positive.\n",
    "\n",
    "~~If we set class_weight to balance, now the model bias toward positive since there are less positive in the training data. Now the model may classify a new data point as positive when there're 4 or even 3 positive neighbors.\n",
    "\n",
    "~~By adjusting class_weight, we can achive our goal of avoiding type 1 error or type 2 error.\n",
    "\n",
    "~~Please read the lesson notebook for the details of the case study and associated python code.~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 3: Introduction to Classification Evaluation II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1\n",
    "\n",
    "#### ROC\n",
    "Receiver operating characteristic\n",
    "#### AUC\n",
    "Area under curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "In this lesson, we will introduce the ROC curve and the AUC score.\n",
    "\n",
    "ROC stands for Receiver operating characteristic, originally developed during World War Two to predict the performance of an individual using a radar system.\n",
    "\n",
    "AUC stands for area under curve, it's the area under the roc curve. The value of AUC can be used to evaluate a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### ROC & AUC\n",
    "<img src='images/roc.png' width=350>\n",
    "\n",
    "**True positive rate(TPR)** aka. positive recall rate.  \n",
    "The ratio of true positive over all actual positive.  \n",
    "$TPR = \\frac{True Positive}{All Actural Positive}= \\frac{TP}{TP + FN}$\n",
    "\n",
    "**False positive rate(FPR)**  \n",
    "The ratio of false positive over all actual negative.  \n",
    "$FPR = \\frac{False Positive}{All Actual Negative} = \\frac{FP}{TN + FP}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "ROC curve displays the relationship between the false positives rate and the true positives rate.\n",
    "\n",
    "In this image, the y axis represents the true positive rate, which is calculated by dividing true positive by all actual positive. \n",
    "\n",
    "The x axis in the image represents the false positive rate. False positive rate is the proportion of true negative cases that are wrongly predicted as positive.\n",
    "\n",
    "You may pause the video here and make sure you understand the concept of true positive rate and false positive rate.\n",
    "\n",
    "To make things simple, let's assume that we have a dataset that has 100 data points, 50 of them has positive outcomes and 50 has negative outcomes\n",
    "\n",
    "As we saw in lesson 2 case study, we can set a hyperparamter class_weight to a classifier to adjust how likely a model classifies a data point as positive.\n",
    "\n",
    "Now let's assume we set class weight for class 0 or negative to 1, and set classweight for positive class to 0. Then  the classifier will classify all data points as negative. At this point, both true positive and false positive are 0, because there's no positive prediction at all. In the ROC plot, this is the starting point of an ROC curve, point at coordinate (0,0). Now let's adjust the class_weight so that our model predicts 10% of all datapoints as positive. First, let's examine a random model, which predicts with random guess, so it predicts correctly 50% of the time. For the 10% or 10 data points that are predicted as positive, 5 are actual positive and 5 are actual negative. The true positive rate is 5 divided by actual positive which is 50. So True positive rate is 5 divided by 50 which is 0.1, similarly, the false positive rate euals to 5 divided by all actual negatives which is also 50, the FPR is also 0.1. If we increase the random model's positive prediction by adjusting class_weight, both True positive rate and False positive rate increase at the same rate. Eventually, when positive class weight is set to 1, all datapoints are predicted as positive, now True positive rate is 1, because all actually positive are predicted as positive, on the other hand, False positive rate is also 1, becuase all actual negative are all predicted as positive. So for a random model, the ROC curve is a straight line from coordinate (0,0) to (1,1).\n",
    "\n",
    "Now let's look at a perfect model, which means it predicts perfectly. The roc curve still starts from (0,0). When we adjust class_weight so that the model predicts 10% of the dataset as positive, since the model is a perfect model, all 10 positive predictions are correct, now True positive rate equals to 10 divided by 50, or 0.2, and False Positive Rate is 0 because false positive is 0. When we increase positive predictions, True positive rate increases and False positive rate remains at 0, to the point when all actual positives are predicted as positive, ROC reaches to the point (0, 1). From there, when we increase positive prediction, TPR remains as 1, and FPR starts to increase, since the model starts to predict negative as positive. When the model predicts all datapoints as positive, ROC is now at point(1,1). So the ROC curve for a perfect model is the vertical and horizental blue line in the image.\n",
    "\n",
    "The ROC cuver of an actual classifier should normally fall in between the roc of the random model and that of the perfect model. In the image, the red curve line represents an ROC curve of a real classification model.\n",
    "\n",
    "The area under the ROC curve, or AUC, is a metric that indicates how good a model is. As shown in the image, AUC of a perfect model is 1X1 which is 1, AUC of a random model is 0.5. So for a classification model's AUC, the closer to 1 the better the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Plot ROC\n",
    "#### Classifiers that have `decision_function`:\n",
    "- Logistic Regression\n",
    "- Support Vector Machine\n",
    "\n",
    "#### Classifiers that have `predict_proba`:\n",
    "- K-nearest Neighbors\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script\n",
    "\n",
    "The scikit learn module has a function roc_curve which plots roc. We need to provide true positive and false positive probabilities under different threshold or class weight. All scikit learn classification models have functions to return these probabilities. But different model use different functions.\n",
    "\n",
    "Logistic regression and support vector machine use decision_function to get the probabilities, other classification models we learned so far use predict_proba function. Please refer to the lesson notebook for more coding details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4\n",
    "#### Compare Models with ROC&AUC\n",
    "<img src='images/roc_auc.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "We can put multiple classification model's roc curve into same a plot to compare models. This roc plot is for 4 classifiers on the adult income data. We can see that random forest and logistic regression have best result since their cure are closest to the perfect roc line and they have largest Area under curve, or AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1\n",
    "\n",
    "#### Regression Evaluation Metrics\n",
    "- Mean Absolute Error(MAE)\n",
    "- Mean Squared Error(MSE)\n",
    "- Root Mean Squared Error(RMSE)\n",
    "- R-Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "In this lesson, we learned different ways to evaluate a machine learning model.\n",
    "\n",
    "For a regression model, we introduced mean absolute error, mean squared error, root mean squared error and r-squared. Among them, the frist 3 are measures of errors, a smaller error indicates a better model. But it's hard to used the error alone to judge a regression model. r-squared, on the other hand, is a model's score relative to a random model, which always predicts with mean value. r-squared normally ranges from 0 to 1, the closer to 1, the better the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2\n",
    "<img src='images/residual.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "The residule vs. observed plot can help us identify the weakness in the regression model. Ideally, the residuals should distributed randomly around 0 for all observed values. If we identify a pattern, for example, as this plot shows, only negative residuals when mpg is greater than 35, it means the model does a poor job in high mpg range. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3\n",
    "\n",
    "####  Classification Evaluation Metrics\n",
    "\n",
    "- Accuracy Score\n",
    "- Classification Report\n",
    "- Confusion Matrix\n",
    " - Type 1 error(False positive)\n",
    " - Type 2 error(False negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a classification model, we introduced accuracy score, classification report and confusion matrix. Please make sure you understand the relationship between confusion matrix and classification report. You also need to understand the concept of type 1 error, or false postive and type 2 error, or false negative. \n",
    "\n",
    "Sometimes, we want to avoid type 1 error, like in a crimial trial; sometimes, we want to avoid tpyp 2 error, like the cancer screening test. You may adjust the model by setting different class weight to achieve your goals in different circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4\n",
    "\n",
    "<img src='images/roc_auc.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "\n",
    "The ROC curve is a good way to compare different models. For different classification model, you need to use different function to get true and false negative probabilities. These probabilities are needed to plot the roc curve.\n",
    "\n",
    "The assignment of this module is fairly straight forward. Please remeber to work on the problems in order.\n",
    "\n",
    "Good luck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
