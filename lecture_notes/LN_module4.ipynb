{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction Script\n",
    "Hello and welcome. \n",
    "\n",
    "We've learned six machine learning algorithms so far. All of them are supervised learning. For supervised learning, we split the dataset to train and test. Fit the models with the training dataset, then evaluate the trained model with the testing dataset. \n",
    "\n",
    "Model evaluation is an important part of data analytics process. If you recall the crisp-dm framework, evaluation is the fifth step after modeling. \n",
    "\n",
    "In previous lessons, we mainly used r-squared score to evaluate regression models and accuracy score to evaluate classification models. There are other evaluation metrics for both regression and classfication. In this lesson, we will discuss various evaluation metrics in detail. We will have a case study to demonstrate how to improve a model with the help of evaluation metrics.\n",
    "\n",
    "As I mentioned before, please watch the video to learn the concepts behind the algorithms, and more importantly, go through the lesson notebooks and practice as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 1: Introduction to Regression Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Evaluation Metrics for Regression\n",
    "\n",
    "- Mean Absolute Error (MAE)\n",
    "- Mean Squared Error (MSE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- R-squared ( ùëÖ2 )\n",
    "\n",
    "<img src='images/linear_regression.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "In this lesson we will learn four evaluation metrics for regression models. All metrics are calculated with residuals, or the difference between predicted outcomes and observed outcomes.\n",
    "\n",
    "In simple linear regression, as shown in this image, residual or epsilon is the difference between y and y hat of each data point x. All regression evaluation metrics are different combinations of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "<img src='images/linear_regression.png' width=400>\n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum |y_i - \\hat{y}_i|$\n",
    "\n",
    "or\n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum |\\epsilon_i|$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "The mean absolute error (MAE) is the simplest regression error metric to understand. We'll calculate the residual for every data point, taking only the absolute value of each so that negative and positive residuals do not cancel out. We then take the average of all these residuals.\n",
    "\n",
    "Mean absolute error gives all residuals same weight. But In many circumstances it makes sense to give more weigt to  large errors. This brings up the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "<img src='images/linear_regression.png' width=400>\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "or\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum \\epsilon_i^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "The difference between mean sqaured error and mean absolutie error is, instead of using absolute of residuals, mean squared error squares the residuals before summing the up. The square term esentially gives larger errors more weight in calcluating the metric.\n",
    "\n",
    "But the square term also changes the unit of the metric, which makes interpretation of mean squared error difficult. To fix this problem, we can take square root of the metric, which brings up the root mean squared errr.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$RMSE = \\sqrt{MSE}$\n",
    "\n",
    "or\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "\n",
    "Root mean squared error is just the square root of mean squared error. It has advantages of both mean squared error and mean absolute error. It gives larger errors more weigth due to square of the residuals. And it has same unit as the dependent variable due to the square root, which makes it easier to interpret.\n",
    "\n",
    "All three regression metrics we learned so far have range from 0 to infinity. Smaller values indicate better model. But it's hard to judge a regression model with this metrics alone. We need a benchmark or a base model to compare to determine how good a regression model is. This brings up the next metric, the R-squared score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5\n",
    "### R-squared ($R^2$)\n",
    "\n",
    "$MSE_{base} = \\frac{1}{n} \\sum (\\bar{y} - \\hat{y}_i)^2$\n",
    "\n",
    "Compare model with base: $\\frac{MSE_{model}}{MSE_{base}}$\n",
    "\n",
    "$R^2 = 1- \\frac{MSE_{model}}{MSE_{base}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "R-sqaured is also called Coefficient of Determination. It compares the Mean Squared Error of the model with mean sqaured error of a base model. The base model simply predict with the mean value of observed values. For example, if training dataset is information of a group of people, the dependent variable is the height of a person. The base model calculates the mean height of group, then predict height of any person with this mean value, no matter what gender, ethic or age this person is.\n",
    "\n",
    "To get r-squared, we first calculate mean squared error of the base model. In the equation, y-bar is mean of training data outcomes. Then we compare our model's Mean Squared Error with the base model's Mean Squared Eerror with the fraction of the two values.\n",
    "\n",
    "If we have a perfect model, which means the model's mean squared error is 0, then the fraction will be 0. If our model predicts as poorly as the base model, then the fraction will be 1 since the model's mean squared error is same to the base model's mean squared error. It's possible that the model's mean squared error is even greater than the base model's, which means the model is simply useless since its predictions are worse then the mean value. So normally this fraction is range from 0 to 1. A smaller value indicates a better model.\n",
    "\n",
    "Since we are used to use bigger number to represent better result, we define R-sqaured as 1 minus the fraction. A perfect model will have r-squared 1, the base model has r-quared 0.\n",
    "\n",
    "With r-squared, we can just a regression model with just one metric value, the closer a model's r-sqaured to 1, the better the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 6\n",
    "#### Metrics Calculation\n",
    "```\n",
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "mae = metrics.mean_absolute_error(dep_test, pred)\n",
    "mse = metrics.mean_squared_error(dep_test, pred)\n",
    "rmse = math.sqrt(mse)\n",
    "r2 = metrics.r2_score(dep_test, pred)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "Scikit learn metrics module defines functions to calculate the regression metrics except for root mean sqaured error since it's simply the square root of mean squared error.\n",
    "\n",
    "To calculate these metrics, we need to use the observed values and predicted values. In the code example in this slide, dep_test is the observed outcome of test dataset and pred is the model prediction. You get prediction by using regression model's predict function. Please refer to the lesson notebook for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7\n",
    "#### Residual vs. Observed Plot\n",
    "<img src='images/residual.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Script\n",
    "\n",
    "We can visualize all the residuals against observed output to evaluate a model. This image is a residual vs observed plot for a random forest regressor trained on the mpg dataset. The x axis represents the observed outputs, or mile per gallon, the y axis represent residuals. Ideally, the residuals should be evenly distributed around zero for all outcomes. \n",
    "\n",
    "In this plot, we can see that the random forest model only has negative residuals when mpg is greater than 35, which means the model tends to under-estimate mile per gallon for vehicles that have high fuel efficiency.\n",
    "\n",
    "We define the function for this plot in our lesson notebook. You can use this function to make this plot for all regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 2: Introduction to Classification Evaluation I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Evaluation Metrics for Classification\n",
    "\n",
    "- Accuracy Score\n",
    "- Classification Report\n",
    "- Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "\n",
    "In this lesson we will discuss evaluation metrics for a classfication model. We actually already used all the 3 metrics listed in this slide in previous lessons. We will discuss them in more detail in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Accuracy Score\n",
    "\n",
    "\n",
    "Accuracy Score = $\\frac{Correct Prediction}{All Prediction}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "\n",
    "Accuracy score is the simplest classification evaluation metric. It's simply the proportion of correct predictions. \n",
    "\n",
    "Scikit learn classification models have function that returns the accuracy score, the function name is score. You can also get accuracy score with accuracy_score function in metrics module. \n",
    "\n",
    "The accuracy score is often the first metric we use to evaluate a model since it's so intuitive. But this metric can be misleading especially when the dataset has imbalanced outcome.\n",
    "\n",
    "For example, if we have a cancer screening test dataset, there's only 1% in the whole dataset has positive outcome. A zero model, which always predict with majority class, in this case, always predicts negative regardless the test result. The zero model will actually achieve 99% of accuracy score because 99% of time, the outcome is negative. But this model is useless since it can't identify any postive case from the test results, which is the purpose of the screening test.\n",
    "\n",
    "It will be helpful if we can evaluate different outcomes separately. And we can achive this by using the classification report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Classification Report\n",
    "\n",
    "<img src=\"./images/classification_report.png\" width=\"600\">\n",
    "\n",
    "#### Precision\n",
    "The proportion of the prediction that is correct. \n",
    "\n",
    "\n",
    "#### Recall\n",
    "Proportion of actual class that is predicted correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "The scikit learn metrics module has a function classification_report that returns a classficication report. \n",
    "\n",
    "This is a classification report for a classifier trained on the adult income dataset. The dataset has two outcomes or classes, less than $50,000 per year which is represented by 0, or higher than 50,000 per year which is represented by 1. In the classification report, the first line is the statistics for less than 50,000 per year, or low income prediction; the second line is for high income prediction.\n",
    "\n",
    "Precision is the proportion of the prediction that is actually correct. \n",
    "\n",
    "For example, in this classification report, the precision of class 0 or low income is 0.81, it means that out of all low income predictions by the model, 81% of them are correct. Likewise, 53% of all high income predictions are actual high income.\n",
    "\n",
    "Recall is the proportion of actual class that is identified correctly.\n",
    "\n",
    "The above classification report shows that, the recall of class 0 is 0.90, it means that out of all low income cases, 90% of them are predicted correctly as low income by the model. On the other hand, out of all high income cases, only 34% of them are classified correctly.\n",
    "\n",
    "So this model may have 80% accuracy rate on this dataset, but it does a poor job to identify high income individuals.\n",
    "\n",
    "The f1-score is calculated from the precision and recall with a special kind of average. The micro, macro and weighted average are different averages of the scores for all outcomes.\n",
    "\n",
    "Support indicates the count of each classes in the dataset. In this example, there are 1211 low income cases and 389 low income cases in the dataset. That 1600 is the sum of all caese, which equals to 1211 + 389.\n",
    "\n",
    "\n",
    "The classification report shows proportion of correct predictions. To see the number of right and wrong predictions, we can use the confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Confusion Matrix\n",
    "<img src=\"images/confusion_matrix.png\" width='500'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "\n",
    "This is the confusion matrix for the same classifier trained on the adult income dataset. \n",
    "\n",
    "Here we use low for low annual income that's below $50,000, or class 0 in the dataset label. High represents high annual income that's over $50,000.\n",
    "\n",
    "The rows in the matrix are actual count of each outcomes or classes. The first row indicates there are 1092+119 low income cases in the dataset. The second row indicates there are 255 + 134 high income cases in the dataset.\n",
    "\n",
    "The columns in the matrix are count of predicted classes. So any individual number in the matrix has two meanings. The number in the top left cell of the matrix, or 1092 indicates that this 1092 cases are actually low income and also predicted correctly by the model as low income. The number in the top right cell, or 119, indicates that this 119 cases are actual low income but predicted by the model as high income. Similarly, 255 high income cases are wrongly predicted as low income and 134 high income cases are predicted correctly.\n",
    "\n",
    "For a dataset that only has two classes, normally represented by 0 and 1, we normally call class 0 as negative and class 1 as positive. We can then name the 4 cells in the confusion matrix as shown in the next slide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "<img src=\"images/confusion_matrix_raw.png\" width='500'>\n",
    "\n",
    "- **True Positive (TP)**: Total number of predicted positives that are actually positive.\n",
    "\n",
    "- **True Negative (TN)**:  Total number of predicted negatives that are actually negative.\n",
    "\n",
    "- **False Positive (FP)** aka **Type 1 Error**: Total number of predicted positives that are actually negative.\n",
    "\n",
    "- **False Negative (FN)** aka **Type 2 Error**: Total number of predicted negatives that are actually positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "The top left cell is called True Negative, because their actual class is negative, and they are predicted as negative. The bottom left cell is called False negative, because they are predicted as negative, but their true class is positive. Similarly, the bottom right cell is called true positive since they are correctly predicted as positive, and the top right cell are wrongly predicted as positive so it's called false positive.\n",
    "\n",
    "False positive is also called type 1 error, and false negative is also called type 2 error. We will discuss these two types of errors later in a case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "#### Metrics Calculation\n",
    "\n",
    "<img src=\"images/confusion_matrix_raw.png\" width='500'>\n",
    "\n",
    "$Accuracy Score = \\frac{Correct Predictions}{All Predictions} = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "$Positive Precision = \\frac{True Positive}{Predicted Positive} = \\frac{TP}{TP + FP}$\n",
    "\n",
    "$Negative Precision = \\frac{True Negative}{Predicted Negative} = \\frac{TN}{TN + FN}$\n",
    "\n",
    "$Positive Recall = \\frac{True Positive}{Actual Positive} = \\frac{TP}{TP + FN}$\n",
    "\n",
    "$Negative Recall = \\frac{True Negative}{Actual Negative} = \\frac{TN}{TN + FP}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Scrip\n",
    "With the confusion matrix, we can easily calculate the classification metrics we introduced so far. \n",
    "\n",
    "For example, accuracy score which is the proportion of correct predictions, equals to true positive + true negative dvivdide by the count of all numbers in the confusion matrix.\n",
    "\n",
    "Positive recall which is the proportion of actual positive that is predicted as positive, equals to true positive divided by all positives, or the sum of second row in the confusion matrix.\n",
    "\n",
    "You don't need to remember these equations since we won't use these to calculate the metrics. But make sure you understand the relationship between the confusion matrix and the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7\n",
    "#### When to Avoid Type 1 Error(False Positive)\n",
    "- Criminal trial\n",
    "\n",
    "#### When to Avoid Type 2 Error(False Negative)\n",
    "- Cancer screening test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Scrip\n",
    "We mentioned above that False positive is also called type 1 error and false negative is also called type 2 error.\n",
    "\n",
    "Let's take a criminal trial as example. In a criminal trial, guilty conviction is positive outcome. If an innocent person is wrongly convicted, it's called false positive, or type 1 error.\n",
    "\n",
    "For type 2 error, let's think about a cancer screening test, positive means the patient has cancer. if a patient who has cancer is tested negative, or the test result doesn't find his illness, it's false negative, or type 2 error.\n",
    "\n",
    "It's clear that we'd like to avoid type 1 error by any means in a criminal trial since we don't want to convict an innocent person. Also we want to avoid type 2 error in cancer screening test because misdiagnosing a cancer patient would delay proper treatment.\n",
    "\n",
    "It's great if we can avoid type 1 error and type 2 error at the same time, but this is often very difficult. We may have to accept the trade off of the two types of errors in many cases. For example, in a criminal trial, to avoid convicting an innocent person, we may sometimes release a guilty person. In the cancer screening test, on the other hand, we may risk to classifiy a healthy person as positve to avoid missing a true cancer patient, but this is a better mistake because we can always have extra tests to confirm.\n",
    "\n",
    "In data analytics, we also face this trade off. Let's look at a case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8\n",
    "#### Case Study: Direct Mail Marking\n",
    "- Case 1: Reach to as many target cusotmers as possible(high positive recall)\n",
    "- Case 2: Reduce mails to wrong customers(high positive precision)\n",
    "<img src='https://eugeneloj.typepad.com/.a/6a00d834516e6369e20120a977b5af970b-pi' width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8 Script\n",
    "Imaging that a company which sells luxury goods and they use direct mail marketing to reach out to their target customers which are wealthy individuals, or who have high income. Their marketing director wants to build a classification model with adult income dataset to help him select customers to send the marketing mail to.\n",
    "\n",
    "Consider two cases here:\n",
    "\n",
    "In the first case, the company has enough budget for marketing, the director wants to reach out to as many wealthy individuals as possible, in another word, he wants to achive high recall rate. He's ok that some mails are sent to low income individual in the process. If high income is positive, wrongly predict high income as low income is false negative, or type 2 error. So in this case the marketing director wants to avoid type 2 error. And he's more tollerent with type 1 error, which means classify some low income individuals as high income.\n",
    "\n",
    "In the second case, the company has limited marketing budget, the company can't afford to send out too many marketing mails. So the director hopes to send mails only to those high income individuals, in another word, he wants to achive high precision rate. He wants to avoid classifying low income as high income, or type 1 error. In the process he's willing to accept the fact that he may miss some high income individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9\n",
    "#### Adjust class_weight Hyperparamter\n",
    "\n",
    "##### class_weight\n",
    "- **None**(default): give all classes same weight\n",
    "- **balanced**: give weights inversely proportional to class frequencies\n",
    "- **custom weight**: `{0:0.8, 1:0.2}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9 Script\n",
    "In scikit learn classification models, there's a hyperparamter class_weight, which is used to determine the weights associated to each outcome class. For example, we can set class_weight to a dictionary, in which we assign weight on negative class to 0.8, and weight on positive class to 0.2. This tells the model to bias toward negative outcome. In another word, the model is more likely to classify a data point as negative. A data point will be classified as positive only when the model is highly confident that it's positive.\n",
    "\n",
    "The default value of class_weight is None, or all classes have same weight. Another value is balanced, which gives class weight based on class distribution in the training dataset. For example, if there are more negative classes in the dataset, less weight will be given the negative class, or make it harder to classify a datapoint as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 10\n",
    "<img src='images/knn2.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 10 Script\n",
    "Let's use k-nearest neighbor as an example. For an imbalanced dataset, for example, 80% of the training dataset has negative outcome. For a new data point, its neighbors are more likely to be negative, if we assign same weight to all classes, assume we use 9 neighbors, we'll classfiy a new datapoint with simple majority of the neighbors. Since 80% of the dataset of the training data has negative outcome,  it's really hard to identify positive cases because we need at least 5 positive neighbors to classify it as positive.\n",
    "\n",
    "If we set class_weight to balance, now the model bias toward positive since there are less positive in the training data. Now the model may classify a new data point as positive when there're 4 or even 3 positive neighbors.\n",
    "\n",
    "By adjusting class_weight, we can achive our goal of avoiding type 1 error or type 2 error.\n",
    "\n",
    "Please read the lesson notebook for the details of the case study and associated python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 3: Introduction to Classification Evaluation II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1\n",
    "\n",
    "#### ROC\n",
    "Receiver operating characteristic\n",
    "#### AUC\n",
    "Area under curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "In this lesson, we will introduce ROC curve and AUC which can be used to evaluate classification models. \n",
    "\n",
    "ROC stands for Receiver operating characteristic, originally developed during World War Two to predict the performance of an individual using a radar system.\n",
    "\n",
    "AUC stands for area under curve, it's the area under the roc curve. The value of AUC can be used to judge a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### ROC & AUC\n",
    "<img src='images/roc.png' width=350>\n",
    "\n",
    "**True positive rate(TPR)** aka. positive recall rate.  \n",
    "The ratio of true positive over all actual positive.  \n",
    "$TPR = \\frac{True Positive}{All Actural Positive}= \\frac{TP}{TP + FN}$\n",
    "\n",
    "**False positive rate(FPR)**  \n",
    "The ratio of false positive over all actual negative.  \n",
    "$FPR = \\frac{False Positive}{All Actual Negative} = \\frac{FP}{TN + FP}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "ROC curve displays the relationship between the false positives rate and the true positives rate.\n",
    "\n",
    "In this image, the y axis represents true positive rate, which is calculated by dividing true positive by all actual positive. This is actually the positive recall rate, or the percentage of all positive classes that are predicted correctly.\n",
    "\n",
    "The x axis in the image represents false positive rate. False positive means wrongly classify negative cases as positive. So the false positive rate is calculated by deviding false positive by all negatives.\n",
    "\n",
    "You may pause the video here and make sure you understand true positive rate and false positive rate.\n",
    "\n",
    "As we saw in lesson 2 case study, we can set a hyperparamter class_weight to a classifier to adjust how likely a model classifies a data point as positive.\n",
    "\n",
    "Now let's assume we set class weight for class 0 or negative to 1, and set classweight for positive class to 0. Then  the classifier will classify all data points as negative. At this point, both true positive and false positive are 0, because there's no positive prediction at all. In the ROC plot, this is the starting point of an ROC curve, point at coordinate (0,0). To make things simple, let's assume the dataset is a balanced dataset, which means about 50% have positive class. Now let's adjust the model so that our model predict 10% of all datapoints as positive. First, let's examine a random model, which predicts with random guess, so it predicts correctly 50% of the time. For the 10% datapoints that are predicted as positive, 5% is correctly predicted, so true positive rate is 5% divided by actual positive which is 50%, since we assume the dataset is balanced. So True positive rate is 5% divided by 50% which is 0.1, similarly, false positive rate is 5% divided by all actual negatives which is also 50%, the FPR is also 0.1. If we increase the random model's positive prediction, both True positive rate and False positive rate increase at same rate. Eventually, when all datapoints are predicted as positive, now True positive rate is 1, because all actually positive are predicted as positive, on the other hand, False positive rate is also 1, becuase all actual negative are all predicted as positive. So for a random model, the ROC curve is a straight line from coordinate (0,0) to (1,1), which is the dashed diagonal line in the plot.\n",
    "\n",
    "Now let's look at a perfect model, which means it predicts perfectly. The roc curve still starts from (0,0), when we adjust class_weight so that the model predicts 10% of the dataset as positive, since the model is a perfect model, all positive predictions are correct, now True positive rate equals to 10% divided by 50%, or 0.2, and False Positive Rate is 0 because false positive is 0. When we increase positive predictions, True positive rate increases and False positive rate remains as 0, to the point when all actual positives are predicted as positive, ROC reaches to the point (0, 1). From there, when we increase positive prediction, TPR remains as 1, and FPR starts to increase, since the model starts to predict negative as positive. When the model predicts all datapoints as positive, ROC is now at point(1,1). So the ROC curve for a perfect model is the vertical and horizental blue line in the image.\n",
    "\n",
    "The ROC cuver of an actual classifier should normally fall in between the roc of the random model and that of the perfect model. In the image, the green curve line represents an ROC curve of a real classification model.\n",
    "\n",
    "The area under the ROC curve, or AUC, is a metric that indicates how good a model is. As shown in the image, AUC of a perfect model is 1X1 which is 1, AUC of a random model is 0.5. So for a classification model's AUC, the closer to 1 the better the model.\n",
    "\n",
    "To plot an ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Plot ROC\n",
    "#### Classifiers that have `decision_function`:\n",
    "- Logistic Regression\n",
    "- Support Vector Machine\n",
    "\n",
    "#### Classifiers that have `predict_proba`:\n",
    "- K-nearest Neighbors\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script\n",
    "The scikit learn module has a function roc_curve which plots roc. We need to provide true positive and false positive probabilities under different threshold or class weight. All scikit learn classification models have functions to return these probabilities. But different model use different functions.\n",
    "\n",
    "Logistic regression and support vector machine use decision_function to get the probabilities, other classification models we learned so far use predict_proba function. Please refer to the lesson notebook for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4\n",
    "#### Compare Models with ROC&AUC\n",
    "<img src='images/roc_auc.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "We can put multiple classification model's roc curve into same plot to compare models. This roc plot is for 4 classifiers on the adult income data. We can see that random forest and logistic regression have best result since their cure are closest to the perfect roc line and they have largest Area under curve, or AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1\n",
    "\n",
    "#### Regression Evaluation Metrics\n",
    "- Mean Absolute Error(MAE)\n",
    "- Mean Squared Error(MSE)\n",
    "- Root Mean Squared Error(RMSE)\n",
    "- R-Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "In this lesson, we learned different ways to evaluate a machine learning model.\n",
    "\n",
    "For a regression model, we introduced mean absolute error, mean squared error, root mean squared error and r-squared. Among them, the frist 3 are measures of errors, a smaller error indicates a better model. But it's hard to used the error alone to judge a regression model. r-squared, on the other hand, is a model's score relative to a random model, which always predicts with mean value. r-squared normally ranges from 0 to 1, the closer to 1, the better the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2\n",
    "<img src='images/residual.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "The residule vs. observed plot can help us identify the weakness in the regression model. Ideally, the residuals should distributed randomly around 0 for all observed values. If we identify a pattern, for example, as this plot shows, only negative residuals when mpg is greater than 35, it means the model does a poor job in high mpg range. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3\n",
    "\n",
    "####  Classification Evaluation Metrics\n",
    "\n",
    "- Accuracy Score\n",
    "- Classification Report\n",
    "- Confusion Matrix\n",
    " - Type 1 error(False positive)\n",
    " - Type 2 error(False negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a classification model, we introduced accuracy score, classification report and confusion matrix. Please make sure you understand the relationship between confusion matrix and classification report. You also need to understand the concept of type 1 error, or false postive and type 2 error, or false negative. \n",
    "\n",
    "Sometimes, we want to avoid type 1 error, like in a crimial trial; sometimes, we want to avoid tpyp 2 error, like the cancer screening test. You may adjust the model by setting different class weight to achieve your goals in different circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4\n",
    "\n",
    "<img src='images/roc_auc.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "\n",
    "The ROC curve is a good way to compare different models. For different classification model, you need to use different function to get true and false negative probabilities. These probabilities are needed to plot the roc curve.\n",
    "\n",
    "The assignment of this module is fairly straight forward. Please remeber to work on the problems in order.\n",
    "\n",
    "Good luck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
