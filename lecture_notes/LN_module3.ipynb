{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Fundamental Algorithms II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Slide\n",
    "### Module 3 Fundamental Algorithms II\n",
    "- K-nearest Neighbors\n",
    "- Support Vector Machine\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction Script\n",
    "Hello and welcome. \n",
    "\n",
    "(slide 1)\n",
    "In this module, you will learn 3 more supervised machine learning algorithms. \n",
    "\n",
    "Lesson1 introduces k-nearest neighbors, which makes prediction on a datapoint based on nearby training datapoints.\n",
    "\n",
    "lesson 2 introduces support vector machine, which makes predictions by finding hyperplanes in the dataset that distinctively classify the datapoints.\n",
    "\n",
    "lesson 3 introduces random forest. Which is a bagging technic that uses an ensemble of decision trees to make predictions.\n",
    "\n",
    "(slide 2)\n",
    "In the lesson notebooks, we often include some python code that create plots to help you understand some concepts. For example, in lesson 2, to demonstrate how support vector machine works, we plot a hyperplane on the iris dataset with python code. You are not required to understand every line of those python code. You should, however, understand how the algorithms work in general, and more importantly, know how to apply those algorithms using python scripts.\n",
    "\n",
    "You also need to know the key hyperparameters of each machine learning model and the value options for those hyperparameters.\n",
    "\n",
    "For each lesson, please watch the video to learn the concepts behind the algorithms, and more importantly, go through the lesson notebooks thoroughly and practice as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 1: Introduction to K-nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### K-nearest Neighbors\n",
    "\n",
    "- Supervised Learning\n",
    "- For both classification and regression problems\n",
    "- No model built(memory demanding)\n",
    "- Predict with class/value of K nearest neighbors\n",
    "- Feature Scaling is important\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "\n",
    "In this lesson, we will learn k-nearest neighbors, which is a very intuitive supervised machine learning algorithm. \n",
    "\n",
    "Most machine learning algorithms will create a model by fitting on a training dataset. For example, a linear regression model will create a model which is defined by the intercept and the coefficient values of the features. A decision tree will create a tree which is defined by the conditions in each node.\n",
    "\n",
    "K-nearest neighbors, or KNN, on the other hand, doesn't build any model from the training data. Instead, it stores all training data points in an efficient way. To classifiy a new data point, KNN simply find some training data points that are close to the new data point, and use these neighbors to predict the outcome of new data point.\n",
    "\n",
    "To find neighbors of a data point, we need to be able to calculate the distance between two data points by the features. So it's very important to scale the features when using K-nearest neighbors.\n",
    "\n",
    "KNN can be used for both classification and regression problems. We'll demonstrate how K-nearest neighbors classification works in the next slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2\n",
    "\n",
    "<img src='images/m3_knn.png' width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "\n",
    "In this image, the dots are training data points. There are two different classes in the dataset, yellow and blue. The red star is a new data point we are trying to classify.\n",
    "\n",
    "In KNN, K represents number of neighbors, which is pre-determined. Assume we choose k equals to 3. We will find 3 closest data points to the red star. In this case, the 3 closest neighbors contain 2 blue dots and 1 yellow dot, we then classify red star with the majority class, which is blue.\n",
    "\n",
    "The concept of KNN is very simple, but there are several things that are worth further discussion.\n",
    "\n",
    "The first thing is how to calculate the distance between two data points. In this image, we assume the straight line distance between two dots. This is so called euclidean distance. There are many other ways to calculate distance.No matter which way we use it's important that all features are in same scale or unit to ensure accurate calculation. So it's very import that we scale the features when we use KNN.\n",
    "\n",
    "Another thing is, KNN is a non parametric lazy learning algorithm. It doesn't create a model with parameters. Instead, it keeps all the training datapoints. In this case it stores all the dots in the model. To classify a new datapoint, KNN first calculate the distances between all trainding datapoints and the new datapoint. Then makes prediction with the k closest training datapoints. \n",
    "\n",
    "The value of k is predetermined and it's the most important hyperparameter for k-nearest neighbors.\n",
    "\n",
    "In this case we choose k equals to 3. What if we choose a different k?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3(No needed?)\n",
    "\n",
    "#### Distance Measurement\n",
    "- Euclidean\n",
    "- Manhattan\n",
    "- Minkowski\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script\n",
    "~~The first thing is how to calculate the distance between two data points. In the previous slide image, we assume distance as the straight line distance betweeb two dots. This is so called euclidean distance. There are many other ways to calculate distance. We will not discuss geometry in more detail. But one thing is clear, no matter which way we use to calculate distance, it's important that all features are in same scale or unit to ensure accurate calculation. So it's very import that we scale the features when we use KNN.~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "\n",
    "#### Choose Proper K\n",
    "- Can’t be too small\n",
    "- Can’t be too large\n",
    "- Better be odd\n",
    "\n",
    "<img src='images/m3_knn_k_value.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "\n",
    "\n",
    "In this slide, we demonstate the classifiacation with different values of k.. As shown in the image, when k equals to three, the red star is classified as blue since there are 2 blue dots and one yellow dot among the 3 closest neighbors. But when we use 6 cloeset neighbors for the classfication, the red star will be classified as yellow since now there are more yellow dots than blue dots in the 6 nearest neighbors.\n",
    "\n",
    "This shows the importance of choosing a proper k. There is a programatic way to help choose the best k which we will discuss in the future lesson. In this lesson, we will just introduce some general rules of choosing the value of k.\n",
    "\n",
    "First, k can't be too small. A too small k makes noises have big impact on the prediction.\n",
    "\n",
    "and k also can't be too large, since a too large k will bias the prediction to the majority class. For example, if we choose k equals to the total number of dots in this image, which is 13, then no matter where the new data point is, it will be classified as yellow since yellow is the majority class in the training dataset.\n",
    "\n",
    "K is also better to be odd if there are even number of classes. This is to avoid a tie. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5(Not needed?)\n",
    "#### Hyperparameters\n",
    "- `n_neighbors`: Number of neighbors to use, default is 5.\n",
    "- `metric` : the distance metric to use. The default metric is `'minkowski'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 6\n",
    "#### Scikit Learn KNeighborsClassifier model\n",
    "```\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knc = KNeighborsClassifier(n_neighbors=5)\n",
    "knc.fit(d_train, l_train)\n",
    "score = knc.score(d_test, l_test)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 script\n",
    "\n",
    "We follow the same standard way to apply a k-nearest neighbors model defined in the Scikit learn module.\n",
    "\n",
    "As mentioned above, for a knn model, we need to set the value of k, which is represented by the hyperparamter n_neighbors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 2: Introduction to Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Support Vector Machine\n",
    "- Supervised learning\n",
    "- For both classification and regression problems\n",
    "- performs classification by finding the hyperplane that maximizes the margin between the two classes\n",
    "- Feature scaling is important\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "In this lesson, we will learn support vector machine which is a supervised learning that can be used in both classification and regression problems. A Support Vector Machine performs classification by finding the hyperplane sthat maximize the margin between different classes. As k-nearest neighbors, support vector machine relies on distance calculation, so feature scaling is a must. We will demonstrate the concept of support vector machine in the next slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2\n",
    "<img src='images/svm_iris.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "Here we use a subset of the iris dataset to domonstrate support vector machine.\n",
    "\n",
    "The two species of iris, setosa and versicolor, are plotted in this two-feature space. The two features are petal width and sepal width. We can easily separate the two species visually. But how do we separate them with an algorithm?\n",
    "\n",
    "The support vector machine does this by finding a hyperplane, which is represented by the gray line in the image. There could be many different hyperplanes, we need to find the best one among them, which has the maxium margin to opposite classes. To calculate margins, we use the data points in each class that are most close to the hyperplane, those data points are called support vectors. We then define the boundary of the hyperplane with the support vectors.\n",
    "\n",
    "This dot is the support vector of the versicolor iris and it defines the upper boundary. These two dots are support vectors of the setosa iris and they define the lower boundary.\n",
    "\n",
    "By maximizing the distance of the two boundaries, we can find the best hyperplane. \n",
    "\n",
    "So in a support vector machine model, only the support vectors are important, all other training data points are neglectable. The support vector machine will classify new data points with the support vectors only. Comparing to the k nearest neighbors we introduced in lesson 1, which stores all training data in the model, support vector machine is much less demanding on storage or memory.\n",
    "\n",
    "The two classes in this example can be separated linearly or with a straight line. When we have a dataset that is not linearly dividable, we will need to transform the dataset first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Support Vector Machine Kernel\n",
    "- linear\n",
    "- rbf: radial basis function\n",
    "\n",
    "<img src='images/svm_rbf.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script\n",
    "In this slide, the left image is the presentation of the original dataset, the two classes are separated by a circle. \n",
    "\n",
    "If we transform the features of the dataset and use the distance to the center of the circle as the new x axis, and the angle with x axis as the new y axis, we will have the new presentation of the dataset as shown in the right image. Now we can use a straight line to separate the two classes.\n",
    "\n",
    "In scikit learn support vector machine model, we can enable this kind of transformation by setting hyperparameter kernel. The transformation we show in this slide can be done by a radial basis function or rbf kernel. There are other kernels which we will briefly introduce in the lesson notebook. You may choose the best kernel programatically with the help of cross validation which we will discuss in future lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4(Not needed)\n",
    "#### Hyperparameters\n",
    "- `kernel`: controls the kernel used to transform the data into a linear space. Options include `linear`, `rbf`.\n",
    "- `C`: penalty term for regularization, setting this high reduces the effects of regularization.\n",
    "- `class_weight`: determines how unbalanced classes are handled.\n",
    "- `random_state`: seed for random number generation, setting this ensures reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Scikit Learn SVC model\n",
    "```\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc = svc.fit(d_train_sc, l_train)\n",
    "score = svc.score(d_test_sc, l_test)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "\n",
    "You should be pretty familar with this piece of python code by now. SVC stands for support vector classifier. scikit learn module also defines support vector regressor which is named as SVR. The most import hyperparmeter for support vector machine model is kernel. In this sample code we choose a linear kernel. By the way, the default kernel is radial basis function or rbf kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 3: Introduction to Bagging and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Bagging\n",
    "<img src='images/dt_image.png' width=500>\n",
    "<img src='images/m3_bagging.png' width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "(slide top)\n",
    "In module 2, we introdueced decision tree. Without limiting the depth of the tree, a decision tree can classify every data point in the training dataset with a combination of conditions. However, this doesn't mean we can identify new data points accurately. This problem is very common in machine learing, when a model captures every detail in the training data including those of the noises but fails to generalize the dataset, it's called overfitting.\n",
    "\n",
    "One of the biggest issue of decision tree is overfitting. A simple approach to overcoming the overfitting problem is to train many decision trees on a subset of the dataset and average the predictions of all the trees. This process is known as bootstrap aggregation, which is often shortened to bagging.\n",
    "\n",
    "(slide bottom)\n",
    "This image demonstrates the bagging process. The big box on top represents the whole dataset. We can choose a subset of the original dataset as a new dataset. This new dataset not only has less data points, the data points in the sample also only has a subset of features. We can randomly create many subsets from the original dataset, each subset are independent to each other, which means they have different datapoints and features. \n",
    "\n",
    "One important rule is, when a data point is picked in a subset, it will remain in the original dataset so that the other subset can also pick it, this is called sampling with replacement.\n",
    "\n",
    "Next we can train many decision trees on the subsets, and use each decision tree to predict on the new data, then combine the results of all decision trees to make the final prediction.\n",
    "\n",
    "You can apply bagging with different machine learning algorithms. In this lesson, we will introduce random forest, which is implemented with many decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Random Forest\n",
    "- Supervised learning\n",
    "- For both classification and regression problems\n",
    "- Can handle categorical feature(numerical)\n",
    "- Feature scaling is not needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "\n",
    "Randome forest inherents many features from decision tree because it's ensentially a group of decision trees. It can be used on both classification and regression problems. As with decision tree, we don't need to create dummy variables for categorical features, and we don't need to scale continuous features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Random Forest Classifier\n",
    "```\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "adult_model = RandomForestClassifier(n_estimators=10)\n",
    "adult_model = adult_model.fit(d_train, l_train)\n",
    "score = adult_model.score(d_test, l_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script\n",
    "The way to apply scikit learn random forest classifier and regressor is just like that of other machine learning models we've learned. \n",
    "\n",
    "The most important random forest hyperparater is n_estimators which defines the number of decision trees in the random forest. In lesson 3 notebook we discuss briefly on the hyperparmeter values. We will learn how to choose the best values programatically in the future lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4(Not needed)\n",
    "#### Random Forest Regressor\n",
    "```\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "auto_model = RandomForestRegressor(random_state=23)\n",
    "auto_model = auto_model.fit(ind_train, dep_train)\n",
    "score = auto_model.score(ind_test, dep_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Review\n",
    "\n",
    "### Slide 1\n",
    "#### Module 3 Review\n",
    "- K-nearest Neighbors\n",
    "- Support Vector Machine\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 script\n",
    "In this module, we learned 3 more machine learning algorithms, k-nearest neighbors, support vector machine and random forest. \n",
    "\n",
    "K-nearest neighbors use closest data points to classify a new data point. There's no model built, instead, the whole training data is stored and used to classify new data points. Thus k-nearest neighbor has high demand on storage or memory. Choosing a proper k or number of neighbors is critical for a k-nearest heighbors model.\n",
    "\n",
    "On the other hand, support vector machine only stores support vectors, or the training data points that are used to defind the boundary of the hyperplanes. For a support vector machine model, the most important hyperparameter is kernel, which defines the method to separate different classes. Common kernels are linear kernel and radial basis function or rbf kernel.\n",
    "\n",
    "Both k-nearest neighbors and support vector machine require distance calculation, so feature scaling is important for the two algorithms.\n",
    "\n",
    "Random forest is an ensemble learning method which trains many decision trees on subsets of the original training dataset, then aggregates predictions of all decision tress to make final prediction. Random forest helps mitigating overfitting problem with decision tree and normally provides more accurate result. As with decision tree, we don't need to scale features or create dummy variables for categorical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2\n",
    "#### Module 3 Assignment Review\n",
    "\n",
    "```\n",
    "#1. Import metrics module\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(predicted, l_test)\n",
    "```\n",
    "\n",
    "```\n",
    "#2. Import method from metrics module\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predicted, l_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "This module's assignment asks you to construct some machine learning models and calculate accuracy score of the trained module using scikit learn metrics module. You might already know this, but I want to demonstrate different ways to call a module function here. \n",
    "\n",
    "To calculate accuracy score, for example, you can import metrics module from scikit learn, then call accuracy_score function with module name metrics as prefix. Or you can directly import acurracy_score function from scikit learn metrics module, then use the function directly.\n",
    "\n",
    "We show both ways in this slide and we also use both ways in our lesson notebooks.\n",
    "\n",
    "The assignment problems are fairly straightforward. Just remember to work on the problems in order.\n",
    "\n",
    "Good luck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
