{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5 Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1\n",
    "#### Module 5 Model Optimization\n",
    "- Lesson 1: Feature Selection\n",
    "- Lesson 2: Cross Validation\n",
    "- Lesson 3: Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "Hello and welcome. \n",
    "\n",
    "We've learned 6 supervised learning algorithms so far, we also learned the different ways to evaluate a model. It's a natual movement to the next step, model optimization.\n",
    "\n",
    "In this module, we will discuss how to improve the performance of a model. \n",
    "\n",
    "In lesson one, we will discuss feature selection. A dataset can have many features, not all of them are useful or even relavent. Selecting features properly has big impact on model performance.\n",
    "\n",
    "In lesson two, we will introduce cross validation, which helps to evaluate models more accurately. By using cross validation, we can select best model hyperparameters, which will be introduced in lesson three.\n",
    "\n",
    "As I mentioned before, for each lesson, please watch the video to learn the concepts, and more importantly, go through the lesson notebooks and practice as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 1: Introduction to Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Key Benefits\n",
    "\n",
    "- Reduces Overfitting\n",
    "- Improves Accuracy\n",
    "- Reduces Training Time\n",
    "- Improves Interpretability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "This lesson explores feature selection, which is a technique for improving the performance of machine learning algorithms by focusing on those features that contain the most predictive power.\n",
    "\n",
    "A dataset may have many features and it's natural to think that more data leads to better model. But not all features are equal. Some features may contain too much noise, some features maybe redundant, some are simply irrelavent to the objective of the analysis.\n",
    "\n",
    "This kind of features may cause overfitting, which makes the model perform well on training data, but poorly on predicting unseen data. Too many features also increases training time and makes it difficult to interpret the model.\n",
    "\n",
    "So it's critical to pick proper subset of features in a dataset to mitigate these problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "\n",
    "#### Feature selection algorithms:\n",
    "- Filter methods\n",
    "- Wrapper methods\n",
    "- Embedded methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "\n",
    "In this lesson, we will introduce three feature selection methods, filter methods, wrapper methods and embedded methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "\n",
    "#### Filter Methods\n",
    "- **Variance Threshold**\n",
    " - Rank features by variance\n",
    " - Feature Scaling is necessary\n",
    " - Doesn't use target feature\n",
    "- **Univariate Techniques**\n",
    " - Relationship between individual feature and target feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "Filter methods typically involve the application of a statistical measure to score the different features. This score allows the features to be ranked. We introduce two filter methods in this lesson.\n",
    "\n",
    "The first method is Variance Threshold. This method rank features by their variances. The reason behind this method is that features with higher variances normally contain more predictive information. An extreme case is if a feature has a constant value, its variace is 0, this feature has no predictive power at all.\n",
    "\n",
    "A feature's variance is very sensitive to the scale of the values. For example, two features that measure height of same group of people, one in centimeter and one in meter, the variance of the first feature can be thousands times larger than that of the second feature, even though the two features measure same information just in different unit.\n",
    "So it's very important to scale the features before ranking them by variance.\n",
    "\n",
    "Another filter method is univariate feature selection which involvs target feature. We all know that a feature has high predictive power if it is highly correlated to the target feature. Just like total bill in tips dataset. Univariate feature selection rank features by the strength of the relationship of the feature with the target feature.\n",
    "\n",
    "There are many statistical measures that can be used to measure the relationship. They are listed in the lesson notebook and we are not going to discuss them in more details in this video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Wrapper methods\n",
    "\n",
    "- A model is needed\n",
    "- Evaluate different feature combinations\n",
    "- More time and resource demanding\n",
    "- Recursive Feature Extraction(RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "The next feature selection method we introduce is wrapper method, which ranks features by training a model with different combination of features. This method is more expensive since a model has to be trained on each combination of features.\n",
    "\n",
    "A popular wrapper method is Recursive Feature Extraction, or RFE, which is defined in the scikit learn feature selection module. To use RFE, you need to construct a machine learning model first. You may use any model in RFE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "#### Embedded Methods\n",
    "- Rank features during training\n",
    "- Model dependent\n",
    " - Decision Tree\n",
    " - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "\n",
    "Some machine learning models, like decision tree, can evaluate feature importance during the learning process. For this kind of models, we can retrieve feature imporance from the trained model directly. This kind of method is called embedded methods. This method is very convenient to use, but not all models have the ability to rank features during training process. Among all machine learning models we've learned so far, only decision tree and random forest have this ability.\n",
    "\n",
    "The python code for all three feature selection methods are very easy to understand. Please refer to the lesson notebook for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 2: Introduction to Cross Validation\n",
    "\n",
    "Make image 1,2,3 consistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Train Test Split vs. Cross Validation\n",
    "\n",
    "<img src='https://miro.medium.com/max/2984/1*pJ5jQHPfHDyuJa4-7LR11Q.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "\n",
    "This lesson introduces cross-validation, which is a technique used to evaluate supervised learning models.\n",
    "\n",
    "In the previous lessons, we split dataset into train and test set, then train the model with the train set and evaluate the model with the test set. \n",
    "\n",
    "The problem with this approach is that the data splitting process is random, with different train test split, the model's evaluation matrics can have big difference. On the other word, the evaluation metrics are not accurate.\n",
    "\n",
    "To solve this problem, we can split the training data into multiple sub groups, train the model multiple times, each time use one sub group as validation group and other groups as train group. Use the train group to train the model and use the validation group to get evaluation metrics of the model. Then we average all the metrics to get a more accurate metric.\n",
    "\n",
    "Depending on how to split the dataset, there are many different cross validation methods. We will focus on the two most used cross validation methods, kfold and stratified kfold in this video. You can find introductions to other cross validation methods in the lesson notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2--Not needed\n",
    "#### Cross Validation\n",
    "\n",
    "- `KFold`\n",
    "- `StratifiedKFold`\n",
    "- `GroupKFold` similar to `KFold`, but limits the testing data to only one group within each fold.\n",
    "- `LeaveOneOut` iteratively leaves one observation out to validate the model trained on the remaining data.\n",
    "- `LeavePOut` iteratively leaves $P$ observations out to validate the model trained on the remaining data.\n",
    "- `ShuffleSplit` generates a user defined number of train/validate data sets, by first randomly shuffling the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### KFold\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2736/1*rgba1BIOUys7wQcXcL4U5A.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "K-fold is one of the most popular cross validation techniques. It randomly splits the dataset into k equal folds. The image in this slide demonstrates k fold with k equals to 5. The dataset is splitted to 5 equal folds. A machine learning model will be trained 5 times. In each iteration, a different fold will be the validation set or test set, and other 4 folds are train set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Stratified KFold\n",
    "<img src=\"https://image.noelshack.com/fichiers/2018/20/6/1526716452-general-tips-for-participating-kaggle-competitions-13-638.jpg\" width='500'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "\n",
    "Stratified kfold is similar to kfold, but instead of splitting the dataset randomly to k-fold, stratified kfold splits dataset in a way that each fold has same class distribution as the whole dataset.\n",
    "\n",
    "For example, as shown in this iamge, the outcome of a dataset is gender. In the original dataset there are about 3 times more male than female in the outcome class. When we split the dataset with stratified kfold, each fold will have similar outcome class distribution, or 3 times more male than female.\n",
    "\n",
    "You may ask, isn't stratified kfold always prefered to kfold? This question is a little tricky to answer. When we use stratified kfold, we assume that the class distribution of the original dataset is true. But this assumption may not hold. In most cases, we won't be able to get population data, what we have is just a sample data. The class distribution of the sample data may not refect the class distribution of the population. So if we make this assumption when splitting data, we esentially add information which may not be true to the training process. So we will need to pick cross validation method based on dataset and the problem we are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5\n",
    "#### Evaluate Model with Cross-Validation\n",
    "```\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=23)\n",
    "score = cross_val_score(adult_model, features, label, cv=skf)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5 Script\n",
    "\n",
    "Scikit learn model_selection module has functions for cross validation methods. We can get model evaluation metrics with cross_val_score function. The cross_val_score function by default returnsaccuracy score for a classification model and r-squared score for a regression model. You may set function argument to get other metric values. Please refer to lesson notebook for coding details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6(Not needed)\n",
    "#### Custom Scoring\n",
    "```\n",
    "precision_score = cross_val_score(adult_model, features, label, cv=skf, scoring='precision')\n",
    "recall_score = cross_val_score(adult_model, features, label, cv=skf, scoring='recall')\n",
    "auc_score = cross_val_score(adult_model, features, label, cv=skf, scoring='roc_auc')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Scrip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 3: Introduction to Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Model Selection\n",
    "- Use cross validation\n",
    "- Grid Search\n",
    "- Random Grid Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "\n",
    "In this lesson, we will discuss model selection techniques. Here model selection doesn't mean select the best model from different models. It actually means select proper hyperparameter values to optimize a model.\n",
    "\n",
    "We talked about the difference between model hyperparameter and model parameter before and I'd like to explain it again here. The model hyperparameters means the parameters used to construct a model, they are determined before the model is trained. For example, value of k in k nearest neighbors, number of trees in randome forest. Model parameters, on the other hand, are parameters that are determined by the training process. For example, the intercept and coefficicents of a linear regression model are model parameters.\n",
    "\n",
    "The goal of model selection is to find best hyperparamter values to construct a model. Model selection is enabled by cross validation which is introduce in the previous lesson.\n",
    "\n",
    "In this lesson, we will introduct two kind of model selection technique, Grid search and random grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Grid Search  (Shade lines while speaking)\n",
    "```\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=23)\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "# Create a dictionary of hyperparameters and values\n",
    "neighbors = [1, 3, 5, 11, 17, 23, 31, 53, 107]\n",
    "params = {'n_neighbors':neighbors}\n",
    "\n",
    "# Create grid search cross validator\n",
    "gse = GridSearchCV(estimator=knc, param_grid=params, cv=skf)\n",
    "gse.fit(features, label)\n",
    "\n",
    "best_n_neighbors=gse.best_estimator_.get_params()[\"n_neighbors\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Slide 2 Script\n",
    "\n",
    "The concept of grid search is very simple, we just define a range of values for a hyperparameter of a model, then we train the model with each of the hyperparameter value and get evaluation score with cross validation. The hyperparameter value that leads to the best evaluation score is the best hyperparameter value.\n",
    "\n",
    "In this piece of code, we first construct a stratified kfold cross validation object and a k nearest neighbor classifier, then define values of k nearest neighbor's hyperparamter n_neighbors, then apply grid search with  GridSearchCV class defined in the scikit learn model_selection module.\n",
    "\n",
    "We can get best parameter value from the best_estimator_ attribute of GridSearchCV object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3\n",
    "#### Multi-dimensional Grid Search\n",
    "```\n",
    "neighbors = [1, 3, 5, 11, 17, 23, 31, 53, 107]\n",
    "weights = ['uniform', 'distance']\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "skf = StratifiedKFold(n_splits=10, random_state=23)\n",
    "\n",
    "params = {'n_neighbors':neighbors, 'weights':weights}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script\n",
    "\n",
    "We can also apply grid search on more than one hyperparameters. For example, in this code sample, we define value ranges for two hyperparameters. Grid search will train the model with every hyperparameter value combinations. Here there are 9 values for n_neighbors adn 2 values for weights, so there are total 18 different combinations.\n",
    "\n",
    "A model may have many hyperparamters and a hyperparameter may have many different values. The number of total combinations can be a very large number which makes gridsearch very time consuming, sometimes not feasible.\n",
    "\n",
    "In that case, we can use a variation of gridsearch, which is called randomized grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4\n",
    "#### Randomized Grid Search\n",
    "```\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "skf = StratifiedKFold(n_splits=10, random_state=23)\n",
    "\n",
    "neighbors = range(1, 51)\n",
    "weights = ['uniform', 'distance']\n",
    "params = {'n_neighbors':neighbors, 'weights':weights}\n",
    " \n",
    "# Run randomized search\n",
    "rscv = RandomizedSearchCV(knc, param_distributions=params, n_iter=20, random_state=23)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "The idea of Randomized grid search is pretty simple, instead train a model on every hyperparameter combination, it randomly pick certain number of combinations to train the model. This way, the training time depends on a predefined iteration number. For example, in this code example, the n_iter is set to 20, which means, no matter how many different combinations there are, the randomzied grid search will only train the model on 20 randomly picked combinations. The best combination among these 20 will be the winner.\n",
    "\n",
    "Of course we can't find the best possible combination with randomized grid search, but this approach can achieve relatively good result with much less training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1\n",
    "#### Module 5 Review\n",
    "\n",
    "- Feature Selection\n",
    " - Filter Methods\n",
    " - Wrapper Methods\n",
    " - Embedded Methods\n",
    "- Cross Validation\n",
    "- Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Review Script\n",
    "\n",
    "In this module we introduce model optimization techniques. Feature selection is used to select best subset of features in the dataset. model selection is used to find the best model hyperparameter values. model selection is enabled by cross validation which provides a better way to evaluate a model.\n",
    "\n",
    "For feature selection, you need to understand the different methods and how to apply those methods with python code.\n",
    "\n",
    "For cross validation, you need to understand different cross validation methods and how to get cross validation scores for a model.\n",
    "\n",
    "For model selection, you need to understand the differences between model hyperparameter and model parameter. You also need to be able to perform grid search and randomized grid search to select the best model.\n",
    "\n",
    "If you understand all the python code in this module, you will have no trouble in finishing the assignment. But please remember to work on the problems in order.\n",
    "\n",
    "Good luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
