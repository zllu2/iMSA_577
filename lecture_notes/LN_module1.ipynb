{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction Script\n",
    "(Cover Slide)\n",
    "\n",
    "Hello and welcome. This is the first module of the course. This course is about machine learning algorithms and more importantly, how to train those algorithms on real world datasest with python code. When you train a model with data, it becomes a model.\n",
    "\n",
    "This module provides the basis for the rest of the course by introducing the basic concepts behind machine learning algorithms. You will be introduced with the different types of the machine learning algorithms. You will also learn the basic steps to apply machine learning models with python scikit learn library. The steps are stardard for all models.\n",
    "\n",
    "(Slide 1)\n",
    "\n",
    "There are three lessons in this module. \n",
    "\n",
    "In the first lesson, we will discuss machine learning in general. More specifically, the relationship between artificial intelligence, machine learning and deep learning. We will also introduce the machine learing algorithms you are going to learn in this course.\n",
    "\n",
    "In the second lesson, we will introduce data pre-processing, which is an integral step in machine learning and directly affects the learning abilities of the models. You will learn 3 types of data pre-processing techniques in this lesson. \n",
    "\n",
    "In the third lesson, we will demonstrate different types of machine learning models with the iris dataset. It's only a brief introduction of the models, you don't need to understand all the python code in the lesson three or how the algorithms work. We will discuss them in more detail in the following modules. \n",
    "\n",
    "(Slide 2)\n",
    "\n",
    "For each lesson, Please go through the lesson notebook briefly first, then watch the lesson video with questions in mind, and more importantly, go through the lesson notebook again after you watch the lesson video. The video explains the concepts but the most effective way to learn python data analytics is through practicing. I encourage you to  play with the code in the lesson notebooks, make modifications and rerun the code to see different results.\n",
    "\n",
    "(Slide 3)\n",
    "\n",
    "After module 1, you should be able to articulate the different types of machine learning algorithms and understand the purpose of data-preprocessing. You also need to be able to performa data-preprocessing with python code. This module's assignment is all about data-preprocessing.\n",
    "\n",
    "Please note that this module serves as an introduction to machine learning. You are not required to understand the details of the models that are demonstrated in lesson three. We are going to discuss those models in the following modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 1: Introduction to Machine Learning\n",
    "- Artificial Intelligence, Machine Learning and Deep Learning\n",
    "- Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 1 Intro Script\n",
    "In lesson 1 of the module, we will discuss the relationship between artificial intelligence, machine learning and deep learning. We will also introduce the machine learing algorithms you are going to learn in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Artificial Intelligence, Machine Learning and Deep Learning\n",
    "<img src=\"https://www.mytectra.com/media/wysiwyg/Blog/deep-learning.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "\n",
    "I bet you've also heard the terms like artificial intelligence or AI and deep learning along with machine learning. Many people use these three terms interchangably. But there's a reason why our course is not called AI for accountting.\n",
    "\n",
    "Artificial intelligence was firts brought up in 1950s. Roughly speaking, AI is a system that mimics the cognitive functions of humans and carries out tasks in an “intelligent” manner like a human. \n",
    "\n",
    "(Show c3po and terminator)\n",
    "\n",
    "When we talk about AI, c3po or terminator always come up in mind. They talk, walk and behave like a human, and some times even express opinions or show emotions. They are examples of so called strong AI or general AI. We are not able to establish strong AI yet. All AI we currently have are so call narrow AI or weak AI, which can handle perticular tasks, like a self-driving car.\n",
    "\n",
    "(show self-driving car)\n",
    "\n",
    "So AI is a whole system, but to enable AI to carry out certain tasks, we rely on machine learning to make data driven decisions. Machine Learning is an algorithm or a technique of parsing data, learn from that data and then apply what they have learned to make an informed decision.\n",
    "\n",
    "(Show a self driving car animation)\n",
    "\n",
    "For example, a self-driving car will collect surrounding data, via camera, radar and gps, then process these data with machine learning algorithms and make decisions on how to maneuver the vehicle.\n",
    "\n",
    "Deep learning is actually a special kind of machine learning that is completely based on artificial neural networks, which mimics the human brain. Deep learning requires tramendous processing power and as the processing power increases exponentially in the last decade, deep learning came into the picture. The self-driving car we mentioned above uses deep learning to process data and make decisions.\n",
    "\n",
    "While deep learning already has some impacts on accounting, like transcripts of conference calls, it still has relatively low adoptions in accounting world, partly due to the lack of interpretability of artificial neural networks. So in this course, we will focus on other machine learning algorithms and their applications in accounting field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Machine Learning\n",
    "Tow main tasks in machine learning field:\n",
    "- #### Supervised Learning\n",
    "Ground truth is available. The algorithms are provided with true outputs for a give inputs.\n",
    " - Classificaiton\n",
    " - Regression\n",
    "- #### Unsupervised Learning\n",
    "No corresponding output. The algorithms identify patterns and features from the data directly.\n",
    " - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "There are two main types of machine learning algorithms: supervised learning and unsupervised learning.\n",
    "\n",
    "In supervised learning, the model or the algorithm is provided with ground truth or true output for given input. For example, fraudulent transaction detection can be implemented with a supervised learning model. The inputs are transaction details such as amount, time, location, merchant etc. The output is whehter a transaction is valid or not, which we already know for past transactions. We then feed the inputs and the outputs to a machine learning model to train the model. After the model is trained with the past data, it can be used to evaluate new transactions at real time and predict whether they are fraudulent or not.\n",
    "\n",
    "We are going to introduce two kinds of supervised learning in this course, classification and regression. Classification is used when the output is discrete. Like in the fraud detection, the output is either fraudulent or not fraudulent. Regression is used when the output is continous. Like predicting sales or profits based on market conditions. \n",
    "\n",
    "In unsupervised learning, corresponding output is not available. The model tries to identify patterns and features from the data directly. For example, unsupervised learning can be used to divide sustomers into different groups by their profiles and purchasing habits. This kind of customer segmentation can help merchants to better serve their customers or promote their products more effectively.\n",
    "\n",
    "We are going to introduce clustering for unsupervised learning in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Machine Learning Algorithms in this Course\n",
    "- Classification\n",
    "- Regression\n",
    "- Clustering\n",
    "- Text Analysis\n",
    "- Time Series Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "In this course, we are going to learn these 5 types of machine learning algoritms. Classification and Regression are supervised learning algorithms. Clustering is an unsupervised learning algorithm. \n",
    "\n",
    "Text analysis is a special kind of classfication problem. We introduce it separately because unlike normal classifications, the training data of text analysis is a collection of texts, like customer reviews or newspaper articles. Machine learning models can only deal with numerical data. So we have to first manipulate the text dataset and convert it to numeric data, then apply classification.\n",
    "\n",
    "Time series analysis is another kind of special machine learning algorithm. It's called time series analysis because the data is indexed by time. Like daily stock price, hourly weather conditions etc. Time series analysis tries to find the trend over time, and predict future values based on previous information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 2: Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "<img src='images/CRISP-DM_Process_Diagram.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "\n",
    "This lesson is about data pre-processing. To understand data pre-processing let's review the crisp-dm framework first. In the framework, the third step is data preparation. We clean up the data, handle missing values, merge various data sources together in this step. After this step we will have a clean dataset. But before we can feed the dataset to a machine learning model, we still need to pre-process the data. Data pre-processing is often misunderstood as part of data preparation. But the fact is, data pre-processing is part of modeling because it depends on the machine learning models. Different machine learning models require different data pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "\n",
    "#### Data Pre-processing Techniques\n",
    "\n",
    "- Categorical Variables Encoding\n",
    "- Dataset Splitting\n",
    "- Data Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "In this lesson, we will introduce three data pre-processing techniques.   \n",
    "Categorical Variable Encoding  \n",
    "Dataset Splitting, and  \n",
    "Data Scaling  \n",
    "\n",
    "Among them, dataset splitting is for supervised learning only. Other two are for both supervised and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Categorical Variables\n",
    "- **Nominal**: \n",
    "Categories have no numerical orders. ie. Gender.\n",
    "- **Ordinal**: \n",
    "Categories have numerical orders. ie. ranking in a race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "The first data pre-processing technique we introduce is categorical variable encoding. \n",
    "\n",
    "There are two types of categorical variables, nominal and ordinal. \n",
    "\n",
    "A nominal variable has no intrinsic order in its categories. For example, gender is a nominal categorical variable because the two categories (male and female) have no intrinsic order. Some other nominal variable examples are day of week, country of origin, color of shirt, etc.\n",
    "\n",
    "An ordinal variable, on the other hand, has some sort of order. For example, ranking in a race, first place, second place and third place, there's an intrinsic order in the categories. Some other ordinal variable examples are passenger class, shirt or shoe size etc.\n",
    "\n",
    "Categorical variables often have text values. But all machine learning algorithms can only work with numeric data, so we will have to encode categorical variables into numeric values.\n",
    "\n",
    "For different type of categorical variables and machine learning models, there are many different encoding techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Categorical Variable Encoding\n",
    "- **Label Encoding**: Encode categories to unique numeric values.\n",
    "- **One Hot Encoding**: Create a dummy variable with value 0 and 1 for each category value.\n",
    "<img src='https://i.imgur.com/mtimFxh.png' width=500>\n",
    "<img src='https://miro.medium.com/max/2736/0*T5jaa2othYfXZX9W.' width=500>\n",
    "- **Ordinal Encoding**: Encode categories to ordered numerica values. Small->0, Medium->1, Large->2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "We introduce 3 most common encoding techniques in this lesson.\n",
    "\n",
    "The simplest approach is label encoding, which converts each category into a unique numeric value.\n",
    "One hot encoding creates extra dummy variables for a categorical variable. \n",
    "\n",
    "ordianl encoding maps a category value to a perticular numeric value based on its characteristic.\n",
    "\n",
    "### Slide 5 Script\n",
    "\n",
    "In this slide we compare label encoding with one hot encoding. The original dataset has a categorical variable food, which has three categories, apple, beef and cabbage.\n",
    "\n",
    "With label encoding, the categories are mapped to interger 0, 1 and 2. The map between category value and numerical value is somewhat random. Normally, alphabetical order of category values is used to determine which numerical value a category is mapped to. In this case apple is mapped to 0, beef to 1 and cabbage to 2.\n",
    "\n",
    "Label encoding is straightforward but it has an apparent problem. It introduces an order into the category values. Some machine learning algorithms are sensitive to the values, like linear regression. For this kind of algorithms, one hot encoding normally provides a better learning result. \n",
    "\n",
    "With one hot encoding, extra dummy variables are created. One dummy variable is created for each cagtegory value. In this example, three dummy variables are created, food-apple, food-beef and food-cabbage. A dummy variable only has value 0 or 1. So for apple, only food-apple has value 1. With one hot encoding, we don't introduce extra information into the dataset.\n",
    "\n",
    "The disadvantage of one hot encoding is that it increases the dataset size, espeically when there are a lot of categorical variables with many different categories. Some machine learning algorithms don't rely on values of categorical variables, like decision tree or random forest. For these kinds of models, label encoding is good enough. For some other algorithms, like linear regression, which is sensitive to the categorical values, one hot encoding normally provides better result.\n",
    "\n",
    "### Slide 6 Script\n",
    "Ordinal encoding is used to encode ordinal categorical variables. With ordianl encoding, a category value is mapped to a perticular numeric value based on its characteristic. For example, in this slide, we map size small to 0, medium to 1 and large to 2.\n",
    "\n",
    "Ordinal encoding has the same problem that label encoding has. In this example, even though large is bigger than medium, it's not necessarily 2 times of medium as the numeric values indicate. When working with algorithms that are sensitive to category values, we still prefer one hot encoding over ordinal encoding for ordinal categorical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5\n",
    "#### Dataset Splitting\n",
    "- For supervised learning\n",
    "- Split dataset to training and testing set\n",
    "- Use the training set to train the model\n",
    "- Use the testing set to evaluate the model\n",
    "\n",
    "<img src='https://www.researchgate.net/profile/Brian_Mwandau/publication/325870973/figure/fig6/AS:639531594285060@1529487622235/Train-Test-Data-Split.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5 Script\n",
    "\n",
    "The second data pre-processing technique is dataset splitting which is for supervised learning only. Dataset splitting divides the dataset into two subsets, training and test. Training data set is used to to train the model. Test set is used to evaluate the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "<img src='images/data_split_code.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "\n",
    "To split a dataset, we first import train_test_split function from the scikit learn model selection module. In thiw example, we pass the data and the label to the function to split. The third argument is the test size, which defines the proportion of the dataset to be included in the test set. Test size should be between 0 and 1. In this case the test size is 0.4 which means 40% of the dataset will be included in the test set and 60% will be in the training set. train_test_split function will split the dataset randomly, so every time you run the code, you'll get different training and tes set. To ensure that we get same result every time, we can set random_state to a particular value. The value of random_state can be any integer. Now when you run the code multiple times, every time you will get same training and test set. \n",
    "\n",
    "train_test_split will return a pair of data object for each data object passed to the function. In this case, we pass two data objects, data and label to the function, so the function returns 4 objects, the first two are training and test set for data, the next two are training and test set for label.\n",
    "\n",
    "After the data and the label are splitted, we can use d_train and l_train to train a model, then use d_test and l_test to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 7\n",
    "#### Data Scaling\n",
    "- Standardizing: zero mean and one standard deviation\n",
    "- Normalization: scale to same range, ie 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 7 Script\n",
    "\n",
    "The third data-preprocessing technique is data sacaling.\n",
    "\n",
    "Many machine learning algorithms are sensitive to the spread of features. For example, an algorithm might give more weight on features with a larger spread, even if this produces a sub-optimal result. To prevent this kind of problems, we scale the features.\n",
    "\n",
    "We introduce two types of data scaling in this lesson, standardizing and normalization.\n",
    "\n",
    "Standardizing scales data to mean 0 and standard deviation 1. Use standardizing when the data is normally distributed.\n",
    "\n",
    "Normalization scales data to a particular range, normally from 0 to 1. ~~Normalization doesn't work well when there're outliers in the dataset. Assume majority of the data is in range 1-100, and only a few data points are in million or billion range, if we normalize the data to 0 to 1 range, most datapoints are mapped to close to 0 due the the very large outliers. In this case, we could eliminate outliers before applying normalization, or use standardization.~~\n",
    "\n",
    "Scaling is not required for all machine learning algorithms. For example, for decision tree or random forest, we don't need to scale the data. We will discuss scaling in more details in the future lessons when we introduce the specific machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8\n",
    "<img src='images/standardizing_code.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8 Script\n",
    "\n",
    "We will use standardizing to demonstrate the data scaling process with python.\n",
    "\n",
    "To standardize data, we will use the StandardScaler in scikit learn preprocessing module. We first create standardscaler object; then train the standardscaler object with the training data. This is to learn learn the distribution of the data, then we call transform() function to transform both the training dataset and test dataset.\n",
    "\n",
    "The reason we only fit the scaler object with the training set is that the test set is used for evaluation only. The distribution of the test set should not be used to transform the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 3: Introduction to Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "\n",
    "#### Iris Dataset\n",
    "<img src='images/iris_dataset.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro Script\n",
    "\n",
    "In this lesson, we will demonstrate how to apply machine learning algorithms with python scikit learn module.\n",
    "We will use the iris dataset to demonstrate three types of machine learning algorithms, classification, regression and clustering.\n",
    "\n",
    "### Slide1 Script\n",
    "\n",
    "\n",
    "The Iris dataset is a seaborn built-in dataset. The data set consists of 50 samples from each of three species of Iris. Four features were measured from each sample: the length and the width of the sepals and the petals, in centimeters.\n",
    "\n",
    "The species column has three different values, setosa, virginica and versicolor. It is the output in the classification example. We will encode the species column to map the species to numeric values. Since this column serves as the classification output, we will use label encoder to encode it. The 4 iris feature columns will be scaled before feeding to a machine learning model. We will also split the dataset to training and test set for the classfication and regression examples.\n",
    "\n",
    "For the clustering example, we will only use the 4 feature columns, since there's no output involved in clustering. We don't need to split the dataset for clustering since it's unsupervised learning, but we still need to scale the 4 feature columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1(Not needed)\n",
    "#### Modeling with Scikit-Learn\n",
    "- Pre-process data\n",
    "- Create Model\n",
    "- Train the model\n",
    "- Evaluate the model\n",
    "- Predict with the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "\n",
    "Python scikit learn library has various machine learning algorithms and all of them have similar interface, which means we can follow the same steps when applying different algorithms. The steps are:\n",
    "- first, Pre-process the data\n",
    "- then Create a Model\n",
    "- then Train the model\n",
    "- then Evaluate the model\n",
    "- finally Predict with the model\n",
    "\n",
    "For classification, we demonstrate K-nearest neighbors classifier; for regression, we demnonstrate decision tree regressor; for clustering, we demonstrate kmeans algorithm. Don't worry if you don't know anything about these algorithms because we will learn the algorithms in the following lessons. In this lesson, just understand the basic steps of applying scikit learn machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2\n",
    "\n",
    "#### Modeling with Scikit-Learn\n",
    "\n",
    "<img src='images/mla_code.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "\n",
    "It's very simple to perform machine learning with python. Assume you already pre-processed the data, you normally just need several lines to apply a machine learning model on the data, as shown in this slide.\n",
    "\n",
    "This is a sample code to apply a classification with k nearest neighbors classifier.\n",
    "You first construct the model with model specific parmaters, then you train the model on training dataset by calling the model's fit() function. Then predict with the trained model by calling the predict() function. \n",
    "\n",
    "You can apply all machine learning models defined in the scikit learn module with similar apporach.\n",
    "\n",
    "Don't worry if you don't know any thing about the algorithms demonstrated in this lesson. We are going to discuss these machine learning algorithms in detail in the following modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Review Script\n",
    "\n",
    "In this lesson you learned some basic concept behind machine learning algorithms. In lesson 3 we demonstrate how to use python scikit learn library to do classification, regression and clustering. You are not required to understand the details of the algorithms demonstrated since you are going to learn them in the following lessons. You need to understand the concept of data pre-processing, and know how to pre-process data with python scripts introduced in lesson 2.\n",
    "\n",
    "Now let's take a look at module 1 assignment.\n",
    "\n",
    "Since this is the first module of the course, I’d like to explain the assignment in general a bit. The assignment is in exactly same format as that in the previous course, Accounting Data Analytics with Python. \n",
    "\n",
    "(open assignment 1)\n",
    "\n",
    "The first markdown cell lists some important information about the assignment. Please read it carefully. \n",
    "\n",
    "You need to run the first code cell which import modules needed by the assignment.\n",
    "\n",
    "You should only write your answers under the text your code here, not anywhere else. \n",
    "\n",
    "The assignments are autograded. Each problem in the assignment has an autograder cell below the answer cell. If your solution is wrong, the autograder of the problem will display error messages when you run it.\n",
    "\n",
    "When you finish all assignment problems, run the whole notebook by clicking Kernel-Restart and Run all. If the notebook runs to the last code cell without error massage, you know you’ve answered all problems correctly. \n",
    "\n",
    "Before you submit your assignment, make sure you save your assignment, either by clicking the save icon or from the menu, click File and then Save and Checkpoint.\n",
    "\n",
    "The first module's assignment is all about data pre-processing. All the problems are fairly straightforward. One thing worth mentioning is that later problems have dependency on previous problems. So just remember to work on the problems in order.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
