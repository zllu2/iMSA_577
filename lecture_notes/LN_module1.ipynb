{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction Script\n",
    "Hello and welcome. This is the first module of the course. This course is about machine learning algorithms and more importantly, how to apply those algorithms on real world dataset with python scripts. This module provides the basis for the rest of the course by introducing the basic concepts behind machine learning algorithms. You will be introduced with the types of the machine learning algorithms you are going to learn. You will also learn the basic steps to apply machine learning algorithms with python scikit learn library. The steps are very simple and stardard for all algorithms. Before applying machine learning algorithms, however, we need to pre-process the data. You will learn 3 types of pre-processing techniques.\n",
    "\n",
    "There are three lessons in this module. \n",
    "\n",
    "In the first lesson, we will discuss what machine learning is. More specifically, the difference between some related buzzwords, AI or artificial intelligence, machine learning and deep learning. We will also discuss some applications of machine learning in accounting domain. \n",
    "\n",
    "In the second lesson, we will introduce data pre-processing technics with python. Data pre-processing is an integral step in machine learning and directly affects the learning abilities of the models. \n",
    "\n",
    "In the third lesson, we will demonstrate different types of machine learning algorithms with iris dataset. It's a breif introduction of the algorithms. We will discuss them in more detail in the following modules. \n",
    "\n",
    "Please go through the lesson notebooks briefly first, then watch the lesson videos with questions in mind, and more importantly, go through the lesson notebooks again carefully after you watch the lesson videos. The videos explain the concepts but the most effective way to learn python data analytics is through practicing. I encourage you to run the code in the notebooks, you may even play with the code, make modifications and rerun the code to see different results.\n",
    "\n",
    "Please note that this module serves as an introduction to machine learning algorithms. You are not required to understand the details of the algorithms that are demonstrated in the lesson notebooks. You will learn those algorithms in more detail in the future modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 1: Introduction to Machine Learning\n",
    "- Artificial Intelligence, Machine Learning and Deep Learning\n",
    "- Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Artificial Intelligence, Machine Learning and Deep Learning\n",
    "<img src=\"https://www.mytectra.com/media/wysiwyg/Blog/deep-learning.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "\n",
    "(face)\n",
    "The name of this course is Machine learning for accounting. I bet you've also heard the terms like artificial intelligence or AI and deep learning. Many people use these three terms interchangably. But there's a reason why our course is not called AI for accoutting.\n",
    "\n",
    "(slide 1)\n",
    "Artificial intelligence was firts brought up in 1950s. Roughly speaking, AI is a system that mimics the cognitive functions of humans and carry out tasks in an “intelligent” manner like a human. When we talk about AI, c3po or terminator always come up in mind. They talk, walk and behave like a humen, and some times even express opinions or show emotions. They are examples of so called strong AI or general AI. We are not able to establish strong AI in reality yet. All AI we currently have are so call narrow AI or weak AI, which can handle perticular tasks, like a self-driving car.\n",
    "\n",
    "To enable AI to carry out certain tasks, we rely on machine learning to make data driven decisions. Machine Learning is an algorithm or a technique of parsing data, learn from that data and then apply what they have learned to make an informed decision.\n",
    "\n",
    "(Show a self driving car animation)\n",
    "\n",
    "For example, a self-driving car will collect surrounding data, via camera, radar and gps, then process these data with machine learning algorithms and make decisions on how to manuever the vehicle.\n",
    "\n",
    "Deep learning is a special kind of machine learning which is completely based on artificial neural networks, which mimics the human brain. Deep learning requires tramendous processing power and as the processing power increases exponentially in the last decade, deep learning came in the picture. The self-driving car we mentioned before is an example of deep learning application.\n",
    "\n",
    "While deep learning already has some impact on accounting, like transcripts of conference calls, it still has relatively low adoptions in accounting world, partly due to the lack of interpretability of deep learning algorithms. In this course, we will focus on other machine learning algorithms and their applications in accounting field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Machine Learning\n",
    "Tow main tasks in machine learning field:\n",
    "- #### Supervised Learning\n",
    "Ground truth is available. The algorithms are provided with true outputs for a give inputs.\n",
    " - Classificaiton\n",
    " - Regression\n",
    "- #### Unsupervised Learning\n",
    "No corresponding output. The algorithms identify patterns and features from the data directly.\n",
    " - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "There are two main types of machine learning: supervised learning and unsupervised learning.\n",
    "\n",
    "In supervised learning, the model or the algorithm is provided with groud truth for given inputs. For example, fraudulent transaction detection can be implemented with a supervised learning model. The inputs are transaction details such as amount, time, location, machant etc. We can train a machine learning model with past transactions, for which we already know whehter they are valid or not. After the model is trained with past data, it can be used to evaluate new transactions at real time and predict whether they fraudulent or not.\n",
    "\n",
    "We are going to introduce two kinds of supervised learning in this course, classification and regression. Classification is used when the output is discrete. Like the fraud detection, the output is either fraudulent or not fraudulent. Regression is used when the output is continous. Like predicting sales or profits based on market conditions. \n",
    "\n",
    "In unsupervised learning, corresponding output is not available. The model tries to identify patterns and features from the data directly. For example, unsupervised learning can be used to segment sustomers into different groups by their profiles and purchasing habits. This kind of customer segmentation can help merchants to better serve their customers or promote their products more effectively.\n",
    "\n",
    "We are going to introduce clustering for unsupervised learning in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Machine Learning Algorithms in this Course\n",
    "- Classification\n",
    "- Regression\n",
    "- Clustering\n",
    "- Text Analysis\n",
    "- Time Series Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "In this course, we are going to learn these 5 types of machine learning algoritms. Classification and Regression are supervised learning algorithms. Clustering is unsupervised learning algorithm. \n",
    "\n",
    "Text analysis is a special kind of classfication problem. We introduce it separately because unlike normal classifications, the training data of text analysis is a collection of texts, like customer reviews or newspaper articles. Machine learning models can only deal with numerical data. So we have to first manipulate the text dataset and convert it to numeric data, then apply classification algorithms.\n",
    "\n",
    "Time series analysis is another kind of special machine learning algorithm. It's called time series analysis because the data is indexed by time. Like daily stock price, hourly weather conditions etc. Time series analysis tries to find the trend over time, and predict future values based on previous information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 2: Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "<img src='images/CRISP-DM_Process_Diagram.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "\n",
    "This lesson is about data pre-processing. To understand data pre-processing let's review the crisp-dm framework first. In the framework, the third step is data preparation, in which we clean up the data, handle missing values, merge various data sources together. After this step we will have a clean dataset that is ready to be applied on machine learning models. Data pre-processing is often misunderstood as part of data preparation. The fact is, data pre-processing is part of modeling because it depends on the machine learning model that is in use. Some data pre-processing techniques are for supervised learning, some are for both supervised and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "\n",
    "#### Data Pre-processing Techniques\n",
    "\n",
    "- Categorical Variables Encoding\n",
    "- Dataset Splitting\n",
    "- Data Scaling and Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "In this lesson, we will introduce three data pre-processing techniques.   \n",
    "Categorical Variable Encoding  \n",
    "Dataset Splitting, and  \n",
    "Data Scaling  \n",
    "\n",
    "Among them, dataset splitting is for supervised learning only. Other two are for both supervised and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Categorical Variables\n",
    "- **Nominal**: \n",
    "Categories have no numerical orders. ie. Gender.\n",
    "- **Ordinal**: \n",
    "Categories have numerical orders. ie. ranking in a race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "The first data pre-processing technique we introduce is categorical variable encoding. Categorical variables are very common in machine learning datasets. There are two types of categorical variables, nominal and ordinal. \n",
    "\n",
    "A nominal variable has no intrinsic order in its categories. For example, gender is a categorical variable because the two categories (male and female) have no intrinsic order. Some other nominal variable examples are day of week, country of origin, color of shirt, etc.\n",
    "\n",
    "An ordinal variable, on the other hand, has some sort of order. For example, ranking in a race, first place, second place and third place, there's intrinsic order in the categories. Some other ordinal variable examples are passenger class, shirt or shoe size etc.\n",
    "\n",
    "Categorical variables often have text values. Since all machine learning algorithms can only work with numeric data, we will have to encode categorical variables into numeric values.\n",
    "\n",
    "For different type of categorical variables and machine learning models, there are many different encoding techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Categorical Variable Encoding\n",
    "- **Label Encoding**: Encode categories to unique numeric values.\n",
    "- **One Hot Encoding**: Create a dummy variable with value 0 and 1 for each category value.\n",
    "<img src='https://i.imgur.com/mtimFxh.png' width=500>\n",
    "<img src='https://miro.medium.com/max/2736/0*T5jaa2othYfXZX9W.' width=500>\n",
    "- **Ordinal Encoding**: Encode categories to ordered numerica values. Small->0, Medium->1, Large->2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "We introduce 3 most common types of encoding techniques in this lesson.\n",
    "\n",
    "The simplest approach is label encoding, which converts each category into a unique numeric value. Normally, if there are n different categories, they will be mapped to numbers from 0 to n-1. For example, if there's a color variable with 3 categories, red, yellow and blue, label encoding will map the three colors to 0, 1 and 2. With label encoding, the map between category value and numeric value is somewhat random. Normally, alphabetical order of category values is used to determine which numeric value a category is mapped to. In the color case, blue is normally mapped to 0, red is mapped to 1 and yellow is mapped to 2 due to alphabetical order of the three words.\n",
    "\n",
    "### Slide 5 Script\n",
    "Label encoding is straightforward but it has an apparent problem. It may mislead machine learning algorithms with the numeric values since it introduces an order into the category values. Some machine learning algorithms are sensitive to categorical values, like linear regression. For this kind of algorithms, one hot encoding normally provides a better learning result. One hot encoding creates extra dummy variables for a categorical variable. One dummy variable is created for each cagtegory value. For example, for the color example, one hot encoding will create three dummy variables, namely red, yellow and blue, one for each color. A dummy variable only has value 0 or 1. For example, if a data point has color yellow, the values in the 3 dummy variables red, yellow and blue will be 0, 1 and 0. Each data point will have only one value 1 in the three dummy variables. With one hot encoding, we don't introduce extra information into the dataset.\n",
    "\n",
    "The disadvantage of one hot encoding is that it increases the dataset size, espeically when there are a lot of categorical variables with many different categories. Some machine learning algorithms don't rely on values of categorical variables. For those algoritms, like decision tree or random forest, label encoding is good enough. For some other algorithms, like linear regression, which is sensitive to the categorical values, one hot encoding normally provides better result.\n",
    "\n",
    "### Slide 6 Script\n",
    "Another encoding is ordinal encoding, which is used to encode ordinal categorical variables. With ordianl encoding, a category value is mapped to a perticular numeric value based on its characteristic. For example, we can map small to 0, medium to 1 and large to 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5\n",
    "#### Dataset Splitting\n",
    "- For supervised learning\n",
    "- Split dataset to training and testing set\n",
    "- Use the training set to train the model\n",
    "- Use the testing set to evaluate the model\n",
    "\n",
    "<img src='https://www.researchgate.net/profile/Brian_Mwandau/publication/325870973/figure/fig6/AS:639531594285060@1529487622235/Train-Test-Data-Split.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5 Script\n",
    "\n",
    "The second data pre-processing technique is dataset splitting. Dataset splitting is used to evaluate supervised learning models so it's for supervised learning only. We split the original dataset into two datasets, training and testing. We use the training data set to train the model, then apply the trained model on the inputs of testing dataset to get predictions, compare the predictions and the true output of testing dataset to evaluate the accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "<img src='images/data_split_code.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "\n",
    "To split dataset, we first import train_test_split function from scikit learn model selection module. The first two values we pass to the function are data and label to be splitted. The two variables should be prepared before we can train_test_split, here we just assume we already have them ready. The third value we pass to the function is test size, which defines the proportion of the dataset to include in the test set. Test size should be between 0 and 1. In this case we pass 0.4 which means 40% of the dataset will be included in the testing set and 60% will be in the training set. train_test_split function will split the dataset randomly, so every time you run the code, you'll get different training and testing set. To ensure that we get same result, we can random_state to the function. The value of random_state can be any integer. In this case we set random_state to 23. Now when you run the code multiple times, every time you will get same training and testing set. \n",
    "\n",
    "train_test_split will return a pair of data object, one train one test, for each data object passed to the function. In this case, we pass two data objects, data and label to the function, so the function returns 4 objects, the first two are train and test for data, the next two are train and test for label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 7\n",
    "#### Data Scaling\n",
    "- Standardizing: zero mean and one standard deviation\n",
    "- Normalization: scale to same range, ie 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 7 Script\n",
    "Many machine learning algorithms are sensitive to variations in the spread of features. For example, an algorithm might give more weight on features with a larger spread, even if this produces a sub-optimal result. To prevent this kind of problems, we scale the features.\n",
    "\n",
    "We introduce two types of scaling in this lesson, standardizing and normalization.\n",
    "\n",
    "Standardizing scales data to mean 0 and standard deviation 1. Use standardizing when the data is normally distributed.\n",
    "\n",
    "Normalization scales data to a particular range, normally from 0 to 1. Normalization doesn't work well when there're outliers in the dataset. Assume majority of the data is in range 1-100, and only a few data points are in million or billion range, if we normalize the data to 0 to 1 range, most datapoints are mapped to close to 0 due the the very large outliers. In this case, we could eliminate outliers before applying normalization, or use standardization.\n",
    "\n",
    "Scaling is not required for all machine learning algorithms. For example, for decsition tree or random forest, we don't need to scale the data. We will discuss scaling in more details in future lessons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8\n",
    "<img src='images/standardizing_code.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8 Script\n",
    "To standardize data, we will use the StandardScaler in scikit learn preprocessing module. We first call fit() function to let StandardardScaler to learn the distribution of the data, then we call transform() function to transform the data, in this case, to 0 mean and 1 standard deviation values. transform() returns a numpy array of scaled data.\n",
    "\n",
    "Scaling are for both supervised and unsupervised learning. When you scale data for supervised learning, you will need to split the dataset to train and test first, then call fit() function on training dataset, then transform both training and testing dataset. Just like this piece of code shows. The reason is, the testing dataset is used to evaluate the model, so its distribution should not be used to transform datasets.\n",
    "\n",
    "We use MinMaxScaler to normalize dataset in lesson notebook, which works same way as StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 3: Introduction to Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Modeling with Scikit-Learn\n",
    "- Pre-process data\n",
    "- Create Model\n",
    "- Train the model\n",
    "- Evaluate the model\n",
    "- Predict with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "In this lesson, we demonstrate modeling with python scikit learn library. We use a python built-in dataset, iris dataset to demonstrate how to perform classification, regression and clustering. Classification and regression are supervised learning and clustering is unsupervised learning.\n",
    "\n",
    "Python scikit learn library has various machine learning algorithms. The best part is, all algorithms have same interface, which means we can follow same steps when applying different algorithms. The steps are:\n",
    "- first, Pre-process data\n",
    "- then Create Model\n",
    "- then Train the model\n",
    "- then Evaluate the model\n",
    "- finally Predict with the model\n",
    "\n",
    "For classification, we demonstrate K-nearest neighbors classifier; for regression, we demnonstrate decision tree regressor; for clustering, we demonstrate kmeans algorithm. Don't worry if you don't know anything about these algorithms because we will learn the algorithms in the following lessons. In this lesson, just understand the basic steps of applying scikit learn machine learning models and appreciate how simple they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2\n",
    "\n",
    "#### Modeling with Scikit-Learn\n",
    "\n",
    "<img src='images/mla_code.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2 Script\n",
    "\n",
    "Python makes it extremely simply to apply machine learning models. Assume you already pre-processed the data, you normally just need several lines to apply a machine learning model on the data, as shown in this slide.\n",
    "\n",
    "You first construct the model with model specific parmaters, then you train the model on training dataset by calling the fit() function. The predict with the model by calling the predict() function. \n",
    "\n",
    "You don't have to understand all the code in this lesson since we are going to discuss the machine learning algorithms in detail in the following modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Review Script\n",
    "\n",
    "In this lesson you learned some basic concept behind machine learning algorithms. In lesson 3 we demonstrate how to use python scikit learn library to do classification, regression and clustering. You are not required to understand the details of the algorithms demonstrated since you are going to learn them in the following lessons. You need to understand the concept of data pre-processing, and know how to pre-process data with python scripts introduced in lesson 2.\n",
    "\n",
    "Now let's take a look at module 1 assignment.\n",
    "\n",
    "Since this is the first module of the course, I’d like to explain the assignment in general a bit. The assignment is in exactly same format as that in the previous course, Accounting Data Analytics with Python. \n",
    "\n",
    "(open assignment 1)\n",
    "\n",
    "The first markdown cell lists some important information about the assignment. Please read it carefully. \n",
    "\n",
    "You need to run the first code cell which import modules needed by the assignment.\n",
    "\n",
    "You should only write your answers under the text your code here, not anywhere else. \n",
    "\n",
    "The assignments are autograded. Each problem in the assignment has an autograder cell below the answer cell. If your solution is wrong, the autograder of the problem will display error messages when you run it.\n",
    "\n",
    "When you finish all assignment problems, run the whole notebook by clicking Kernel-Restart and Run all. If the notebook runs to the last code cell without error massage, you know you’ve answered all problems correctly. \n",
    "\n",
    "Before you submit your assignment, make sure you save your assignment, either by clicking the save icon or from the menu, click File and then Save and Checkpoint.\n",
    "\n",
    "The first module's assignment is all about data pre-processing. All the problems are fairly straightforward. One thing worth mentioning is that later problems have dependency on previous problems. So just remember to work on the problems in order.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
