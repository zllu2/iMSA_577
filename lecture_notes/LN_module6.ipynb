{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6 Introduction to Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1\n",
    "#### Module 6: Introduction to Text Analysis\n",
    "- Text Analytics\n",
    "- Text Classfication\n",
    "- Advance Topics and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction Script\n",
    "Hello and welcome. \n",
    "\n",
    "In this module, we will discuss text analysis, which is used widely to get insights from text, such as social media posts, customer reviews, and newspaper articles. \n",
    "\n",
    "Unlike the classification examples we've learned so far, whose data has many features, dataset in the text analysis is a collection of text, which is also called a corpus.\n",
    "\n",
    "All machine learning models we learned can only deal with numeric data, so to apply machine learning models on a corpus, we will need to first convert the text data to numeric data.\n",
    "\n",
    "In lesson one, We will introduce some basic concepts in text analytics. specifically, how to convert a corpus of text into a table of numeric values. \n",
    "\n",
    "In Lesson two, we will introduce a new classification algorithm Naive Bayes which is based on bayes theorem. \n",
    "\n",
    "We will apply different classification algorithms including naive bayes on 20 newsgroup dataset for a classification task. \n",
    "\n",
    "The 20 newsgroup dataset is a scikit learn built-in text dataset, it's a collection of messages from different interest groups on a bulletin board.\n",
    "\n",
    "In lesson three, we will discuss some more advanced topics in text classification. We will also introduce a special kind of text analysis, sentiment analysis, which identifies and extracts opinions or emotions from text like customer reviews. We will use the movie review dataset for sentiment analysis.\n",
    "\n",
    "As I mentioned before, please watch the videos to learn the concepts behind the algorithms, and more importantly, go through the lesson notebooks and practice as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 1: Introduction to Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Text Tokenization\n",
    "\n",
    "- Tokenization\n",
    "  - Remove unwanted characters\n",
    "  - Convert to same case\n",
    "  - Stop Words\n",
    "  - Stemming\n",
    "- Bag of words\n",
    "- TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "\n",
    "In this lesson,  We will introduce some basic concepts in text analysis.\n",
    "\n",
    "Unlike datasets we've used so far, datasets in text analysis don't have many features. A typical text analysis dataset is just a collection of text, which is called a corpus. Machine learning algorithms can't be trained on text data. So we will have to first process the text data. \n",
    "\n",
    "Tokenization is the first step of the process.\n",
    "\n",
    "In this lesson, we will discuss the process of tokenization along with some related concepts like stop words and stemming.\n",
    "\n",
    "The result of tokenization is a bag of words, which can be used to create a numeric presentation of a piece of text. The table of the numeric vlaues is called document term matrix. \n",
    "\n",
    "Finally, we will introduce TF-IDF which is a technic to normalize the document term matrix.\n",
    "\n",
    "We start with tokenization which breaks down a paragraph of text to smaller chunks, normally a single word or a phrase, which are called tokens.\n",
    "\n",
    "we'll demonstrate the process with an example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "\n",
    "#### Text Tokenization\n",
    "**Text**  \n",
    "`Machine Learning is a technique of parsing data, learn from that data and then \n",
    "apply what is learned to make an informed decision.`\n",
    "\n",
    "**Tokenization**\n",
    "\n",
    "`Machine` `Learning` `is` `a` `technique` `of` `parsing` `data,` `learn` `from` `that` `data`  \n",
    "`and` `then` `apply` `what` `is` `learned` `to` `make` `an` `informed` `decision.`\n",
    "\n",
    "**Remove special characters and convert to same case**\n",
    "\n",
    "`machine` `learning` `is` `a` `technique` `of` `parsing` `data` `learn` `from` `that` `data`  \n",
    "`and` `then` `apply` `what` `is` `learned` `to` `make` `an` `informed` `decision`\n",
    "\n",
    "**Remove stop words**\n",
    "\n",
    "`machine` `learning` ~~`is`~~ ~~`a`~~ `technique` ~~`of`~~ `parsing` `data` `learn` ~~`from`~~ ~~`that`~~ `data`  \n",
    "~~`and`~~ ~~`then`~~ `apply` ~~`what`~~ ~~`is`~~ `learned` ~~`to`~~ `make` ~~`an`~~ `informed` `decision`\n",
    "\n",
    "**stemming**\n",
    "\n",
    "`machin` `learn` `techniqu` `pars` `data` `learn` `data` `appli` `learn` `make` `inform` `decis`\n",
    "\n",
    "<img src='https://www.meaningcloud.com/developer/img/resources/models/models-tokenization-example.png' width=500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "(slide 1)\n",
    "\n",
    "The text to be tokenized here is a sentence reads as \"Machine Learning is a technique of parsing data, learn from that data and then apply what is learned to make an informed decision.\"\n",
    "\n",
    "\n",
    "(slide2)\n",
    "\n",
    "We begin with the simplest form of tokens, which are individual words. If we split the sentence by whitesapces we'll get a list of words.\n",
    "\n",
    "There are two problems in this token list, one is that some special charaters like punctuations are part of some tokens, like the last token which is decsion followed by a period. The other problem is, python is case sensitive, so same word in different cases are considered as different tokens. \n",
    "\n",
    "(slide3)\n",
    "\n",
    "So in the next step, we will remove all special characters and unify the cases of all tokens.\n",
    "\n",
    "~~To fix the second problem is easy, we simply convert all characters to lower case. To fix the first problem, we can remove all characters that are not part of a word. A common way to accomplish this is through regular expression. Regular expression is a big topic and we are not going to discuss it in depth here. In the lesson one notebook, we define a simple regular expression that removes all characters from a sentence that are not numbers or letters.~~\n",
    "\n",
    "Now we have a list of pretty clean tokens. But in text analysis, not all tokens are equal. Some commonly used words like it, that, is etc, don't have any predictive power. Those words are called stop words. We'd like to filter out all stop words from the tokens.\n",
    "\n",
    "(slide 4)\n",
    "\n",
    "Python NLTL library provides collections of stops words for many different languages. NLTK stands for natural language toolkit. So far we've beening using python scikit learn module exclusively for our machine learning tasks. NLTL is a libary for text analysis only, it provides much more text analytics functionalities, like stop words.\n",
    "\n",
    "After removing the stop words, the token list becomes much shorter, but notice that the three words, learning, learn and learned are actually different forms of the same word. In text analysis, it usually helps to convert all words into its original form, or stem. This process is called stemming.\n",
    "\n",
    "(slide 5)\n",
    "\n",
    "After applying stemming, we have this list of tokens. In this list, learn, learning and learned are all reduced to learn. \n",
    "\n",
    "Let's see how we can use these tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "\n",
    "#### Word Counts\n",
    "**Text:**  \n",
    "`Machine Learning is a technique of parsing data, learn from that data and then \n",
    "apply what they have learned to make an informed decision.`\n",
    "\n",
    "**Tokens:**  \n",
    "`machin` `learn` `techniqu` `pars` `data` `learn` `data` `appli` `learn` `make` `inform` `decis`\n",
    " \n",
    "**Word counts:**  \n",
    "`learn`: 3  \n",
    "`data`: 2  \n",
    "`machin`: 1  \n",
    "`techniqu`:1  \n",
    "`pars`:1  \n",
    "`appli`:1  \n",
    "`make`:1  \n",
    "`inform`:1  \n",
    "`decis`:1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "The simplest way is to conut appearence of tokens in the text. This is the list of token counts. The top 2 used tokens or words are learn and data, from just these two words, we can already have a good guess on what the text is about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Bag of Words\n",
    "\n",
    "**Text:**  \n",
    "`Machine Learning is a technique of parsing data, learn from that data and then \n",
    "apply what they have learned to make an informed decision.`\n",
    "\n",
    "\n",
    "|make|inform|machin|techniqu|data|decis|learn|pars|appli|\n",
    "| :-: | :-: | :-: | :-: | :- :| :-: | :-: | :-: | :-: |\n",
    "| 1 | 1 | 1 | 1 | 2 | 1 | 3 | 1 | 1 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "\n",
    "With the count of tokens, we can create a matrix, the name of each column in the matrix is a token. \n",
    "\n",
    "We can then add the count of each token into the matrix as a new row.\n",
    "\n",
    "For our sample text, we can create a matrix with one row like the table in this slide. Now we esentially created a numeric representation of the text.\n",
    "\n",
    "This concept to tokenize documents to build this matrix is known as bag of words. The matrix is called document term matrix, or DTM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "#### Document Term Matrix\n",
    "`\"This course is about machine learning algorithms\"`\n",
    "\n",
    "|make|inform|machin|techniqu|data|decis|learn|pars|appli|\n",
    "| :-: | :-: | :-: | :-: | :- :| :-: | :-: | :-: | :-: |\n",
    "| 1 | 1 | 1 | 1 | 2 | 1 | 3 | 1 | 1 |\n",
    "| 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "\n",
    "We can add another piece of text into the matrix. For exmaple, we can tokenize the sentence \"This course is about machine learning algorithms\" and add the token counts to the matrix. Now there is a second row. Notice that all columns have value 0 except for \"machin\" and \"learn\". The reason is that the columns in this matrix are created from the first sample sentence. There are some words in the second sentence that are not in the first sentence. So those words are not in the document term matrix.\n",
    "\n",
    "In reality, we will create the document term matrix with a corpus of text, which may have thousands of texts. The matrix then may have many thousands of columns. Each text in the corpus will be converted to a row in the matrix. Each row in that matrix will only have a very small subset of all columns that have non 0 value. That's why this matrix is also called a sparse matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5-2\n",
    "### CountVecterizer\n",
    "```\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Build a vocabulary from training texts\n",
    "cv.fit(training_text)\n",
    "\n",
    "#transform training texts to a Document Term Matrix (DTM)\n",
    "training_dtm = cv.transform(training_text)\n",
    "\n",
    "#transform testing texts to a DTM\n",
    "testing_dtm = cv.transform(testing_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script 5-2\n",
    "\n",
    "Python scikit learn module has a `CountVectorizer` class that creates document term matrix for a corpus. The class will handle special characters and different letter cases by default. You can set argument stop_words to filter out the stop words. In this example, we set stop_words to english, it tells countvectorizer to remove english stop words from the tokens.\n",
    "\n",
    "~~But `CountVectorizer` can't do stemming. You'll have to apply stemming separately.~~\n",
    "\n",
    "The CountVecterizer will create the bag of words from the training set, then convert both training and test set to document term matrices.\n",
    "\n",
    "(Next Slide)\n",
    "\n",
    "This is a subset of a training document term matrix.\n",
    "\n",
    "With the document term matrix that have numeric values, we can perform text classification. The count of each word or token provides classification power. Tokens with higher counts generally provide more predictive power. But this is not always true, let's take a look at another example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "#### TF-IDF\n",
    "\n",
    "##### Text 1\n",
    "Machine `Learning` is a technique of parsing `data`, learn from that `data` and then \n",
    "apply what they have learned to make an informed decision. Machine `learning` focuses on \n",
    "designing algorithms that can learn from and make predictions on the `data`. \n",
    "The `learning` can be supervised or unsupervised.\n",
    "\n",
    "##### Text 2\n",
    "A special school is a school catering for students who have special educational needs \n",
    "due to `learning` difficulties, physical disabilities or behavioral problems. Special \n",
    "schools may be specifically designed, staffed and resourced to provide appropriate \n",
    "special education for children with additional needs.\n",
    "\n",
    "|data|learning|\n",
    "| :-: | :-: |\n",
    "| 3 | 3 |\n",
    "| 0 | 1 |\n",
    "\n",
    "##### After TF-IDF\n",
    "|data|learning|\n",
    "| :-: | :-: |\n",
    "| 0.49 | 0.35 |\n",
    "| 0 | 0.11 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "\n",
    "In this example, we have two pieces of texts, the first one is about machine learning and the second one is about special education. We can create a document term matrix with these two messages. The bottom left table is a subset of the DTM with only two tokens, data and learning. By the way, we don't do stemming in this case so learning is not stemmed to learn. \n",
    "\n",
    "The first row of the DTM represents the first text which is about machine learning. Both data and learning columns have value 3 because both words appear in the first text 3 times. But do they really bear same predictive power?\n",
    "\n",
    "If we examine the texts we used to create the DTM, pay attention to the highlighted words. We can see that word \"learning\" also appears in the second text which is about special education, but data only appears in the first one.\n",
    "\n",
    "So in this case, word \"data\" actually has higher predictive power than learning because when a text has word learning, it could be either about machine learning or about special education, but if a text has word data, it's much more likely to be about machine learning than about special education.\n",
    "\n",
    "We can make improvements based on this observation by considering the frequecy a word appears in the whole corpus. We can normalize the count of a word in a piece of text by the count of the word in the whole corpus. The more a word appears in the whole corpus, the less importance it has to predict individual text.\n",
    "\n",
    "This technic is called term frequency–inverse document frequency, or TF-IDF. Term frequency is the count of a word in a particuler text, document frequency is the count of the word in the whole corpus. For our sample texts, the term frequency of word learning in the first text is 3, the document frequency of learning in the whole corpus is 4. On the other hand the term frequency of word data is 3, and the document frequency of it is also 3. \n",
    "\n",
    "After we apply TF-IDF, we get a new DTM. which is the the bottom right table. \n",
    "\n",
    "Now for the first text, word data has value 0.49 and learning has value 0.35. data is thus considered to have more predictive power than learning in the first text.\n",
    "\n",
    "Scikit learn module has a class tfidfvectorizer which creates tfidf document term matrix. We apply tfidfvectorizer the same we as we do with countvectorizer. Please refer to the lesson notebook for more coding details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 2: Introduction to Text Classification\n",
    "\n",
    "Make image 1,2,3 consistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side 1\n",
    "#### Introduction to Text Classification\n",
    "- Data\n",
    " - 20 Newsgroup Dataset\n",
    "- Algorithms\n",
    " - Naïve Bayes\n",
    " - Linear SVC\n",
    " - Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "In lesson 1, we introduced how to convert a corpus of text into a document term matrix. With the DTM, we can perform text classfication which is widely used in product review analysis, sentiment analysis, document classification and many other applications.\n",
    "\n",
    "In this lesson, we will use the 20 newsgroup dataset to domonstrate text classification. \n",
    "\n",
    "We will introduce a new classification algorithm, Naive bayes, which is based on bayes thereom. We'll also deomonstrate text classfication iwth linear support vector machine and logistic regression, both of which are pouplar text classification algorithms.\n",
    "\n",
    "Now let's look at the 20 newsgroup dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### 20 Newgroupd Dataset\n",
    "##### Newsgroups\n",
    "Class  0 = alt.atheism  \n",
    "Class  1 = comp.graphics  \n",
    "Class  2 = comp.os.ms-windows.misc  \n",
    "Class  3 = comp.sys.ibm.pc.hardware  \n",
    "Class  4 = comp.sys.mac.hardware  \n",
    "Class  5 = comp.windows.x  \n",
    "Class  6 = misc.forsale  \n",
    "Class  7 = rec.autos  \n",
    "Class  8 = rec.motorcycles  \n",
    "Class  9 = rec.sport.baseball  \n",
    "Class 10 = rec.sport.hockey  \n",
    "Class 11 = sci.crypt   \n",
    "Class 12 = sci.electronics  \n",
    "Class 13 = sci.med  \n",
    "Class 14 = sci.space  \n",
    "Class 15 = soc.religion.christian  \n",
    "Class 16 = talk.politics.guns  \n",
    "Class 17 = talk.politics.mideast  \n",
    "Class 18 = talk.politics.misc  \n",
    "Class 19 = talk.religion.misc  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup messages, partitioned across 20 different newsgroups.\n",
    "\n",
    "We list the 20 newgroups in this slide. The news group category will be the label of the dataset. Each category has a numeric code. For example, news group alt.atheism has code 0, and talk.religion.misc has code 19.\n",
    "\n",
    "The training data is the collection of the newsgroup messages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3\n",
    "\n",
    "#### Sample Message (alt.atheism)\n",
    "\n",
    "```\n",
    "From: mathew <mathew@mantis.co.uk>\n",
    "Subject: Re: STRONG & weak Atheism\n",
    "Organization: Mantis Consultants, Cambridge. UK.\n",
    "X-Newsreader: rusnews v1.02\n",
    "Lines: 9\n",
    "\n",
    "acooper@mac.cc.macalstr.edu (Turin Turambar, ME Department of Utter Misery) writes:\n",
    "> Did that FAQ ever got modified to re-define strong atheists as not those who\n",
    "> assert the nonexistence of God, but as those who assert that they BELIEVE in \n",
    "> the nonexistence of God?\n",
    "\n",
    "In a word, yes.\n",
    "\n",
    "mathew\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script\n",
    "This is a sample message in the dataset, which belongs to alt.atheism newsgroup.\n",
    "\n",
    "All the messages in the dataset have same format, which is typical email format. The messages have from address and subjects, as well as quoted messages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4\n",
    "\n",
    "#### Load 20 Newsgroup Data\n",
    "```\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(subset='train')\n",
    "test = fetch_20newsgroups(subset='test')\n",
    "```\n",
    "\n",
    "#### data and target Attribute\n",
    "```\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "train_dtm = cv.fit_transform(train['data'])\n",
    "test_dtm = cv.transform(test['data'])\n",
    "\n",
    "# Create Classifier\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm, train['target'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "The 20 newsgroup dataset is built-in in scikit learn module. \n",
    "\n",
    "To split the dataset to train and test, we don't need to use train_test_split function. We directly load train and test data by usring fetch_20newsgroups function defined in the scikitlearn datasets module.\n",
    "\n",
    "In train and test, there are multiple attributes, attribute data contains a collection of newsgroup messages as shown in the previous slide. Attribute target contains the label or newgroups category of each message.\n",
    "\n",
    "We create document term matrix with train['data'], then train a classifier with the dtm and the target label which is in train['target']\n",
    "\n",
    "The goal of the classification is to predict which newsgroup a message belongs to based on the message text.\n",
    "\n",
    "The classifier in this code sample is multinomial naive bayes classifier. It's a classification model based on bayes theorem. Let's take a look the Bayes theorem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Bayes Theorem\n",
    "$$\n",
    "\\textrm{P(A|B)} = \\frac{\\textrm{P(B|A)} \\textrm{P(A)}}{\\textrm{P(B)}}\n",
    "$$\n",
    "\n",
    "Where A and B are events.\n",
    "- P(A) is the probability of observing event A.\n",
    "- P(B) is the probability of observing event B.\n",
    "- P(A | B) is a conditional probability: the likelihood of event A occurring given that B is true.\n",
    "- P(B | A) is a conditional probability: the likelihood of event B occurring given that A is true.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "\n",
    "Bayes Theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. \n",
    "\n",
    "Bayes theorem can be mathematically stated as the equation shown in this slide. In this equation:\n",
    "- P(A) is the probability of observing event A.\n",
    "- P(B) is the probability of observing event B.\n",
    "- P(A | B) which reads as p(a) given b, is a conditional probability, or the probability of event A given that event B already happened.\n",
    "- P(B | A) is the probability of event B given that A is true.\n",
    "\n",
    "We will use a cancer screening test as example to explain bayes theorem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5\n",
    "#### Bayes Theorem\n",
    "- 10,000 cancer screening test results\n",
    "- 1% have caner\n",
    "- 80% cancer patients tested positive\n",
    "- 5% healthy people tested positive\n",
    "\n",
    "Question: What's the likelihood of a person to have cancer if tested positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5 Script\n",
    "\n",
    "Assume we have a dataset of cancer screening test results for 10,000 patients. 1% of all patients actually have the cancer; The cancer screening test is not perfect, If a patient has the cancer, there's 80% of chance that the test will be positive, this is the true positive; If a patient doesn't have the cancer, the test still has 5% of chance to be positive, and this is the false positive. \n",
    "\n",
    "The question is, if a person tested positive with this cancer screening test, what's the probability that this person really has the cancer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 6\n",
    "#### Bayes Theorem\n",
    "\n",
    "- 1% has cancer: 10000 x 0.01 = 100 have cancer, 9900 don’t\n",
    "- Predicted Positive: 100 x 0.8 + 9900 x 0.05 = 80 + 495 = 575\n",
    "- True positive among predicted positive: 80\n",
    "- Probability to have cancer if tested positive: 80/575 = 14%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 6 Script\n",
    "First, let's calculate number of predicted positives. We know that 1% out of 10,000, which is 100 people who have the cancer, for this 100 people, 80% of them, or 80 people tested positive. For the 99,00 who donot have the cancer, 5% of them tested positive, so 99,00*%5, is 495. So the total tested positive is 80 + 495, or 575. Among this 575 people, only 80 people really have the cancel, so the likelihood to have the cancer when tested positive is 80/575, which is about 14%.\n",
    "\n",
    "Now I'll give you a quiz, what is the accuracy score of this sreening test? Remember that accuracy score is calculated with true positive + true gegative divided by total number of tests. You may pause the video for a minute and calculate it.\n",
    "\n",
    "\n",
    "You might be surprised by the result, the test actually has 95% accurracy score. So the hospital can legitimately boasts that their cancer screening test has a 95% accuracy rate. But actually, even tested positive, a patient still only has about 14% of chance to really have the cancer. This again demonstrates that accuracy rate can be misleading in evaluating a classification model.\n",
    "\n",
    "Ok, now let's see how we can use bayes theorem to get the likelihood of having the cancer when tested positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 7\n",
    "#### Bayes Therom\n",
    "- Event A is has the cancer, P(A): 1%\n",
    "- Event B is test positive, P(B): $\\frac{80+495}{10000}$=5.75%.\n",
    "- P(B|A) is the likelihood of testing positive when having the cancer, 80%\n",
    "- P(A|B) is the likelihood of having cancer when tested positive, or the probablity we are trying to find out.\n",
    "\n",
    "$\\textrm{P(A|B)} = \\frac{\\textrm{P(B|A)} \\textrm{P(A)}}{\\textrm{P(B)}}$ = $\\frac{0.8*0.01}{0.0575}$ = 14%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 7 Script\n",
    "\n",
    "With bayes theorem, event A is has the cancer, P(A) is the probability to have the cancer which is 1%, \n",
    "\n",
    "event B is tested positive, this include true positive and flase positive, p(B) is the probability of testing positive, which is about 5.75%, \n",
    "\n",
    "p B given A is the probability to test positive when having the cancer, which is 80%. Now we can calculate p A given B with the bayes theorem formula, from which we get the same result, 14%.\n",
    "\n",
    "Now let's see how we can apply bayes theorem in text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8\n",
    "#### Bayes Theorem on Text Classification\n",
    "\n",
    "- A represents a topic, P(A) is the proportion of all messages that are in topic A.\n",
    "- B represents occurrence of a word B, P(B) is the frequency of the word B in the text.\n",
    "- P(B|A) is the likely frequency of the word B appears in the text if the text belongs to the - topic A\n",
    "- P(A|B) is the likelihood of a text in topic A given the frequency of word B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8 Scrip\n",
    "\n",
    "In a text classification,\n",
    "- A represents a topic, in 20 newsgroup dataset, A represents a newsgroup category. P(A) is the proportion of all messages that are in the newsgroup A.\n",
    "- B represents a word, P(B) is the frequency of the word B in a message.\n",
    "- P(B|A) is the likely frequency of the word B appears in a message if the message belongs to the newsgroup A.\n",
    "- P(A|B) is the likelihood of a message belongs to newsgroup A given the frequency of word B.\n",
    "\n",
    "\n",
    "With Bayes Theorem, we can calculate P(A|B), or the likelihood that a messages belongs to the topic \"A\" if word \"B\" appears in the text for a give freqenucy.  We can get P(A), P(B) and P(B|A) from the training data, and calcuate P(A|B). \n",
    "\n",
    "In 20 newgroups case, for example, if word \"God\" appears 3 times in a message, we can use bayes theorem to calculate the probablity of the message belonging to category alt.atheism.\n",
    "\n",
    "When we combine P(A|B) of all different words in a message, we can get the overall probablity of the message belonging to the newsgroup \"A\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9\n",
    "#### Naive Bayes Classifier\n",
    "```\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create DTM\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "train_dtm = cv.fit_transform(train['data'])\n",
    "test_dtm = cv.transform(test['data'])\n",
    "\n",
    "# Create Classifier\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm, train['target'])\n",
    "\n",
    "score = nb.score(test_dtm, test['target'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9 Script\n",
    "\n",
    "Python scikit learn module defines MultinomialNB which is a classification model based on bayes theorem. It's called naive bayes because we assume that occurance of a word is independent to all other words in the text, which is not always true, but we just make this naive assumption. This assumption enables the calculation of overal probability from all individual probabilies.\n",
    "\n",
    "In the lesson notebook, we demonstrate text classification with MultinomialNB classifier, as well as linear support vector machine and logistic regression classifiers. They all follow the same standard steps we introduced in the previous lessons. The only special step is that we need to first create the document term matrix before applying classifiers. We domonstrated both regular dtm and tf-idf dtm in the lesson notebook. For the 20 news group dataset, as expected, tf-idf dtm gives better result. We also plot the confusion matrix for each model to show the performance. Since there are 20 target labels, this confusion matrix has 20 rows and 20 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 10\n",
    "<img src='images/nb_confusion_matrix.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 10 Script\n",
    "This is the confusion matrix of the multinomialNB model. The numbers in the diagonal cells are correct predictions. We can see the model does a decent job to predict message newsgroups except for the third newsgroup, windows.misc.\n",
    "\n",
    "Linear Support vector machine and logistic regression actually give better results. You may see the details from the lesson notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 3: Introduction to Text Classification II\n",
    "- Explore Classifiers\n",
    "\n",
    "- N-Grams\n",
    "\n",
    "- N-Gram Classification\n",
    "\n",
    "- Sentiment Analysis\n",
    "\n",
    "- Stemming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 3 Script\n",
    "In this lesson, we will explore classifiers we created in the previous lesson. Specifically, we will extract top words in each newsgroup and try to improve the classification with this new information.\n",
    "\n",
    "We will also introduce concept of N-Grams, which means a phrase with multiple words. A lot of times, combination of words bears more accurate information than individual words. For example, \"university of illinois\" as a whole phrase carries specific meaning.\n",
    "\n",
    "We will also introduce sentiment analysis which is a popular application of text classification. In Sentiment analysis, the target label is customer sentiment or feeling, like positive neutual and negative.\n",
    "\n",
    "We'll also demonstrate stemming in the sentiment analysis example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Top Words\n",
    "\n",
    "|Topic|Top 5 Words|\n",
    "| :-: | :-: |\n",
    "| comp.sys.mac.hardware | mac, apple, **edu**, drive, quadra |\n",
    "| comp.windows.x | window, motif, mit, server, **com** |\n",
    "| misc.forsale | sale, **edu**, 00, offer, shipping |\n",
    "| rec.autos | car, **com**, cars, __edu__, engine |\n",
    "| rec.motorcycles | bike, **com**, dod, __edu__, ride |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'bike', 'dod', 'ride', 'bikes', 'motorcycle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Words\n",
    "\n",
    "|Topic|Top 5 Words|\n",
    "| :-: | :-: |\n",
    "| comp.sys.mac.hardware | mac, apple, drive, quadra, se |\n",
    "| comp.windows.x | window, motif, mit, server, widget |\n",
    "| misc.forsale | sale, 00, offer, shipping, new |\n",
    "| rec.autos | car, cars, engine, article, would |\n",
    "| rec.motorcycles | bike, dod, ride, bikes, motorcycle |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "The classifilers we used in the previous lesson have an attribute coef_ that can be used to extract top words in each topic. Please note that not all classifiers have this attribute, k-nearest neighbor, decision tree and random forest don't have this attribute.\n",
    "\n",
    "This slide displays top 5 words for 5 newsgroups. Many them make a lot of sense. For example, top words for mac hardware include mac and apple. But some common words appear in many topics, like edu and com. This is because every message has a header which contains email address. Both edu and com are very common in email addresses. These two words don't really provide much predictive power and can be considered as stop words.\n",
    "\n",
    "In the lesson notebook, we demonstrate how to add these words to stop words and create document term matrix with the new stop words. We hope this change may improve the classification accuracy. Unfortunately, the model accuracy rate actually drops a little bit after the change. This phenomenon is very common in data analytics, that extra efforts may not always result a better model, we'll have to try combinations of improvements iteratively to get the optimum result.\n",
    "\n",
    "In this case, even we don't get better accuracy with the change, we do get more meaningful top words for the newsgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2(Not needed)\n",
    "#### Top Words with Custom Stop Words\n",
    "|Topic|Top 5 Words|\n",
    "| :-: | :-: |\n",
    "| comp.sys.mac.hardware | mac, apple, drive, quadra, se |\n",
    "| comp.windows.x | window, motif, mit, server, widget |\n",
    "| misc.forsale | sale, 00, offer, shipping, new |\n",
    "| rec.autos | car, cars, engine, article, would |\n",
    "| rec.motorcycles | bike, dod, ride, bikes, motorcycle |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Slide 2 Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3\n",
    "#### N-Grams\n",
    "`This course introduces many concepts in data science.`\n",
    "##### 1-3 Grams\n",
    "`['this', 'course', 'introduces', 'many', 'concepts', 'in', 'data', 'science', 'this course', 'course introduces', 'introduces many', 'many concepts', 'concepts in', 'in data', 'data science', 'this course introduces', 'course introduces many', 'introduces many concepts', 'many concepts in', 'concepts in data', 'in data science']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script\n",
    "Now let's look at a new concept, n-grams. an n-gram is a contiguous sequence of n words, so 3-grams is a phrase of 3 continuous words. Let's use an example to understand this concept.\n",
    "\n",
    "The sample sentence is:\n",
    "\n",
    "This course introduces many concepts in data science. \n",
    "\n",
    "This sentence has 8 words. Untill now we tokenize text by individual word, so the DTM created by this sentence has 8 columns if we don't filter out the stop words. But if we create a DTM with 1 to 3 grams, which means all combinations of two and three continuous words are also considered as tokens. we'll have 21 tokens, or 21 columns in the new DTM. \n",
    "\n",
    "If we use this new DTM in a classification, we may achieve better result. But this is by no means guaranteed. This approach just provide a possible way to improve our classification model. But one thing is for sure, that the computation complexity is much higher if we apply n-grams because of the dramatic increase in number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4\n",
    "#### Sentiment Analysis\n",
    "<img src='https://www.kdnuggets.com/images/sentiment-fig-1-689.jpg' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "Now let's look at a popular text analysis application, so called sentiment analysis.\n",
    "\n",
    "Sentiment analysis identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service from reviews and online conversations. \n",
    "\n",
    "In sentiment analysis, the target label is the customer sentiment toward a product or service, like positive, neutral or negative, training data is the customer reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "#### Movie Review Dataset\n",
    "- Built-in in NLTK\n",
    "- 2000 movie reviews\n",
    "- 1000 positive, 1000 negative\n",
    "\n",
    "##### Sample Review(negative)\n",
    "```\n",
    "Label: 0\n",
    "not a great twelve months for either of the principals from this movie . earlier this year , nora ephron wrote and produced one of the year ' s least likeable \" comedies \" called hanging up , featuring a bunch of annoying women ( ironically , lisa kudrow played one in that film as well ) who barely have time to care about anyone but themselves...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "To demonstrate sentiment analysis, we use a movie review dataset which is built-in in the nltk library.\n",
    "This dataset has 2000 movie reviews, with 1000 positive reviews and 1000 negative reviews. A snippet of a negative review is shown in this slide.\n",
    "\n",
    "The python code to load this review data is not very intuitive. So you don't have to understand the code that loads the reviews in the lesson's notebook. Just assume that we have training text dataset and target labels available and focus on the analysis part when you read the lesson notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "#### What Customer Cares Most\n",
    "Top 20 Words of Positive Reviews:  \n",
    "`great, fun, life, hilarious, memorable, overall, quite, different, good, terrific, especially, trek, works, seen, performances, perfect, perfectly, comic, town, gives`\n",
    "\n",
    "Top 20 words of Negative Reviews:  \n",
    "`bad, worst, plot, supposed, unfortunately, harry, boring, script, stupid, reason, poor, awful, waste, cheap, attempt, jakob, dull, lame, looks, better`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "\n",
    "The training process is same as what we did in the 20 newsgroup case. Just this time the label is not newsgroup category, but 0 and 1 which represents negative and positive.\n",
    "\n",
    "After the classifier is trained, we can get top words in both positive and negative reviews to understand what customers care most. This slide shows the top 20 words in both positive and negative reviews given by a logistic regression classifier. \n",
    "\n",
    "The top words in movie reviews may seem obvious and not very useful. But if the reviews are about a particular product, we may learn what customers care most about the product from the top words in the positive and negative reviews, which can be very valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7\n",
    "#### Stemming\n",
    "- Not supported by default\n",
    "- Define custom tokenize fucntion with PotterStemmer\n",
    "- Slow down training process\n",
    "- Improve accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Script\n",
    "\n",
    "In the previous lesson, we introduced the concept of stemming. In this notebook, we demonstrate how to apply stemming in the sentiment analysis. \n",
    "\n",
    "Unlike stopwords which is supported by counter vectorizer directly, to apply stemming, we will have to create customized tokenize function. In the lesson's notebook, we use PotterStemmer which is defined in the nltk library to do stemming. Since stemming is applied on every word in the corpus, this process slows down the trainning process noticablly. But stemming does result a more accurate model for the movie review dataset.\n",
    "\n",
    "Please refer to the lesson notebook for more coding details about stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1\n",
    "#### Module 6 Review\n",
    "- Text Data Pre-Process\n",
    " - stop words\n",
    " - stemming\n",
    " - Document term matrix(DTM)\n",
    " - TF-IDF\n",
    "- Text Classification\n",
    " - Naive Bayes\n",
    " - Support Vector Machine\n",
    " - Logistic Regression\n",
    "- Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Review Script\n",
    "\n",
    "In this module, we learned a popular machine learning application, text analysis. \n",
    "\n",
    "Text analysis is special in that the training data is just a collection of texts. To enable machine learning on text data, we have to first convert text data to numeric values. In lesson one we learned the basic technics and concepts in text data pre-processing, including  stop words and stemming. The text data will be transformed to document term matrix before feeding to a classifier. Please make sure you understand the concept of bag of words, document term matrix, and tf-idf, which is a normalized document term matrix.\n",
    "\n",
    "Text analysis is just an application of classification. You can apply any classifier we learned so far on text data. But the three classifiers we used in this module, multinomial naive bayes, linear support vector machine and logistic regression, are more popular classifiers. We can get useful information like top words with these three classifiers. This is not true with some other classifiers like decision tree or random forest.\n",
    "\n",
    "In lesson three we also introduced sentiment analysis which is a popular text analysis application. The label of sentiment analysis is customer sentiment.\n",
    "\n",
    "In this module's assignment, we will do a sentiment analysis on resturant reviews extracted from yelp. In the assignment you will need to map ratings to label. You may use lambda function to do this. If you don't remember how to use a lambda function, you can refer to module 3 where we perform classification on adult income data. We map salary greater than 50,000 to 1 and less than 50,000 to 0 in module 3 notebooks.\n",
    "\n",
    "Other than this, the assignment is fairly straightforward. Just remember to work on the problems in order.\n",
    "\n",
    "Good luck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
