{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6 Introduction to Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction Script\n",
    "Hello and welcome. \n",
    "\n",
    "\n",
    "As I mentioned before, please watch the video to learn the concepts behind the algorithms, and more importantly, go through the lesson notebooks and practice as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 1: Introduction to Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Text Tokenization\n",
    "\n",
    "- Tokenization\n",
    "- Remove unwanted characters\n",
    "- Convert to same case\n",
    "- Stop Words\n",
    "- Stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1 Script\n",
    "Unlike datasets we've used so far, text analysis datasets don't have many features. A typical text analysis dataset is just a collection of text, which is called corpus. Machine learning algorithms can't directly train on text data. So we will have to first pre-process the text data. Tokenization is normally the first step in text analysis.\n",
    "\n",
    "Tokenization is a process to break down a paragraph of text to smaller chunks, normally a single word or a phrase, which are called tokens.\n",
    "\n",
    "We begin with the simplest form of token, which is an individual word in a text. Words in a piecie of text are natually separated by whitespaces, so we can easily use python string function `split` to transform a piece of text into a list of words, or in our case, tokens.\n",
    "\n",
    "There are several things we need to consider, the first is the special characters, like punctuations, quotation marks and parenthesis. We need to remove the special characters to improve the quality of tokens. \n",
    "\n",
    "We also need to convert all words to same case since python is case sensitive, same words in different cases are considered as different token in python.\n",
    "\n",
    "The next concept is stop words. Stop words are words that are used frequently but don't provide much information, like it, that, you, I, of, about, etc. \n",
    "\n",
    "Python NLTL library provides collections of stops words for many different languages. NLTK stands for natural language toolkit. So far we've beening using python scikit learn module exclusively for our machine learning tasks. NLTL is a libary for text analysis only, it provides much more text analytics functionalities, like stop words and stemming. We will also use nltk to load text data for our text analysis examples in the following lessons.\n",
    "\n",
    "And lastly, stemming. stemming is a process that convert a word to its stem form. For example, learning, learned and learn are three different words, but they all have same stem which is learn.\n",
    "\n",
    "We will use an example to demonstrate the concepts we learned so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "\n",
    "#### Text Tokenization\n",
    "`Machine Learning is a technique of parsing data, learn from that data and then \n",
    "apply what they have learned to make an informed decision.`\n",
    "\n",
    "**Tokenization**\n",
    "\n",
    "`Machine` `Learning` `is` `a` `technique` `of` `parsing` `data,` `learn` `from` `that` `data`  \n",
    "`and` `then` `apply` `what` `they` `have` `learned` `to` `make` `an` `informed` `decision.`\n",
    "\n",
    "**Remove special characters and convert to same case**\n",
    "\n",
    "`machine` `learning` `is` `a` `technique` `of` `parsing` `data` `learn` `from` `that` `data`  \n",
    "`and` `then` `apply` `what` `they` `have` `learned` `to` `make` `an` `informed` `decision`\n",
    "\n",
    "**Remove stop words**\n",
    "\n",
    "`machine` `learning` ~~`is`~~ ~~`a`~~ `technique` ~~`of`~~ `parsing` `data` `learn` ~~`from`~~ ~~`that`~~ `data`  \n",
    "~~`and`~~ ~~`then`~~ `apply` ~~`what`~~ ~~`they`~~ ~~`have`~~ `learned` ~~`to`~~ `make` ~~`an`~~ `informed` `decision`\n",
    "\n",
    "**stemming**\n",
    "\n",
    "`machin` `learn` `techniqu` `pars` `data` `learn` `data`  \n",
    " `appli` `learn` `make` `inform` `decis`\n",
    "\n",
    "<img src='https://www.meaningcloud.com/developer/img/resources/models/models-tokenization-example.png' width=500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "\n",
    "#### Word Counts\n",
    "`Machine Learning is a technique of parsing data, learn from that data and then \n",
    "apply what they have learned to make an informed decision.`\n",
    "\n",
    "`learn`: 3  \n",
    "`data`: 2  \n",
    "`machin`: 1  \n",
    "`techniqu`:1  \n",
    "`pars`:1  \n",
    "`appli`:1  \n",
    "`make`:1  \n",
    "`inform`:1  \n",
    "`decis`:1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "After text tokenization, we can conut appearence of words in the text. This is the list of word counts. From the top 2 used words, learn and data, we can already have a good guess on what the text is about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Bag of Words\n",
    "\n",
    "|make|inform|machin|techniqu|data|decis|learn|pars|appli|\n",
    "| :-: | :-: | :-: | :-: | :- :| :-: | :-: | :-: | :-: |\n",
    "| 1 | 1 | 1 | 1 | 2 | 1 | 3 | 1 | 1 |\n",
    "\n",
    "#### Document Term Matrix\n",
    "`\"This course is about machine learning algorithms\"`\n",
    "\n",
    "|make|inform|machin|techniqu|data|decis|learn|pars|appli|\n",
    "| :-: | :-: | :-: | :-: | :- :| :-: | :-: | :-: | :-: |\n",
    "| 1 | 1 | 1 | 1 | 2 | 1 | 3 | 1 | 1 |\n",
    "| 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "\n",
    "Now we can convert the word counts list to a matrix, each column in the matrix is a token. For our sample text, we can create a matrix like this. Each row in the matrix represents a piece of text.\n",
    "\n",
    "This concept to tokenize documents to build this matrix is known as bag of words. The matrix is call document term matrix, or DTM.\n",
    "\n",
    "We can add another piece of text into the matrix. For exmaple, \"This course is about machine learning algorithms\". Now there is second row. Notice that all columns have value 0 except for \"machin\" and \"learn\". The reason is that we created the columns in this matrix with only one sentence, and many words in the second sentence are not in the first sentence. In reality, we will create the document term matrix with a corpus of text, which may have thousands of articles. The matrix then may have thousands of columns. Each row in that matrix will only have non-zero values in a small subset of all columns. That's why this matrix is also called a sparse matrix.\n",
    "\n",
    "Python scikit learn module has a `CountVectorizer` class which makes it easy to create document term for a corpus. The class will handle special characters and different cases by default. You can set argument to filter out stop words too. But `CountVecgtorizer` can't do stemming. You'll have to apply stemming separately which we will introduce in lesson 3.\n",
    "\n",
    "Now with the document term matrix, which is similar to the traditional machine learning dataset with all numeric values, we can perform text analysis. The count of each word provides classification power. Tokens with higher count generally provide more predictive power. But this is not always true, let's take a look at this example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "#### TF-IDF\n",
    "\n",
    "##### Text 1\n",
    "Machine `Learning` is a technique of parsing `data`, learn from that `data` and then \n",
    "apply what they have learned to make an informed decision. Machine `learning` focuses on \n",
    "designing algorithms that can learn from and make predictions on the `data`. \n",
    "The `learning` can be supervised or unsupervised.\n",
    "\n",
    "##### Text 2\n",
    "A special school is a school catering for students who have special educational needs \n",
    "due to `learning` difficulties, physical disabilities or behavioral problems. Special \n",
    "schools may be specifically designed, staffed and resourced to provide appropriate \n",
    "special education for children with additional needs.\n",
    "\n",
    "|data|learning|\n",
    "| :-: | :-: |\n",
    "| 3 | 3 |\n",
    "| 0 | 1 |\n",
    "\n",
    "##### After TF-IDF\n",
    "|data|learning|\n",
    "| :-: | :-: |\n",
    "| 0.49 | 0.35 |\n",
    "| 0 | 0.11 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "\n",
    "Here we have two pieces of text, the first one is about machine learning and the second one is about special education. We can create a document term matrix with these two messages. The image is a subset of the DTM with only two columns, data and learning. By the way, we ignore stemming in this case so learning is not stemmed to learn. The first row of the DTM represents the first text which is about machine learning. Both data and learning column have value 3 because they both appear in the first text 3 times. But do they really bear same predictive power?\n",
    "\n",
    "If we examine the texts we used to create the DTM, word \"learning\" is also used in the second text which is about special education, but data only appears in the first text. So in this case, word \"data\" actually has higher predictive power than learning because when a text has learning, it could be about machine learning or special education, while if a text has data, it's much more likely to be about machine learning than special education.\n",
    "\n",
    "We can make improvements based on this observation by considering the frequecy a word appears in the whole corpus that's used to create the DTM. We can normalize the count of a word in a piece of text by the count of the word in the whole corpus. The more a word appears in the whole corpus, the less importance it has to predict individual text.\n",
    "\n",
    "This technic is called term frequency–inverse document frequency, or TF-IDF. Term frequency is count of a word in a particuler text, document frequency is count of the word in the whole corpus. After we apply TF-IDF, we get a new DTM. which is the second DTM in the slide. Now for the first text, token data has value 0.49 and learning has value 0.35. data is thus considered to have more predictive power than learning in the first text.\n",
    "\n",
    "Now we've converted test data into a matrix of numeric data, we can then apply machine learning models on this new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 2: Introduction to Text Classification\n",
    "\n",
    "Make image 1,2,3 consistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### 20 Newgroupd Dataset\n",
    "##### Newsgroups\n",
    "Class  0 = alt.atheism  \n",
    "Class  1 = comp.graphics  \n",
    "Class  2 = comp.os.ms-windows.misc  \n",
    "Class  3 = comp.sys.ibm.pc.hardware  \n",
    "Class  4 = comp.sys.mac.hardware  \n",
    "Class  5 = comp.windows.x  \n",
    "Class  6 = misc.forsale  \n",
    "Class  7 = rec.autos  \n",
    "Class  8 = rec.motorcycles  \n",
    "Class  9 = rec.sport.baseball  \n",
    "Class 10 = rec.sport.hockey  \n",
    "Class 11 = sci.crypt   \n",
    "Class 12 = sci.electronics  \n",
    "Class 13 = sci.med  \n",
    "Class 14 = sci.space  \n",
    "Class 15 = soc.religion.christian  \n",
    "Class 16 = talk.politics.guns  \n",
    "Class 17 = talk.politics.mideast  \n",
    "Class 18 = talk.politics.misc  \n",
    "Class 19 = talk.religion.misc  \n",
    "##### Sample Message\n",
    "```\n",
    "From: mathew <mathew@mantis.co.uk>\n",
    "Subject: Re: STRONG & weak Atheism\n",
    "Organization: Mantis Consultants, Cambridge. UK.\n",
    "X-Newsreader: rusnews v1.02\n",
    "Lines: 9\n",
    "\n",
    "acooper@mac.cc.macalstr.edu (Turin Turambar, ME Department of Utter Misery) writes:\n",
    "> Did that FAQ ever got modified to re-define strong atheists as not those who\n",
    "> assert the nonexistence of God, but as those who assert that they BELIEVE in \n",
    "> the nonexistence of God?\n",
    "\n",
    "In a word, yes.\n",
    "\n",
    "mathew\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "\n",
    "In lesson 1, we introduced how to convert a corpus of text into a matrix of numeric values, or DTM. With the DTM, we can perform text classfication which is widely used in product review analysis, sentiment analysis, document classification and many other applications.\n",
    "\n",
    "In this lesson, we will use a scikit learn built-in text dataset, the 20 newsgroup, to demonstrate text classification.\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup messages, partitioned across 20 different newsgroups.\n",
    "\n",
    "We list the 20 newgroups in the slide, as well as a sample message which belongs to the first news group, alt.athesim.\n",
    "\n",
    "The training data is collections of all messages, and the target lable is the newsgroup a message belongs to. For the sample message, it's lable is 0. There are total 20 different labels corresponding to 20 newsgroups.\n",
    "\n",
    "Our goal is to train a text classification model with 20 newsgroups dataset, then use the model to classify a message into one of the 20 newsgroups.\n",
    "\n",
    "But first, we will introduce a popular text classification algorithm, naive bayes classifier, which is based on bayes therom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Bayes Therom\n",
    "$$\n",
    "\\textrm{P(A|B)} = \\frac{\\textrm{P(B|A)} \\textrm{P(A)}}{\\textrm{P(B)}}\n",
    "$$\n",
    "\n",
    "Where A and B are events.\n",
    "- P(A) is the probability of observing event A.\n",
    "- P(B) is the probability of observing event B.\n",
    "- P(A | B) is a conditional probability: the likelihood of event A occurring given that B is true.\n",
    "- P(B | A) is a conditional probability: the likelihood of event B occurring given that A is true.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2 Script\n",
    "Bayes Theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. \n",
    "\n",
    "Bayes theorem can be mathematically stated as the equation shown in this slide. In this equation:\n",
    "- P(A) is the probability of observing event A.\n",
    "- P(B) is the probability of observing event B.\n",
    "- P(A | B) which reads as p(a) given b, is a conditional probability, or the probability of event A given that event B already happened.\n",
    "- P(B | A) is the probability of event B given that A is true.\n",
    "\n",
    "We will use a cancer screening test as example to explain bayes therom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3\n",
    "#### Bayes Therom\n",
    "- 10,000 cancer screening test results\n",
    "- 1% have caner\n",
    "- 80% cancer patients tested positive\n",
    "- 5% healthy people tested positive\n",
    "\n",
    "Question: What's the likelihood of a person to have cancer if tested positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 3 Script\n",
    "\n",
    "Assume we have a dataset of cancer screening test results for 10,000 patients. 1% of patients actually have the cancer; The cancer screening test is not perfect, If a patient has the cancer, there's 80% of chance that the test will be positive, remember that this is the true positive; If a patient doesn't have the cancer, the test still has 5% of chance to be positive, and this is the false positive. The question is, if a person tested positive with this cancer screening test, what's the probability that this person really has the cancer?\n",
    "\n",
    "Think from a classifcation point of view, what we are trying to find out here is actually the positve precision, or among all predicted positives, what's the percentage that really havs cancer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4\n",
    "#### Bayes Therom\n",
    "\n",
    "- predicted Positive: 100*0.8 + 9900*0.05 = 80 + 495 = 575\n",
    "- true positive among predicted positive: 80\n",
    "- positive precision: 80/575 = 14%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 4 Script\n",
    "First, let's calculate number of predicted positive. We know that 1% out of 10,000, or total 100 people have cancer, for this 100 people, 80% of them, or 80 people tested positive. For the 99,00 healthy people, 5% of them tested positive, so 99,00*%5, which is 495. So the total tested positive is 80 + 495, or 575. Among this 575 people, 80 really have cancel, so the likelihood to have cancer when tested positive is 80/575, which is about 14%.\n",
    "\n",
    "If we calculate the accuracy score of this screening test, which is true positive + true negative divivded by total tests, we will get 80+9900X0.95/10000, which is about 95%. So the hospital can legitimately boast that their cancer screening has 95% accuracy rate. But actually, even tested positive, a patient still only has about 14% of chance to really have the cancer. This again demonstrates that accuracy rate can be misleading in evaluating a classification model.\n",
    "\n",
    "Ok, now let's see how we can use bayes therom to get the likelihood of a person to have cancer if tested positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5\n",
    "#### Bayes Therom\n",
    "- Event A is has the cancer, P(A): 1%\n",
    "- Event B is test positive, P(B): $\\frac{80+495}{10000}$=5.75%.\n",
    "- P(B|A) is the likelihood of testing positive when having the cancer, 80%\n",
    "- P(A|B) is the likelihood of having cancer when tested positive,the probablity we are trying to find out.\n",
    "\n",
    "$\\textrm{P(A|B)} = \\frac{\\textrm{P(B|A)} \\textrm{P(A)}}{\\textrm{P(B)}}$ = $\\frac{0.8*0.01}{0.0575}$ = 14%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 5 Script\n",
    "\n",
    "Now event A is has the cancer, P(A) is then 1%, event B is tested positive, this include true positive and flase positive, which is about 5.75%, p B given A is the probability to test positive when having the cancer, which is 80%. Now we can calculate p A given B with bayes therom, from which we get the same result, 14%.\n",
    "\n",
    "Now let's see how we can apply bayes therom in text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "#### Bayes Theorem on Text Classification\n",
    "\n",
    "- A represents a topic, P(A) is the percent of topic \"A\" among all messages.\n",
    "- B represents occurence of a word \"B\", P(B) is the frequency of the word \"B\" in the text.\n",
    "- P(B|A) is the likely frequency of the word \"B\" appears in the text if the text belongs to the topic \"A\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Scrip\n",
    "\n",
    "In text classification, \n",
    "- A represents a topic, P(A) is the percent of topic \"A\" among all messages.\n",
    "- B represents occurence of a word \"B\", P(B) is the frequency of the word \"B\" in a text.\n",
    "- P(B|A) is the likely frequency of the word \"B\" appears in a text if the text belongs to the topic \"A\"\n",
    "\n",
    "\n",
    "With Bayes Theorem, we can calculate P(A|B), or the likelihood that the text belongs to topic \"A\" if word \"B\" appears in the text for a give frequcy. We can get P(A), P(B) and P(B|A) from the training data, and calcuate P(A|B). When we combine P(A|B) of all words(all different \"B\"s) in the text, we can get the overall probablity of the text belonging to the topic \"A\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7\n",
    "#### Naive Bayes Classifier\n",
    "```\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create DTM\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "train_dtm = cv.fit_transform(train['data'])\n",
    "test_dtm = cv.transform(test['data'])\n",
    "\n",
    "# Create Classifier\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm, train['target'])\n",
    "\n",
    "score = nb.score(test_dtm, test['target'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Script\n",
    "\n",
    "In python scikit learn module, we have MultinomialNB which is a classification model based on bayes therom. It's called naive bayes because we assume that occurance of a word is independent to all other words in the text, which is not always true, but we just make this naive assumption. This assumption enables calculation of overal probability from all individual probabilies.\n",
    "\n",
    "In the lesson notebook, we demonstrate text classification with MultinomialNB classifier, as well as linear support vector machine and logistic regression classifiers. They all follow the same standard steps we introduced in previous lessons. The only special step is we need to first create the document term matrix before applying classifiers. We domonstrated both regular dtm and tf-idf dtm in the lesson notebook. For the 20 news group dataset, as expected, tf-idf dtm gives better result. We also plot confusion matrix for each model to show the performance. Since there are 20 target lables, this confusion matrix has 20 rows and 20 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8\n",
    "<img src='images/nb_confusion_matrix.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8 Script\n",
    "This is the confusion matrix of the multinomialNB model. We can see it does a decent job to predict message topics except for the third topic, windows.misc.\n",
    "\n",
    "Linear Support vector machine and logistic regression actually give better result. You may see the details from the lesson notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lesson 3: Introduction to Text Classification II\n",
    "- Explore Classifiers\n",
    "\n",
    "- N-Grams\n",
    "\n",
    "- N-Gram Classification\n",
    "\n",
    "- Sentiment Analysis\n",
    "\n",
    "- Stemming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 3 Script\n",
    "In this lesson, we will explore classifiers we created in previous lesson. Specifically, we will extract top words in each topic and try to improve the classification with these new information.\n",
    "\n",
    "We will also introduce concept of N-Grams, which means a phase with multiple words. A lot of times, combination of words bear more accurate information than individual word. For example, \"university of illinois\" as a whole phrase carries specific meaning.\n",
    "\n",
    "We will also introduce sentiment analysis which is an application of text classification. In Sentiment analysis, the target label is customer sentiment or feeling, like positive and negative.\n",
    "\n",
    "We'll also demonstrate stemming in the sentiment analysis example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1\n",
    "#### Top Words\n",
    "\n",
    "|Topic|Top 5 Words|\n",
    "| :-: | :-: |\n",
    "| comp.sys.mac.hardware | mac, apple, **edu**, drive, quadra |\n",
    "| comp.windows.x | window, motif, mit, server, **com** |\n",
    "| misc.forsale | sale, **edu**, 00, offer, shipping |\n",
    "| rec.autos | car, **com**, cars, __edu__, engine |\n",
    "| rec.motorcycles | bike, **com**, dod, __edu__, ride |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 1 Script\n",
    "The classifilers we used in previous lesson to perform 20 newsgroup classification have an attribute coef_, which can be used to extract top words in each topic. Please note that not all classifiers have this attribute, k-nearest neighbor, decision tree and random forest don't have this attribute.\n",
    "\n",
    "This slide displays top 5 words for 5 topics. A lot of them make much sense. For example, top words for mac hardware includes mac and apple. But some common words appear in many topics, like edu and com. This is because every message has header which contains email address. Both edu and com are very common in email addresses. These two words don't really provide much predictive power and can be considered as stop words.\n",
    "\n",
    "In the lesson's notebook, we demonstrate how to add these words to stop words list and create document term matrix with the new stop words. We hope this change may improve the classification accuracy. Unfortunately, the change doesn't improve the model accuracy. The accuracy rate actually drops a little bit with the change. Of course, since we use train-test split approach, the accuracy score may not be very accurate. We need to use cross validation to get more accurate score. But this phenomenon is very common in data analytics, that extra effort may not always result a better model, we'll have to try combinations of improvements iteratively to get the optimum result.\n",
    "\n",
    "In this case, even we don't get better accuracy with the change, we do get more meaningful top words for topics as shown in the next slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Slide 2\n",
    "#### Grid Search  (Shade lines while speaking)\n",
    "|Topic|Top 5 Words|\n",
    "| :-: | :-: |\n",
    "| comp.sys.mac.hardware | mac, apple, drive, quadra, se |\n",
    "| comp.windows.x | window, motif, mit, server, widget |\n",
    "| misc.forsale | sale, 00, offer, shipping, new |\n",
    "| rec.autos | car, cars, engine, article, would |\n",
    "| rec.motorcycles | bike, dod, ride, bikes, motorcycle |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Slide 2 Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3\n",
    "#### N-Grams\n",
    "`This course introduces many concepts in data science.`\n",
    "##### 1-3 Grams\n",
    "`['this', 'course', 'introduces', 'many', 'concepts', 'in', 'data', 'science', 'this course', 'course introduces', 'introduces many', 'many concepts', 'concepts in', 'in data', 'data science', 'this course introduces', 'course introduces many', 'introduces many concepts', 'many concepts in', 'concepts in data', 'in data science']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3 Script\n",
    "Now let's look at a new concept, n-grams. an n-gram is a contiguous sequence of n words, so 3-grams is a phrase of 3 contiguous words. Let's use an example to understand this concept.\n",
    "\n",
    "For the sample sentence in the slide, This course introduces many concepts in data science. This sentence has 8 words. Untill now we tokenize text by individual word, so the DTM created by this sentence has 8 columns if we keep the stop words. But if we create DTM with 1 to 3 grams, which means all combinations of two and three contiguous words are also considered as tokens. Now we have 21 tokens, or 21 columns in the new DTM. \n",
    "\n",
    "If we use this new DTM in classification, we may achieve better result. But this is by no means guaranteed. This approach just provide a possible way to improve our classification model. But one thing is for sure, that the computation complexity is much higher if we apply n-grams because of the dramatic increase in number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4\n",
    "#### Sentiment Analysis\n",
    "<img src='https://www.kdnuggets.com/images/sentiment-fig-1-689.jpg' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4 Script\n",
    "Now let's look at a popular text analysis application, so called sentiment analysis.\n",
    "\n",
    "Sentiment analysis identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. In sentiment analysis, target label is customer sentiment, like positive, neutral or negative, toward a product or service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5\n",
    "#### Data\n",
    "- Built-in in NLTK\n",
    "- 2000 movie reviews\n",
    "- 1000 positive, 1000 negative\n",
    "\n",
    "##### Sample Review(negative)\n",
    "```\n",
    "Label: 0\n",
    "not a great twelve months for either of the principals from this movie . earlier this year , nora ephron wrote and produced one of the year ' s least likeable \" comedies \" called hanging up , featuring a bunch of annoying women ( ironically , lisa kudrow played one in that film as well ) who barely have time to care about anyone but themselves...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5 Script\n",
    "To demonstrate sentiment analysis, we use a movie review dataset which is built-in in the nltk library.\n",
    "This dataset has 2000 movie reviews, with 1000 posirive reviews and 1000 negative reviews. A snippet of  negative review is shown in this slide.\n",
    "\n",
    "To load this review data is not very straightforward. So you don't have to understand the code that loads the reviews in the lesson's notebook. Just assume that we have training text dataset and target labels available and focus on the analysis part when you read the lesson notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6\n",
    "#### What Customer Cares Most\n",
    "Top 20 Words of Positive Reviews:  \n",
    "`great, fun, life, hilarious, memorable, overall, quite, different, good, terrific, especially, trek, works, seen, performances, perfect, perfectly, comic, town, gives`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6 Script\n",
    "We can get top words in positive and negative reviews to understand what customers care. This slide shows the top 20 words in positive reviews given by a logistic regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7\n",
    "#### Stemming\n",
    "- Not supported by default\n",
    "- Define custom tokenize fucntion with PotterStemmer\n",
    "- Slow down training process\n",
    "- Improve accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7 Script\n",
    "\n",
    "In previous lesson, we introduced the concept of stemming. In this notebook, we demonstrate how to apply stemming in text classification. Unlike stopwords which is supported by counter vectorizer directly, to apply stemming, we will have to create customized tokenize function. In the lesson's notebook, we use PotterStemmer which is defined in the nltk library to do stemming. Since stemming is applied on every word in the corpus, this process slows down the trainning process noticablly. But stemming does result a more accurate model for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Review Script\n",
    "\n",
    "\n",
    "The first module's assignment is fairly straightforward. Just remember to work on the problems in order.\n",
    "\n",
    "Good luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
